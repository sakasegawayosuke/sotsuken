{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成されたサンプルを辞書に基づいて日本語に直す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seqGANで生成されたサンプルを辞書に基づいて日本語の文字列に変換する\n",
    "\n",
    "また、変換する際には、seqGANの識別機が算出したサンプルに対する本物かどうかの確率と紐付ける\n",
    "\n",
    "これは、生成されたキャッチコピーに対する自然さの度合いを表していると考えられ、キャッチコピーに対するスコア付けの手法の一つとして用いる"
   ]
  },
  {
   "source": [
    "今回は博多駅を対象にしており、他の観光地にて生成を行う場合は、適当に合わせること"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "生成数 6400\n重複をなくした生成数 464\n464\n\n\n[['アクセスが良い駅だなと思いまし', '0.9576534'], ['駅前の道路の工事渋滞で', '0.8326542'], ['お土産は１Fみやげもん市場', '0.70703167'], ['イベントもあるし１日遊べます', '0.66840965'], ['良いのが難点ですがアクセスも開催て', '0.6257231'], ['雰囲気が良い駅だなと思いまし', '0.55826056'], ['粥が良い駅だなと思いまし', '0.54197824'], ['シーズンになるとたくさんありました', '0.25650907'], ['地下鉄も新幹線でつながって', '0.21183991'], ['全体が数年前にリニューアルられ１日思います', '0.19926642']]\n"
     ]
    }
   ],
   "source": [
    "# 生成されたサンプルを辞書に基づいて日本語に戻す\n",
    "import pickle\n",
    "\n",
    "# seqGANにて生成した結果を読み込む\n",
    "with open('data/generated/hakataeki/output/output_text_200.txt') as f:\n",
    "    s = f.readlines()\n",
    "\n",
    "# seqGANの生成結果に対する識別器の確率を読み込む\n",
    "with open('data/generated/hakataeki/output/prob_200.txt') as f:\n",
    "    prob = f.readlines()\n",
    "\n",
    "# 生成結果はid列なので日本語に戻すために作成した辞書を読み込む\n",
    "with open('data/dict/all_dict.pickle', 'rb') as f:\n",
    "    indices_char = pickle.load(f)\n",
    "\n",
    "temp_list = []\n",
    "\n",
    "for i, j in zip(s, prob):\n",
    "    temp = [i,j]\n",
    "    temp_list.append(temp)\n",
    "\n",
    "print(\"生成数\",len(temp_list))\n",
    "\n",
    "num_prob_list = []\n",
    "\n",
    "for k in temp_list:\n",
    "    if k not in num_prob_list:\n",
    "        num_prob_list.append(k)\n",
    "\n",
    "print(\"重複をなくした生成数\",len(num_prob_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 順番を保持したまま重複をなくす\n",
    "# s = sorted(set(s), key=s.index)\n",
    "# print(len(s))\n",
    "\n",
    "haiku_prob_list = []\n",
    "haiku_temp = []\n",
    "result = \"\"\n",
    "for j in range(len(num_prob_list)):\n",
    "    line = num_prob_list[j][0].split()\n",
    "    line = [int(i) for i in line]\n",
    "    # print(line)\n",
    "    # 単語に対応するidを並べて行く\n",
    "    for i in line:\n",
    "        if i > len(indices_char):\n",
    "            continue\n",
    "        # print(char_indices[i])\n",
    "        result += str(indices_char[i])\n",
    "        result += \" \"\n",
    "    # result += \"0\" # 文末記号を追加\n",
    "    result = result.replace('_', '')\n",
    "    result = result.replace(' ', '')\n",
    "    # result += \"\\n\"\n",
    "    haiku_temp.append(result)\n",
    "    result = \"\"\n",
    "# print(haiku_temp[:10])\n",
    "for p in range(len(haiku_temp)):\n",
    "    num_prob_list[p][1] = num_prob_list[p][1][:-1]\n",
    "    temp = [haiku_temp[p],num_prob_list[p][1]]\n",
    "    haiku_prob_list.append(temp)\n",
    "\n",
    "print(len(haiku_prob_list))\n",
    "print(\"\\n\")\n",
    "print(haiku_prob_list[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語に変換したサンプルが575の音韻を満たしているかフィルタリングを行う\n",
    "\n",
    "やってること自体は、レビューから575の文字列を抽出しているのと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "136\n[['アクセスが良い駅だなと思いまし', '0.9576534'], ['駅前の道路の工事渋滞で', '0.8326542'], ['お土産は１Fみやげもん市場', '0.70703167'], ['イベントもあるし１日遊べます', '0.66840965'], ['雰囲気が良い駅だなと思いまし', '0.55826056'], ['シーズンになるとたくさんありました', '0.25650907'], ['地下鉄も新幹線でつながって', '0.21183991'], ['全体が数年前にリニューアル', '0.19926642'], ['福岡の窓口である博多駅', '0.14426497'], ['福岡の美味しいものが揃ってい', '0.1317006']]\n"
     ]
    }
   ],
   "source": [
    "# with open('generated/sepa/hakataeki/content/save/output/output_2_haiku.txt') as f:\n",
    "#     s = f.readlines()\n",
    "\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\") #mecab neologd\n",
    "\n",
    "\n",
    "# text_list = []\n",
    "\n",
    "# for line in s:\n",
    "#     line = line.replace(\" \", \"\")\n",
    "#     text_list.append(line)\n",
    "\n",
    "# print(text_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "haiku_list = []\n",
    "for x,y in haiku_prob_list:\n",
    "    temp = x\n",
    "    # print(temp)\n",
    "\n",
    "    word_list = []\n",
    "\n",
    "    # 形態素解析をして、単語毎にリストに格納\n",
    "    for word in tagger.parse(temp).splitlines():\n",
    "        word = word.replace('\\t', ',')\n",
    "        # print(word)\n",
    "        temp_word_list = word.split(',')\n",
    "        word_list.append(temp_word_list)\n",
    "\n",
    "    # 文末の記号を削除\n",
    "    word_list.pop(-1)\n",
    "\n",
    "    # print(word_list)\n",
    "\n",
    "    # 格納された単語を繋げて17音の文字列を作る\n",
    "    for j in range(len(word_list)):\n",
    "\n",
    "        # 最初に空の文字列を用意\n",
    "        temp_haiku = \"\"\n",
    "        part = \"\"\n",
    "        # 次の単語を示すカウント\n",
    "        count = 0\n",
    "        # 音を数えるcount\n",
    "        haku_count = 0\n",
    "\n",
    "        # 17音を超えるまで単語を繋げる\n",
    "\n",
    "        if not ('名詞' == word_list[j+count][1] or '動詞' == word_list[j+count][1] or '形容詞' == word_list[j+count][1]):\n",
    "            continue\n",
    "        if ('名詞' ==  word_list[j+count][1] and ('非自立' == word_list[j+count][2] or '接尾' == word_list[j+count][2])):\n",
    "            continue\n",
    "        if ('動詞' ==  word_list[j+count][1] and '非自立' == word_list[j+count][2]):\n",
    "            continue\n",
    "\n",
    "        # 上の句5\n",
    "        while haku_count < 5:\n",
    "            # 単語を繋げる\n",
    "            part =part + word_list[j + count][0]\n",
    "            # 音を記録\n",
    "\n",
    "            if '記号' not in word_list[j+count][1]:\n",
    "                temp = word_list[j+count][-1]\n",
    "\n",
    "                # 小さい文字は音にならないので消去\n",
    "                # temp = temp.replace('ッ', '')\n",
    "                temp = temp.replace('ャ', '')\n",
    "                temp = temp.replace('ュ', '')\n",
    "                temp = temp.replace('ョ', '')\n",
    "                temp = temp.replace('ァ', '')\n",
    "                temp = temp.replace('\bィ', '')\n",
    "                temp = temp.replace('ゥ', '')\n",
    "                temp = temp.replace('ェ', '')\n",
    "                temp = temp.replace('ォ', '')\n",
    "\n",
    "                haku_count += len(temp)\n",
    "            # 元の文が終わっていたら終了\n",
    "            if j + count >= len(word_list)-1:\n",
    "                break\n",
    "            count += 1\n",
    "            # print(part, haku_count)\n",
    "        \n",
    "        if haku_count == 5:\n",
    "            temp_haiku += part\n",
    "            # temp_haiku += \"\\n\"\n",
    "            part = \"\"\n",
    "            haku_count = 0\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not ('名詞' == word_list[j+count][1] or '動詞' == word_list[j+count][1] or '形容詞' == word_list[j+count][1]):\n",
    "            continue\n",
    "        if ('名詞' ==  word_list[j+count][1] and ('非自立' == word_list[j+count][2] or '接尾' == word_list[j+count][2])):\n",
    "            continue\n",
    "        if ('動詞' ==  word_list[j+count][1] and '非自立' == word_list[j+count][2]):\n",
    "            continue\n",
    "\n",
    "        # 中の句7\n",
    "        while haku_count < 7:\n",
    "            # 単語を繋げる\n",
    "            part =part + word_list[j + count][0]\n",
    "            # 音を記録\n",
    "\n",
    "            if '記号' not in word_list[j+count][1]:\n",
    "                temp = word_list[j+count][-1]\n",
    "\n",
    "                # 小さい文字は音にならないので消去\n",
    "                # temp = temp.replace('ッ', '')\n",
    "                temp = temp.replace('ャ', '')\n",
    "                temp = temp.replace('ュ', '')\n",
    "                temp = temp.replace('ョ', '')\n",
    "                temp = temp.replace('ァ', '')\n",
    "                temp = temp.replace('\bィ', '')\n",
    "                temp = temp.replace('ゥ', '')\n",
    "                temp = temp.replace('ェ', '')\n",
    "                temp = temp.replace('ォ', '')\n",
    "\n",
    "                haku_count += len(temp)\n",
    "            # 元の文が終わっていたら終了\n",
    "            if j + count >= len(word_list)-1:\n",
    "                break\n",
    "            count += 1\n",
    "            # print(temp_haiku, haku_count)\n",
    "\n",
    "        \n",
    "        \n",
    "        if haku_count == 7:\n",
    "            temp_haiku += part\n",
    "            # temp_haiku += \"\\n\"\n",
    "            part = \"\"\n",
    "            haku_count = 0\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not ('名詞' == word_list[j+count][1] or '動詞' == word_list[j+count][1] or '形容詞' == word_list[j+count][1]):\n",
    "            continue\n",
    "        if ('名詞' ==  word_list[j+count][1] and ('非自立' == word_list[j+count][2] or '接尾' == word_list[j+count][2])):\n",
    "            continue\n",
    "        if ('動詞' ==  word_list[j+count][1] and '非自立' == word_list[j+count][2]):\n",
    "            continue\n",
    "\n",
    "        # 下の句5\n",
    "        while haku_count < 5:\n",
    "            # 単語を繋げる\n",
    "            part =part + word_list[j + count][0]\n",
    "            # 音を記録\n",
    "\n",
    "            if '記号' not in word_list[j+count][1]:\n",
    "                temp = word_list[j+count][-1]\n",
    "\n",
    "                # 小さい文字は音にならないので消去\n",
    "                # temp = temp.replace('ッ', '')\n",
    "                temp = temp.replace('ャ', '')\n",
    "                temp = temp.replace('ュ', '')\n",
    "                temp = temp.replace('ョ', '')\n",
    "                temp = temp.replace('ァ', '')\n",
    "                temp = temp.replace('\bィ', '')\n",
    "                temp = temp.replace('ゥ', '')\n",
    "                temp = temp.replace('ェ', '')\n",
    "                temp = temp.replace('ォ', '')\n",
    "\n",
    "                haku_count += len(temp)\n",
    "            # 元の文が終わっていたら終了\n",
    "            if j + count >= len(word_list)-1:\n",
    "                break\n",
    "            count += 1\n",
    "            # print(temp_haiku, haku_count)\n",
    "\n",
    "        \n",
    "        if haku_count == 5:\n",
    "            temp_haiku += part\n",
    "            # temp_haiku += \"\\n\"\n",
    "            # temp_haiku += \"\\n\"\n",
    "            part = \"\"\n",
    "            haku_count = 0\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if temp_haiku not in haiku_list:\n",
    "            haiku_list.append([temp_haiku,y])\n",
    "\n",
    "print(len(haiku_list))\n",
    "\n",
    "print(haiku_list[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スコア付け"
   ]
  },
  {
   "source": [
    "フィルタリングを行ったサンプルに対して、３つの手法でスコア付けを行う\n",
    "\n",
    "- tf-idfを用いたスコア付け　⇨　観光地の特徴度を表す\n",
    "- せqGANの識別器が算出したキャッチコピーに対する本物かどうかの確率を用いる　⇨　キャッチコピーの自然さを表す\n",
    "- 先の２つのスコアを組み合わせたスコア付け　⇨　キャッチコピーの全体的な完成度を表す"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# seqGANの識別器を用いたスコア付け"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "seqGANでは生成器が生成したキャッチコピーに対して、識別器がそれが本物かどうかの確率を算出する。\n",
    "\n",
    "この時、識別器が本物だと判断すると、生成器は識別器を騙すことができており、自然なキャッチコピーが生成できていると考えられる。\n",
    "\n",
    "したがって、識別器の確率は、キャッチコピーの自然さを表していると考えられる。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idfを用いたスコア付け"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idfを用いたスコア付けでは、サンプルに含まれる名詞に対するtf-idfの値の合計をキャッチコピーのスコアとする\n",
    "\n",
    "これはキャッチコピーが持つ観光地の特徴度合いを表していると考えられる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.0, 0.0033141180043055746, 0.018665275928839205, 0.00022918083362860256, 0.0, 0.0, 0.019500547406398814, 0.0007971909090526924, 0.00020799975214914443, 0.0, 0.0057355019333819425, 0.008381355684012938, 0.016549528840836197, 0.0005627747554481164, 0.0007521069133698054, 0.014247983022216394, 0.019500547406398814, 0.008381355684012938, 0.008381355684012938, 0.010981352585877244, 0.0, 0.000637516496555504, 0.00029159146122394435, 0.002047021129348379, 0.0025609697923372867, 0.0030266006636713388, 0.00023281543566702607, 0.019500547406398814, 0.0020953389210032346, 0.0, 0.00469533582286754, 2.864760420357532e-05, 0.008381355684012938, 0.0020953389210032346, 0.008482523949407184, 0.0, 0.00046563087133405214, 0.005564564012406137, 0.019500547406398814, 0.00035947721037082403, 0.0, 0.014247983022216394, 0.001221569705389551, 0.0007776304995577687, 0.0025609697923372867, 0.0, 0.0025999969018643053, 0.0005627747554481164, 0.00022918083362860256, 0.00041599950429828885, 0.014247983022216394, 0.014435574607365767, 0.002328154356670261, 0.014247983022216394, 0.00029159146122394435, 0.005252564384182421, 0, 0.0009379579257468608, 0.014247983022216394, 0.00046563087133405214, 0.00023281543566702607, 0.0033852912427934615, 0.011305765711525098, 0.008381355684012938, 0.016257937379612255, 0.0001718856252214519, 0.0004723894913880875, 0.008381355684012938, 0.008381355684012938, 0.0017679978932677277, 0.00041892620366663056, 0.0, 0.0003119996282237166, 0.0, 0.0017679978932677277, 0.0053565642602569925, 0.016257937379612255, 0.018665275928839205, 0.0060532013273426775, 0.008381355684012938, 0.014247983022216394, 0.014247983022216394, 0.014247983022216394, 0.001975997645416872, 0.0, 0.0, 0.014247983022216394, 0.0, 0.0, 0.002047021129348379, 0.0, 0.001975997645416872, 0.0, 0.0025609697923372867, 0.014247983022216394, 0.0001718856252214519, 0.005564564012406137, 0.0, 0.016257937379612255, 0.0011640771783351304, 0.0017679978932677277, 0.014247983022216394, 0.008381355684012938, 0.0007971909090526924, 0.013968926140021565, 0.0, 0.014247983022216394, 0.0004723894913880875, 0.001975997645416872, 0.005252564384182421, 0.0017406638644450602, 0.0, 0.0, 0.0011640771783351304, 0.015944923785438436, 0.006416641562517551, 0.032202861165050695, 0.0, 0.0, 0.0, 0.0022088130810838983, 0.0058872990093671255, 0.002047021129348379, 0.008128968689806127, 0.0002005332294250272, 0.008128968689806127, 0.0, 0, 0.0, 0.0, 0.0, 0.0009380203627221396, 0.0011640771783351304, 0.03449187240464138, 0.018233935025029128, 0.018233935025029128]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------データベース----------------------------------------\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "db_word_list = []\n",
    "\n",
    "conn = sqlite3.connect('data/tfidf_db.db')\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('select word from hakataeki')\n",
    "db_words = c.fetchall()\n",
    "\n",
    "for word in db_words:\n",
    "    db_word_list.append(word[0])\n",
    "\n",
    "# print(db_word_list)\n",
    "\n",
    "#-------------------------------------------------tfidfのスコア付け----------------------------------------\n",
    "score_list = []\n",
    "\n",
    "for i,_ in haiku_list:\n",
    "    temp = i\n",
    "    # print(temp)\n",
    "\n",
    "    word_list = []\n",
    "    score_sum = 0\n",
    "\n",
    "    # 形態素解析をして、単語毎にリストに格納\n",
    "    for word in tagger.parse(temp).splitlines():\n",
    "        word = word.replace('\\t', ',')\n",
    "        temp_word_list = word.split(',')\n",
    "        word_list.append(temp_word_list)\n",
    "\n",
    "    # 文末の記号を削除\n",
    "    word_list.pop(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    for j in range(len(word_list)):\n",
    "        # print(word_list[j][0])\n",
    "\n",
    "        temp_score = 0\n",
    "\n",
    "        if '名詞' not in word_list[j][1]:\n",
    "            continue\n",
    "\n",
    "        # 形態素解析した名詞がdbに含まれているならtfidfの値をスコアとして加算していく\n",
    "        if word_list[j][0] in db_word_list:\n",
    "            c.execute('select tfidf from hakataeki where word = ?', (word_list[j][0],))\n",
    "            tfidf = c.fetchone()[0]\n",
    "            temp_score = tfidf\n",
    "        \n",
    "        score_sum += temp_score\n",
    "\n",
    "    score_list.append(score_sum)\n",
    "\n",
    "        \n",
    "\n",
    "# print(score_list)\n",
    "\n",
    "#-------------------------------------------------データベース----------------------------------------\n",
    "\n",
    "#閉じる\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "生成したキャッチコピー数： 136\n\n生成例\nキャッチコピー: アクセスが良い駅だなと思いまし  \t\t 識別器の確率: 0.9576534 \t\t tf-idf値: 0.0\nキャッチコピー: 駅前の道路の工事渋滞で  \t\t 識別器の確率: 0.8326542 \t\t tf-idf値: 0.0033141180043055746\nキャッチコピー: お土産は１Fみやげもん市場  \t\t 識別器の確率: 0.70703167 \t\t tf-idf値: 0.018665275928839205\nキャッチコピー: イベントもあるし１日遊べます  \t\t 識別器の確率: 0.66840965 \t\t tf-idf値: 0.00022918083362860256\nキャッチコピー: 雰囲気が良い駅だなと思いまし  \t\t 識別器の確率: 0.55826056 \t\t tf-idf値: 0.0\nキャッチコピー: シーズンになるとたくさんありました  \t\t 識別器の確率: 0.25650907 \t\t tf-idf値: 0.0\nキャッチコピー: 地下鉄も新幹線でつながって  \t\t 識別器の確率: 0.21183991 \t\t tf-idf値: 0.019500547406398814\nキャッチコピー: 全体が数年前にリニューアル  \t\t 識別器の確率: 0.19926642 \t\t tf-idf値: 0.0007971909090526924\nキャッチコピー: 福岡の窓口である博多駅  \t\t 識別器の確率: 0.14426497 \t\t tf-idf値: 0.00020799975214914443\nキャッチコピー: 福岡の美味しいものが揃ってい  \t\t 識別器の確率: 0.1317006 \t\t tf-idf値: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 俳句とtfidfのスコアをまとめたリストを作成\n",
    "\n",
    "for i in range(len(haiku_list)):\n",
    "    haiku_list[i].append(score_list[i])\n",
    "\n",
    "print(\"生成したキャッチコピー数：\",len(haiku_list))\n",
    "print(\"\")\n",
    "print(\"生成例\")\n",
    "for n in haiku_list[:10]:\n",
    "    print(\"キャッチコピー:\", n[0], \" \\t\\t\",\"識別器の確率:\",n[1], \"\\t\\t\", \"tf-idf値:\",n[2])\n",
    "\n",
    "# print(haiku_list)\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open('data/generated/sepa/dazaifu/content/save/output/haiku_dis_tfidf.pickle', 'wb') as f:\n",
    "#     pickle.dump(haiku_list, f)"
   ]
  },
  {
   "source": [
    "# 識別器の確率とtfidfを組み合わせた総合的なスコア付け"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "識別器の確率とtf-idfの値を組み合わせることで、キャッチコピーの総合的な完成度を表す。\n",
    "\n",
    "組み合わせには、正規化した両方の値の調和平均を用いた。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.0, 0.09608402714198873, 0.5411499761412639, 0.006644488038804265, 0.0, 0.0, 0.5653664485832532, 0.023112427754006733, 0.00603039897947538, 0.0, 0.16628560682632432, 0.24299509129823596, 0.47980952285470063, 0.01631615555241318, 0.021805337342851772, 0.4130823300940636, 0.5653664485832532, 0.24299509129823596, 0.24299509129823596, 0.31837507854167824, 0.0, 0.01848309332344964, 0.00845391800720875, 0.05934792710971882, 0.07424850011890544, 0.08774822741325188, 0.006749863647173221, 0.5653664485832532, 0.06074877282455899, 0.0, 0.13612876006800123, 0.0008305610048505331, 0.24299509129823596, 0.06074877282455899, 0.24592819577593406, 0.0, 0.013499727294346443, 0.16132971695840276, 0.5653664485832532, 0.010422084546574258, 0.0, 0.4130823300940636, 0.03541616097435091, 0.022545325763559512, 0.07424850011890544, 0.0, 0.07537998724344225, 0.01631615555241318, 0.006644488038804265, 0.01206079795895076, 0.4130823300940636, 0.41852104861153466, 0.06749863647173222, 0.4130823300940636, 0.00845391800720875, 0.15228411848918968, 0.0, 0.027193592587355302, 0.4130823300940636, 0.013499727294346443, 0.006749863647173221, 0.09814750568130715, 0.32778057331569344, 0.24299509129823596, 0.47135560484749195, 0.0049833660291031985, 0.013695675486858192, 0.24299509129823596, 0.24299509129823596, 0.051258391325540734, 0.012145649814309819, 0.0, 0.00904559846921307, 0.0, 0.051258391325540734, 0.15529931797892738, 0.47135560484749195, 0.5411499761412639, 0.17549645482650375, 0.24299509129823596, 0.4130823300940636, 0.4130823300940636, 0.4130823300940636, 0.057288790305016116, 0.0, 0.0, 0.4130823300940636, 0.0, 0.0, 0.05934792710971882, 0.0, 0.057288790305016116, 0.0, 0.07424850011890544, 0.4130823300940636, 0.0049833660291031985, 0.16132971695840276, 0.0, 0.47135560484749195, 0.03374931823586611, 0.051258391325540734, 0.4130823300940636, 0.24299509129823596, 0.023112427754006733, 0.4049918188303933, 0.0, 0.4130823300940636, 0.013695675486858192, 0.057288790305016116, 0.15228411848918968, 0.05046591394124572, 0.0, 0.0, 0.03374931823586611, 0.4622806091354094, 0.1860334367250558, 0.9336362139829014, 0.0, 0.0, 0.0, 0.06403865395218934, 0.17068655885944028, 0.05934792710971882, 0.23567780242374597, 0.0058139270339537304, 0.23567780242374597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027195402781204633, 0.03374931823586611, 1.0, 0.528644395152508, 0.528644395152508]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "haiku_score_list = haiku_list\n",
    "\n",
    "haiku_list_fin = []\n",
    "tfidf_list = []\n",
    "dis_list = []\n",
    "\n",
    "for i in haiku_score_list:\n",
    "    haiku_list_fin.append(i[0])\n",
    "    dis_list.append(float(i[1]))\n",
    "    tfidf_list.append(float(i[2]))\n",
    "\n",
    "def min_max(l):\n",
    "    l_min = min(l)\n",
    "    l_max = max(l)\n",
    "    return [(i - l_min) / (l_max - l_min) for i in l]\n",
    "\n",
    "def standardization(l):\n",
    "    l_mean = statistics.mean(l)\n",
    "    l_stdev = statistics.stdev(l)\n",
    "    return [(i - l_mean) / l_stdev for i in l]\n",
    "\n",
    "# 標準化\n",
    "# tfidf_list_std = standardization(tfidf_list)\n",
    "# dis_list_std = standardization(dis_list)\n",
    "# print(tfidf_list_std)\n",
    "\n",
    "# 調和平均では値がマイナスだと困るため正規化を用いる\n",
    "tfidf_list_std = min_max(tfidf_list)\n",
    "dis_list_std = min_max(dis_list)\n",
    "# print(tfidf_list_std)"
   ]
  },
  {
   "source": [
    "調和平均を求める"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 0.17303496477142397, 0.6241658019042455, 0.013163449335735355, 0, 0, 0.31481923477463325, 0.041540615332564534, 0.011586470500946362, 0, 0.14164687373901252, 0.14517734959389478, 0.13768295165397126, 0.02638947508359181, 0.03299454037829724, 0.11163222012888578, 0.11529563937951873, 0.094058178641278, 0.093290145989042, 0.09396530397602362, 0, 0.027085199411670906, 0.014441128137855728, 0.05153062745260041, 0.05556343671528252, 0.05895731082480468, 0.011701556716858198, 0.07809120061471288, 0.04884959752380007, 0, 0.062286327775097586, 0.001627173220920884, 0.06807156734576096, 0.04709312550453226, 0.06499663057347074, 0, 0.019655806689244274, 0.058103165699126116, 0.06387963289134495, 0.015916012793348134, 0, 0.05954775798011057, 0.03364210882154876, 0.026462437240233932, 0.04413056428675509, 0, 0.043981248873862844, 0.021369204648615945, 0.010940571205093888, 0.01731166102972886, 0.05530116603988069, 0.055147698790540475, 0.04068706408910845, 0.052544219230473595, 0.012968835679222863, 0.04659451900763195, 0, 0.02669790980461778, 0.04923081590972045, 0.01767631009723115, 0.010682541980444557, 0.04039854475258082, 0.04673017450339396, 0.04555459048001655, 0.047156560428504905, 0.00829205723280566, 0.017466268707939978, 0.04356968914718137, 0.043299230106814475, 0.03206893718099438, 0.015777018166376878, 0, 0.012767061811717918, 0, 0.03035649488467862, 0.037829101623348765, 0.04110550786462803, 0.04113126815263116, 0.0377944054450079, 0.037892646328665985, 0.03890645562726372, 0.038089548309117975, 0.03692658643516837, 0.028389255464339378, 0, 0, 0.0326278355997518, 0, 0, 0.025676643923690096, 0, 0.024533968093969755, 0, 0.02516681801429716, 0.02891344979426312, 0.007477563554218509, 0.027334928597884323, 0, 0.02764056498895981, 0.0197829299690002, 0.02172417865029409, 0.0264459292660195, 0.025733646192690446, 0.01701669378369575, 0.025693768908200507, 0, 0.02481099940720799, 0.013096135032196388, 0.02054410138136136, 0.0228535760520393, 0.01920925128199538, 0, 0, 0.016849302070567944, 0.021302557953449555, 0.02059751115481939, 0.021403767647789597, 0, 0, 0, 0.01569789010738372, 0.01613022064568001, 0.014677890982831624, 0.015611509627723745, 0.006711839145706705, 0.013835920820432383, 0, 0, 0, 0, 0, 0.009133651815973564, 0.0030490233801827474, 0.0015259215659632643, 7.364213822576024e-05, 0]\n"
     ]
    }
   ],
   "source": [
    "sum_list = [0]*len(tfidf_list_std)\n",
    "\n",
    "# 調和平均を計算\n",
    "# 識別器の確率かtfidfのどちらかが0の場合は、調和平均を0にする\n",
    "for i in range(len(tfidf_list_std)):\n",
    "    if tfidf_list_std[i] == 0 or dis_list_std[i] == 0:\n",
    "        sum_list[i] = 0\n",
    "    else:\n",
    "        sum_list[i] = 2/(1/tfidf_list_std[i] + 1/dis_list_std[i])\n",
    "\n",
    "# print(sum_list)"
   ]
  },
  {
   "source": [
    "生成例とそれに対するスコアを表示\n",
    "\n",
    "識別器の確率とtfidfの値は、正規化されているため0~1で表示される"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "生成例: アクセスが良い駅だなと思いまし \t 識別器: 1.0 \t tf-idf: 0.0 \t 調和平均: 0\n生成例: 駅前の道路の工事渋滞で \t 識別器: 0.8689603548189178 \t tf-idf: 0.09608402714198873 \t 調和平均: 0.17303496477142397\n生成例: お土産は１Fみやげもん市場 \t 識別器: 0.7372672579194987 \t tf-idf: 0.5411499761412639 \t 調和平均: 0.6241658019042455\n生成例: イベントもあるし１日遊べます \t 識別器: 0.6967788724180182 \t tf-idf: 0.006644488038804265 \t 調和平均: 0.013163449335735355\n生成例: 雰囲気が良い駅だなと思いまし \t 識別器: 0.5813069520327749 \t tf-idf: 0.0 \t 調和平均: 0\n生成例: シーズンになるとたくさんありました \t 識別器: 0.2649736620400159 \t tf-idf: 0.0 \t 調和平均: 0\n生成例: 地下鉄も新幹線でつながって \t 識別器: 0.21814591532694091 \t tf-idf: 0.5653664485832532 \t 調和平均: 0.31481923477463325\n生成例: 全体が数年前にリニューアル \t 識別器: 0.20496482562166374 \t tf-idf: 0.023112427754006733 \t 調和平均: 0.041540615332564534\n生成例: 福岡の窓口である博多駅 \t 識別器: 0.14730549266237247 \t tf-idf: 0.00603039897947538 \t 調和平均: 0.011586470500946362\n生成例: 福岡の美味しいものが揃ってい \t 識別器: 0.13413396367079627 \t tf-idf: 0.0 \t 調和平均: 0\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "for i in range(len(sum_list)):\n",
    "    temp = [haiku_list_fin[i],dis_list_std[i],tfidf_list_std[i],sum_list[i]]\n",
    "    score_list.append(temp)\n",
    "\n",
    "for n in score_list[:10]:\n",
    "    print(\"生成例:\", n[0], \"\\t\", \"識別器:\", n[1], \"\\t\", \"tf-idf:\", n[2], \"\\t\", \"調和平均:\", n[3])\n",
    "\n",
    "# スコアを保存\n",
    "import pickle\n",
    "\n",
    "with open('data/generated/hakataeki/score/score.pickle', 'wb') as f:\n",
    "    pickle.dump(score_list, f)"
   ]
  },
  {
   "source": [
    "# 各スコアでの上位のキャッチコピー"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "各スコア付けでの上位１０件のキャッチコピーを表示"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/generated/hakataeki/score/score.pickle', 'rb') as f:\n",
    "    score_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "識別器の確率によるスコアでの上位のキャッチコピー\n\n生成例: アクセスが良い駅だなと思いまし \t 識別器: 1.0\n生成例: 駅前の道路の工事渋滞で \t 識別器: 0.8689603548189178\n生成例: お土産は１Fみやげもん市場 \t 識別器: 0.7372672579194987\n生成例: イベントもあるし１日遊べます \t 識別器: 0.6967788724180182\n生成例: 雰囲気が良い駅だなと思いまし \t 識別器: 0.5813069520327749\n生成例: シーズンになるとたくさんありました \t 識別器: 0.2649736620400159\n生成例: 地下鉄も新幹線でつながって \t 識別器: 0.21814591532694091\n生成例: 全体が数年前にリニューアル \t 識別器: 0.20496482562166374\n生成例: 福岡の窓口である博多駅 \t 識別器: 0.14730549266237247\n生成例: 福岡の美味しいものが揃ってい \t 識別器: 0.13413396367079627\n"
     ]
    }
   ],
   "source": [
    "# 識別器の確率によるスコアでの上位のキャッチコピー\n",
    "\n",
    "score_list_10 = sorted(score_list, key=lambda x: x[1])\n",
    "score_list_10.reverse()\n",
    "# score_list_10.pop(4)\n",
    "score_list_10 = score_list_10[:10]\n",
    "\n",
    "print(\"識別器の確率によるスコアでの上位のキャッチコピー\")\n",
    "print(\"\")\n",
    "for n in score_list_10:\n",
    "    print(\"生成例:\", n[0], \"\\t\", \"識別器:\", n[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tfidfによるスコアでの上位のキャッチコピー\n\n生成例: お土産に直結なのでお土産に \t tfidf: 1.0\n生成例: 阪急に直結なのでお土産に \t tfidf: 0.9336362139829014\n生成例: 地下鉄も新幹線で行きました \t tfidf: 0.5653664485832532\n生成例: 地下鉄も新幹線で開催て \t tfidf: 0.5653664485832532\n生成例: 地下鉄も新幹線で取りました \t tfidf: 0.5653664485832532\n生成例: 地下鉄も新幹線でつながって \t tfidf: 0.5653664485832532\n生成例: お土産や１Fみやげもん市場 \t tfidf: 0.5411499761412639\n生成例: お土産は１Fみやげもん市場 \t tfidf: 0.5411499761412639\n生成例: 境内に直結なのでお土産に \t tfidf: 0.528644395152508\n生成例: 散策に直結なのでお土産に \t tfidf: 0.528644395152508\n"
     ]
    }
   ],
   "source": [
    "# tfidfによるスコアでの上位のキャッチコピー\n",
    "\n",
    "score_list_10 = sorted(score_list, key=lambda x: x[2])\n",
    "score_list_10.reverse()\n",
    "# score_list_10.pop(4)\n",
    "score_list_10 = score_list_10[:10]\n",
    "\n",
    "print(\"tfidfによるスコアでの上位のキャッチコピー\")\n",
    "print(\"\")\n",
    "for n in score_list_10:\n",
    "    print(\"生成例:\", n[0], \"\\t\", \"tfidf:\", n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "調和平均によるスコアでの上位のキャッチコピー\n\n生成例: お土産は１Fみやげもん市場 \t 調和平均: 0.6241658019042455\n生成例: 地下鉄も新幹線でつながって \t 調和平均: 0.31481923477463325\n生成例: 駅前の道路の工事渋滞で \t 調和平均: 0.17303496477142397\n生成例: 福岡の玄関口につき人が \t 調和平均: 0.14517734959389478\n生成例: 地下鉄も外国人の思入れ \t 調和平均: 0.14164687373901252\n生成例: お土産を外国からのお客様 \t 調和平均: 0.13768295165397126\n生成例: 地下鉄も新幹線で取りました \t 調和平均: 0.11529563937951873\n生成例: イベントに新幹線で開催さ \t 調和平均: 0.11163222012888578\n生成例: 九州の玄関口でつき人が \t 調和平均: 0.094058178641278\n生成例: 九州の玄関口でJR \t 調和平均: 0.09396530397602362\n"
     ]
    }
   ],
   "source": [
    "# 調和平均によるスコアでの上位のキャッチコピー\n",
    "\n",
    "score_list_10 = sorted(score_list, key=lambda x: x[3])\n",
    "score_list_10.reverse()\n",
    "# score_list_10.pop(4)\n",
    "score_list_10 = score_list_10[:10]\n",
    "\n",
    "print(\"調和平均によるスコアでの上位のキャッチコピー\")\n",
    "print(\"\")\n",
    "for n in score_list_10:\n",
    "    print(\"生成例:\", n[0], \"\\t\", \"調和平均:\", n[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python387jvsc74a57bd09f1e5f8df66e0355b93d6a73e8e18cceb2fedad000b7b1dd4a514e55097c6a9d",
   "display_name": "Python 3.8.7 64-bit ('3.8.7': pyenv)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.7-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "9f1e5f8df66e0355b93d6a73e8e18cceb2fedad000b7b1dd4a514e55097c6a9d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}