{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成されたサンプルを辞書に基づいて日本語に直す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seqGANで生成されたサンプルを辞書に基づいて日本語の文字列に変換する\n",
    "\n",
    "また、変換する際には、seqGANの識別機が算出したサンプルに対する本物かどうかの確率と紐付ける\n",
    "\n",
    "これは、生成されたキャッチコピーに対する自然さの度合いを表していると考えられ、キャッチコピーに対するスコア付けの手法の一つとして用いる"
   ]
  },
  {
   "source": [
    "今回は博多駅を対象にしており、他の観光地にて生成を行う場合は、適当に合わせること"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "生成数 6400\n重複をなくした生成数 604\n604\n\n\n[['有名なバリエーションの商店街もにぎわっました', '0.94820225'], ['お参りにたどり着くまで４時間', '0.6153461'], ['2月に途中一年半振りに', '0.53529686'], ['大好きな和菓子さんになる店が', '0.4433223'], ['参道も天満宮も参拝者', '0.43593347'], ['参道の梅が枝餅を購入し', '0.42193896'], ['いつ見ても９時前を', '0.3748845'], ['され失意のうちにました', '0.35776415'], ['境内の梅が枝餅を焼く自分', '0.31326932'], ['方々の観光客もにぎわっました', '0.29361945']]\n"
     ]
    }
   ],
   "source": [
    "# 生成されたサンプルを辞書に基づいて日本語に戻す\n",
    "import pickle\n",
    "\n",
    "# seqGANにて生成した結果を読み込む\n",
    "with open('data/generated/dazaifu/output/output_text_200.txt') as f:\n",
    "    s = f.readlines()\n",
    "\n",
    "# seqGANの生成結果に対する識別器の確率を読み込む\n",
    "with open('data/generated/dazaifu/output/prob_200.txt') as f:\n",
    "    prob = f.readlines()\n",
    "\n",
    "# 生成結果はid列なので日本語に戻すために作成した辞書を読み込む\n",
    "with open('data/dict/all_dict.pickle', 'rb') as f:\n",
    "    indices_char = pickle.load(f)\n",
    "\n",
    "temp_list = []\n",
    "\n",
    "for i, j in zip(s, prob):\n",
    "    temp = [i,j]\n",
    "    temp_list.append(temp)\n",
    "\n",
    "print(\"生成数\",len(temp_list))\n",
    "\n",
    "num_prob_list = []\n",
    "\n",
    "for k in temp_list:\n",
    "    if k not in num_prob_list:\n",
    "        num_prob_list.append(k)\n",
    "\n",
    "print(\"重複をなくした生成数\",len(num_prob_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 順番を保持したまま重複をなくす\n",
    "# s = sorted(set(s), key=s.index)\n",
    "# print(len(s))\n",
    "\n",
    "haiku_prob_list = []\n",
    "haiku_temp = []\n",
    "result = \"\"\n",
    "for j in range(len(num_prob_list)):\n",
    "    line = num_prob_list[j][0].split()\n",
    "    line = [int(i) for i in line]\n",
    "    # print(line)\n",
    "    # 単語に対応するidを並べて行く\n",
    "    for i in line:\n",
    "        if i > len(indices_char):\n",
    "            continue\n",
    "        # print(char_indices[i])\n",
    "        result += str(indices_char[i])\n",
    "        result += \" \"\n",
    "    # result += \"0\" # 文末記号を追加\n",
    "    result = result.replace('_', '')\n",
    "    result = result.replace(' ', '')\n",
    "    # result += \"\\n\"\n",
    "    haiku_temp.append(result)\n",
    "    result = \"\"\n",
    "# print(haiku_temp[:10])\n",
    "for p in range(len(haiku_temp)):\n",
    "    num_prob_list[p][1] = num_prob_list[p][1][:-1]\n",
    "    temp = [haiku_temp[p],num_prob_list[p][1]]\n",
    "    haiku_prob_list.append(temp)\n",
    "\n",
    "print(len(haiku_prob_list))\n",
    "print(\"\\n\")\n",
    "print(haiku_prob_list[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語に変換したサンプルが575の音韻を満たしているかフィルタリングを行う\n",
    "\n",
    "やってること自体は、レビューから575の文字列を抽出しているのと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "156\n[['参道も天満宮も参拝者', '0.43593347'], ['参道の梅が枝餅を購入し', '0.42193896'], ['境内の梅が枝餅を焼く自分', '0.31326932'], ['名物のクルーズ船が入ったと', '0.26894203'], ['参道を梅ヶ枝餅を食べながら', '0.23119384'], ['焼きもちを焼くので橋を渡るのは', '0.22360377'], ['参道の梅が枝餅を焼く香り', '0.19296506'], ['熱々の梅ヶ枝餅を食べるのが', '0.18865052'], ['お守りも買えます長い参道に', '0.18257838'], ['道中も気になる店が多かった', '0.17767288']]\n"
     ]
    }
   ],
   "source": [
    "# with open('generated/sepa/hakataeki/content/save/output/output_2_haiku.txt') as f:\n",
    "#     s = f.readlines()\n",
    "\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\") #mecab neologd\n",
    "\n",
    "\n",
    "# text_list = []\n",
    "\n",
    "# for line in s:\n",
    "#     line = line.replace(\" \", \"\")\n",
    "#     text_list.append(line)\n",
    "\n",
    "# print(text_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "haiku_list = []\n",
    "for x,y in haiku_prob_list:\n",
    "    temp = x\n",
    "    # print(temp)\n",
    "\n",
    "    word_list = []\n",
    "\n",
    "    # 形態素解析をして、単語毎にリストに格納\n",
    "    for word in tagger.parse(temp).splitlines():\n",
    "        word = word.replace('\\t', ',')\n",
    "        # print(word)\n",
    "        temp_word_list = word.split(',')\n",
    "        word_list.append(temp_word_list)\n",
    "\n",
    "    # 文末の記号を削除\n",
    "    word_list.pop(-1)\n",
    "\n",
    "    # print(word_list)\n",
    "\n",
    "    # 格納された単語を繋げて17音の文字列を作る\n",
    "    for j in range(len(word_list)):\n",
    "\n",
    "        # 最初に空の文字列を用意\n",
    "        temp_haiku = \"\"\n",
    "        part = \"\"\n",
    "        # 次の単語を示すカウント\n",
    "        count = 0\n",
    "        # 音を数えるcount\n",
    "        haku_count = 0\n",
    "\n",
    "        # 17音を超えるまで単語を繋げる\n",
    "\n",
    "        if not ('名詞' == word_list[j+count][1] or '動詞' == word_list[j+count][1] or '形容詞' == word_list[j+count][1]):\n",
    "            continue\n",
    "        if ('名詞' ==  word_list[j+count][1] and ('非自立' == word_list[j+count][2] or '接尾' == word_list[j+count][2])):\n",
    "            continue\n",
    "        if ('動詞' ==  word_list[j+count][1] and '非自立' == word_list[j+count][2]):\n",
    "            continue\n",
    "\n",
    "        # 上の句5\n",
    "        while haku_count < 5:\n",
    "            # 単語を繋げる\n",
    "            part =part + word_list[j + count][0]\n",
    "            # 音を記録\n",
    "\n",
    "            if '記号' not in word_list[j+count][1]:\n",
    "                temp = word_list[j+count][-1]\n",
    "\n",
    "                # 小さい文字は音にならないので消去\n",
    "                # temp = temp.replace('ッ', '')\n",
    "                temp = temp.replace('ャ', '')\n",
    "                temp = temp.replace('ュ', '')\n",
    "                temp = temp.replace('ョ', '')\n",
    "                temp = temp.replace('ァ', '')\n",
    "                temp = temp.replace('\bィ', '')\n",
    "                temp = temp.replace('ゥ', '')\n",
    "                temp = temp.replace('ェ', '')\n",
    "                temp = temp.replace('ォ', '')\n",
    "\n",
    "                haku_count += len(temp)\n",
    "            # 元の文が終わっていたら終了\n",
    "            if j + count >= len(word_list)-1:\n",
    "                break\n",
    "            count += 1\n",
    "            # print(part, haku_count)\n",
    "        \n",
    "        if haku_count == 5:\n",
    "            temp_haiku += part\n",
    "            # temp_haiku += \"\\n\"\n",
    "            part = \"\"\n",
    "            haku_count = 0\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not ('名詞' == word_list[j+count][1] or '動詞' == word_list[j+count][1] or '形容詞' == word_list[j+count][1]):\n",
    "            continue\n",
    "        if ('名詞' ==  word_list[j+count][1] and ('非自立' == word_list[j+count][2] or '接尾' == word_list[j+count][2])):\n",
    "            continue\n",
    "        if ('動詞' ==  word_list[j+count][1] and '非自立' == word_list[j+count][2]):\n",
    "            continue\n",
    "\n",
    "        # 中の句7\n",
    "        while haku_count < 7:\n",
    "            # 単語を繋げる\n",
    "            part =part + word_list[j + count][0]\n",
    "            # 音を記録\n",
    "\n",
    "            if '記号' not in word_list[j+count][1]:\n",
    "                temp = word_list[j+count][-1]\n",
    "\n",
    "                # 小さい文字は音にならないので消去\n",
    "                # temp = temp.replace('ッ', '')\n",
    "                temp = temp.replace('ャ', '')\n",
    "                temp = temp.replace('ュ', '')\n",
    "                temp = temp.replace('ョ', '')\n",
    "                temp = temp.replace('ァ', '')\n",
    "                temp = temp.replace('\bィ', '')\n",
    "                temp = temp.replace('ゥ', '')\n",
    "                temp = temp.replace('ェ', '')\n",
    "                temp = temp.replace('ォ', '')\n",
    "\n",
    "                haku_count += len(temp)\n",
    "            # 元の文が終わっていたら終了\n",
    "            if j + count >= len(word_list)-1:\n",
    "                break\n",
    "            count += 1\n",
    "            # print(temp_haiku, haku_count)\n",
    "\n",
    "        \n",
    "        \n",
    "        if haku_count == 7:\n",
    "            temp_haiku += part\n",
    "            # temp_haiku += \"\\n\"\n",
    "            part = \"\"\n",
    "            haku_count = 0\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not ('名詞' == word_list[j+count][1] or '動詞' == word_list[j+count][1] or '形容詞' == word_list[j+count][1]):\n",
    "            continue\n",
    "        if ('名詞' ==  word_list[j+count][1] and ('非自立' == word_list[j+count][2] or '接尾' == word_list[j+count][2])):\n",
    "            continue\n",
    "        if ('動詞' ==  word_list[j+count][1] and '非自立' == word_list[j+count][2]):\n",
    "            continue\n",
    "\n",
    "        # 下の句5\n",
    "        while haku_count < 5:\n",
    "            # 単語を繋げる\n",
    "            part =part + word_list[j + count][0]\n",
    "            # 音を記録\n",
    "\n",
    "            if '記号' not in word_list[j+count][1]:\n",
    "                temp = word_list[j+count][-1]\n",
    "\n",
    "                # 小さい文字は音にならないので消去\n",
    "                # temp = temp.replace('ッ', '')\n",
    "                temp = temp.replace('ャ', '')\n",
    "                temp = temp.replace('ュ', '')\n",
    "                temp = temp.replace('ョ', '')\n",
    "                temp = temp.replace('ァ', '')\n",
    "                temp = temp.replace('\bィ', '')\n",
    "                temp = temp.replace('ゥ', '')\n",
    "                temp = temp.replace('ェ', '')\n",
    "                temp = temp.replace('ォ', '')\n",
    "\n",
    "                haku_count += len(temp)\n",
    "            # 元の文が終わっていたら終了\n",
    "            if j + count >= len(word_list)-1:\n",
    "                break\n",
    "            count += 1\n",
    "            # print(temp_haiku, haku_count)\n",
    "\n",
    "        \n",
    "        if haku_count == 5:\n",
    "            temp_haiku += part\n",
    "            # temp_haiku += \"\\n\"\n",
    "            # temp_haiku += \"\\n\"\n",
    "            part = \"\"\n",
    "            haku_count = 0\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if temp_haiku not in haiku_list:\n",
    "            haiku_list.append([temp_haiku,y])\n",
    "\n",
    "print(len(haiku_list))\n",
    "\n",
    "print(haiku_list[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スコア付け"
   ]
  },
  {
   "source": [
    "フィルタリングを行ったサンプルに対して、３つの手法でスコア付けを行う\n",
    "\n",
    "- tf-idfを用いたスコア付け　⇨　観光地の特徴度を表す\n",
    "- せqGANの識別器が算出したキャッチコピーに対する本物かどうかの確率を用いる　⇨　キャッチコピーの自然さを表す\n",
    "- 先の２つのスコアを組み合わせたスコア付け　⇨　キャッチコピーの全体的な完成度を表す"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# seqGANの識別器を用いたスコア付け"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "seqGANでは生成器が生成したキャッチコピーに対して、識別器がそれが本物かどうかの確率を算出する。\n",
    "\n",
    "この時、識別器が本物だと判断すると、生成器は識別器を騙すことができており、自然なキャッチコピーが生成できていると考えられる。\n",
    "\n",
    "したがって、識別器の確率は、キャッチコピーの自然さを表していると考えられる。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idfを用いたスコア付け"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idfを用いたスコア付けでは、サンプルに含まれる名詞に対するtf-idfの値の合計をキャッチコピーのスコアとする\n",
    "\n",
    "これはキャッチコピーが持つ観光地の特徴度合いを表していると考えられる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------データベース----------------------------------------\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "db_word_list = []\n",
    "\n",
    "conn = sqlite3.connect('data/tfidf_db.db')\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('select word from dazaifu')\n",
    "db_words = c.fetchall()\n",
    "\n",
    "for word in db_words:\n",
    "    db_word_list.append(word[0])\n",
    "\n",
    "# print(db_word_list)\n",
    "\n",
    "#-------------------------------------------------tfidfのスコア付け----------------------------------------\n",
    "score_list = []\n",
    "\n",
    "for i,_ in haiku_list:\n",
    "    temp = i\n",
    "    # print(temp)\n",
    "\n",
    "    word_list = []\n",
    "    score_sum = 0\n",
    "\n",
    "    # 形態素解析をして、単語毎にリストに格納\n",
    "    for word in tagger.parse(temp).splitlines():\n",
    "        word = word.replace('\\t', ',')\n",
    "        temp_word_list = word.split(',')\n",
    "        word_list.append(temp_word_list)\n",
    "\n",
    "    # 文末の記号を削除\n",
    "    word_list.pop(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    for j in range(len(word_list)):\n",
    "        # print(word_list[j][0])\n",
    "\n",
    "        temp_score = 0\n",
    "\n",
    "        if '名詞' not in word_list[j][1]:\n",
    "            continue\n",
    "\n",
    "        # 形態素解析した名詞がdbに含まれているならtfidfの値をスコアとして加算していく\n",
    "        if word_list[j][0] in db_word_list:\n",
    "            c.execute('select tfidf from dazaifu where word = ?', (word_list[j][0],))\n",
    "            tfidf = c.fetchone()[0]\n",
    "            temp_score = tfidf\n",
    "        \n",
    "        score_sum += temp_score\n",
    "\n",
    "    score_list.append(score_sum)\n",
    "\n",
    "        \n",
    "\n",
    "# print(score_list)\n",
    "\n",
    "#-------------------------------------------------データベース----------------------------------------\n",
    "\n",
    "#閉じる\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "生成したキャッチコピー数： 156\n\n生成例\nキャッチコピー: 参道も天満宮も参拝者  \t\t 識別器の確率: 0.43593347 \t\t tf-idf値: 0.08266877790933559\nキャッチコピー: 参道の梅が枝餅を購入し  \t\t 識別器の確率: 0.42193896 \t\t tf-idf値: 0.07427657739892096\nキャッチコピー: 境内の梅が枝餅を焼く自分  \t\t 識別器の確率: 0.31326932 \t\t tf-idf値: 0.03896579286478265\nキャッチコピー: 名物のクルーズ船が入ったと  \t\t 識別器の確率: 0.26894203 \t\t tf-idf値: 0.0012811665595249965\nキャッチコピー: 参道を梅ヶ枝餅を食べながら  \t\t 識別器の確率: 0.23119384 \t\t tf-idf値: 0.08059833969157103\nキャッチコピー: 焼きもちを焼くので橋を渡るのは  \t\t 識別器の確率: 0.22360377 \t\t tf-idf値: 0.003547983760713978\nキャッチコピー: 参道の梅が枝餅を焼く香り  \t\t 識別器の確率: 0.19296506 \t\t tf-idf値: 0.07442992826495887\nキャッチコピー: 熱々の梅ヶ枝餅を食べるのが  \t\t 識別器の確率: 0.18865052 \t\t tf-idf値: 0.03006349179171362\nキャッチコピー: お守りも買えます長い参道に  \t\t 識別器の確率: 0.18257838 \t\t tf-idf値: 0.0631439911669297\nキャッチコピー: 道中も気になる店が多かった  \t\t 識別器の確率: 0.17767288 \t\t tf-idf値: 0.00022315096260099208\n"
     ]
    }
   ],
   "source": [
    "# 俳句とtfidfのスコアをまとめたリストを作成\n",
    "\n",
    "for i in range(len(haiku_list)):\n",
    "    haiku_list[i].append(score_list[i])\n",
    "\n",
    "print(\"生成したキャッチコピー数：\",len(haiku_list))\n",
    "print(\"\")\n",
    "print(\"生成例\")\n",
    "for n in haiku_list[:10]:\n",
    "    print(\"キャッチコピー:\", n[0], \" \\t\\t\",\"識別器の確率:\",n[1], \"\\t\\t\", \"tf-idf値:\",n[2])\n",
    "\n",
    "# print(haiku_list)\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open('data/generated/sepa/dazaifu/content/save/output/haiku_dis_tfidf.pickle', 'wb') as f:\n",
    "#     pickle.dump(haiku_list, f)"
   ]
  },
  {
   "source": [
    "# 識別器の確率とtfidfを組み合わせた総合的なスコア付け"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "識別器の確率とtf-idfの値を組み合わせることで、キャッチコピーの総合的な完成度を表す。\n",
    "\n",
    "組み合わせには、正規化した両方の値の調和平均を用いた。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "haiku_score_list = haiku_list\n",
    "\n",
    "haiku_list_fin = []\n",
    "tfidf_list = []\n",
    "dis_list = []\n",
    "\n",
    "for i in haiku_score_list:\n",
    "    haiku_list_fin.append(i[0])\n",
    "    dis_list.append(float(i[1]))\n",
    "    tfidf_list.append(float(i[2]))\n",
    "\n",
    "def min_max(l):\n",
    "    l_min = min(l)\n",
    "    l_max = max(l)\n",
    "    return [(i - l_min) / (l_max - l_min) for i in l]\n",
    "\n",
    "def standardization(l):\n",
    "    l_mean = statistics.mean(l)\n",
    "    l_stdev = statistics.stdev(l)\n",
    "    return [(i - l_mean) / l_stdev for i in l]\n",
    "\n",
    "# 標準化\n",
    "# tfidf_list_std = standardization(tfidf_list)\n",
    "# dis_list_std = standardization(dis_list)\n",
    "# print(tfidf_list_std)\n",
    "\n",
    "# 調和平均では値がマイナスだと困るため正規化を用いる\n",
    "tfidf_list_std = min_max(tfidf_list)\n",
    "dis_list_std = min_max(dis_list)\n",
    "# print(tfidf_list_std)"
   ]
  },
  {
   "source": [
    "調和平均を求める"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list = [0]*len(tfidf_list_std)\n",
    "\n",
    "# 調和平均を計算\n",
    "# 識別器の確率かtfidfのどちらかが0の場合は、調和平均を0にする\n",
    "for i in range(len(tfidf_list_std)):\n",
    "    if tfidf_list_std[i] == 0 or dis_list_std[i] == 0:\n",
    "        sum_list[i] = 0\n",
    "    else:\n",
    "        sum_list[i] = 2/(1/tfidf_list_std[i] + 1/dis_list_std[i])\n",
    "\n",
    "# print(sum_list)"
   ]
  },
  {
   "source": [
    "生成例とそれに対するスコアを表示\n",
    "\n",
    "識別器の確率とtfidfの値は、正規化されているため0~1で表示される"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "生成例: 参道も天満宮も参拝者 \t 識別器: 1.0 \t tf-idf: 1.0 \t 調和平均: 1.0\n生成例: 参道の梅が枝餅を購入し \t 識別器: 0.9675870764820672 \t tf-idf: 0.89848403807761 \t 調和平均: 0.9317560696227312\n生成例: 境内の梅が枝餅を焼く自分 \t 識別器: 0.7158954681269851 \t tf-idf: 0.4713483596856502 \t 調和平均: 0.5684361488401817\n生成例: 名物のクルーズ船が入ったと \t 識別器: 0.6132282750257457 \t tf-idf: 0.015497586778530536 \t 調和平均: 0.030231167459812757\n生成例: 参道を梅ヶ枝餅を食べながら \t 識別器: 0.5257990477494502 \t tf-idf: 0.974955016027511 \t 調和平均: 0.683163792654572\n生成例: 焼きもちを焼くので橋を渡るのは \t 識別器: 0.5082195570487571 \t tf-idf: 0.04291806230164814 \t 調和平均: 0.0791519135929894\n生成例: 参道の梅が枝餅を焼く香り \t 識別器: 0.4372567177697741 \t tf-idf: 0.9003390415011042 \t 調和平均: 0.588637174479906\n生成例: 熱々の梅ヶ枝餅を食べるのが \t 識別器: 0.4272637380202108 \t tf-idf: 0.3636619840284178 \t 調和平均: 0.39290561513000016\n生成例: お守りも買えます長い参道に \t 識別器: 0.4131999509058224 \t tf-idf: 0.763819095477387 \t 調和平均: 0.5362870103453379\n生成例: 道中も気になる店が多かった \t 識別器: 0.4018382386117 \t tf-idf: 0.002699337866657784 \t 調和平均: 0.00536265225692159\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "for i in range(len(sum_list)):\n",
    "    temp = [haiku_list_fin[i],dis_list_std[i],tfidf_list_std[i],sum_list[i]]\n",
    "    score_list.append(temp)\n",
    "\n",
    "for n in score_list[:10]:\n",
    "    print(\"生成例:\", n[0], \"\\t\", \"識別器:\", n[1], \"\\t\", \"tf-idf:\", n[2], \"\\t\", \"調和平均:\", n[3])\n",
    "\n",
    "# スコアを保存\n",
    "import pickle\n",
    "\n",
    "with open('data/generated/dazaifu/score/score.pickle', 'wb') as f:\n",
    "    pickle.dump(score_list, f)"
   ]
  },
  {
   "source": [
    "# 各スコアでの上位のキャッチコピー"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "各スコア付けでの上位１０件のキャッチコピーを表示"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/generated/dazaifu/score/score.pickle', 'rb') as f:\n",
    "    score_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "識別器の確率によるスコアでの上位のキャッチコピー\n\n生成例: 参道も天満宮も参拝者 \t 識別器: 1.0\n生成例: 参道の梅が枝餅を購入し \t 識別器: 0.9675870764820672\n生成例: 境内の梅が枝餅を焼く自分 \t 識別器: 0.7158954681269851\n生成例: 名物のクルーズ船が入ったと \t 識別器: 0.6132282750257457\n生成例: 参道を梅ヶ枝餅を食べながら \t 識別器: 0.5257990477494502\n生成例: 焼きもちを焼くので橋を渡るのは \t 識別器: 0.5082195570487571\n生成例: 参道の梅が枝餅を焼く香り \t 識別器: 0.4372567177697741\n生成例: 熱々の梅ヶ枝餅を食べるのが \t 識別器: 0.4272637380202108\n生成例: お守りも買えます長い参道に \t 識別器: 0.4131999509058224\n生成例: 道中も気になる店が多かった \t 識別器: 0.4018382386117\n"
     ]
    }
   ],
   "source": [
    "# 識別器の確率によるスコアでの上位のキャッチコピー\n",
    "\n",
    "score_list_10 = sorted(score_list, key=lambda x: x[1])\n",
    "score_list_10.reverse()\n",
    "# score_list_10.pop(4)\n",
    "score_list_10 = score_list_10[:10]\n",
    "\n",
    "print(\"識別器の確率によるスコアでの上位のキャッチコピー\")\n",
    "print(\"\")\n",
    "for n in score_list_10:\n",
    "    print(\"生成例:\", n[0], \"\\t\", \"識別器:\", n[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tfidfによるスコアでの上位のキャッチコピー\n\n生成例: 参道も天満宮も参拝者 \t tfidf: 1.0\n生成例: 参道で梅ヶ枝餅を食べながら \t tfidf: 0.974955016027511\n生成例: 参道の梅ヶ枝餅を食べながら \t tfidf: 0.974955016027511\n生成例: 参道を梅ヶ枝餅を食べながら \t tfidf: 0.974955016027511\n生成例: 参道を楽しみながらお参りし \t tfidf: 0.9171769437986969\n生成例: 参道を楽しみながらお参りを \t tfidf: 0.9171769437986969\n生成例: 参道を楽しみながらお参りを \t tfidf: 0.9171769437986969\n生成例: 参道を楽しみながらお参りし \t tfidf: 0.9171769437986969\n生成例: 参道を楽しみながらお参りに \t tfidf: 0.9171769437986969\n生成例: 参道の梅が枝餅を焼く香り \t tfidf: 0.9003390415011042\n"
     ]
    }
   ],
   "source": [
    "# tfidfによるスコアでの上位のキャッチコピー\n",
    "\n",
    "score_list_10 = sorted(score_list, key=lambda x: x[2])\n",
    "score_list_10.reverse()\n",
    "# score_list_10.pop(4)\n",
    "score_list_10 = score_list_10[:10]\n",
    "\n",
    "print(\"tfidfによるスコアでの上位のキャッチコピー\")\n",
    "print(\"\")\n",
    "for n in score_list_10:\n",
    "    print(\"生成例:\", n[0], \"\\t\", \"tfidf:\", n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "調和平均によるスコアでの上位のキャッチコピー\n\n生成例: 参道も天満宮も参拝者 \t 調和平均: 1.0\n生成例: 参道の梅が枝餅を購入し \t 調和平均: 0.9317560696227312\n生成例: 参道を梅ヶ枝餅を食べながら \t 調和平均: 0.683163792654572\n生成例: 参道の梅が枝餅を焼く香り \t 調和平均: 0.588637174479906\n生成例: 境内の梅が枝餅を焼く自分 \t 調和平均: 0.5684361488401817\n生成例: お守りも買えます長い参道に \t 調和平均: 0.5362870103453379\n生成例: 熱々の梅ヶ枝餅を食べるのが \t 調和平均: 0.39290561513000016\n生成例: 参道の土産物屋を見物し \t 調和平均: 0.38137167720309284\n生成例: 焼きたての梅が枝餅を食べながら \t 調和平均: 0.3368964003806297\n生成例: 複数の梅ヶ枝餅を食べ比べ \t 調和平均: 0.313192453060879\n"
     ]
    }
   ],
   "source": [
    "# 調和平均によるスコアでの上位のキャッチコピー\n",
    "\n",
    "score_list_10 = sorted(score_list, key=lambda x: x[3])\n",
    "score_list_10.reverse()\n",
    "# score_list_10.pop(4)\n",
    "score_list_10 = score_list_10[:10]\n",
    "\n",
    "print(\"調和平均によるスコアでの上位のキャッチコピー\")\n",
    "print(\"\")\n",
    "for n in score_list_10:\n",
    "    print(\"生成例:\", n[0], \"\\t\", \"調和平均:\", n[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python387jvsc74a57bd09f1e5f8df66e0355b93d6a73e8e18cceb2fedad000b7b1dd4a514e55097c6a9d",
   "display_name": "Python 3.8.7 64-bit ('3.8.7')"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.7-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "9f1e5f8df66e0355b93d6a73e8e18cceb2fedad000b7b1dd4a514e55097c6a9d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}