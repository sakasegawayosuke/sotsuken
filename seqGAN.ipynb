{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQw0EF5av_D_"
   },
   "source": [
    "# seqGAN\n",
    "ここでは、make_datasetで準備した学習データを用いて、seqGANによるキャッチコピーの生成を行う\n",
    "\n",
    "google colabにて実行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjPzDI2fv_EH"
   },
   "source": [
    "参考サイト（https://qiita.com/everylittle/items/19c4988a135d36150dc0）\n",
    "\n",
    "こちらの方のコードにほぼ乗っかる形で、ところどころで自分のデータに合わせながら作成しました。\n",
    "\n",
    "とてもお世話になりました。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWYN51zIv_EH"
   },
   "source": [
    "# seqGANのコード一覧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWlrTpVYv_EI"
   },
   "source": [
    "generator.py ：生成器について\n",
    "\n",
    "discriminator.py：識別器について\n",
    "\n",
    "dataloader.py：入力データを読み込んでリストを作る\n",
    "\n",
    "rnnlm.py：生成器で使用するlstmの雛形を作ってる感じ、ワンバッチとか\n",
    "\n",
    "rollout.py：生成器と識別器のパラメータを調整している\n",
    "\n",
    "target_lstm.py：lstmのモデルを作っている\n",
    "\n",
    "utils.py：ボキャブラリーの作成、生成器の事前学習、識別器と生成器の掛け合いについて、正直一番よくわかっていない部分\n",
    "\n",
    "sequence_gan.py：メイン関数がある、全体の統括、ここでパラメータをいじればいいって書いてあった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCB-X9TCv_EI"
   },
   "source": [
    "# それぞれのコードの中身について調べてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aysGHjUov_EI"
   },
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込む部分\n",
    "\n",
    "今回は、観光地レビューの575文字列をid化したものを読み込ませる\n",
    "\n",
    "データセットは生成器用と識別器用のふたつを作る\n",
    "\n",
    "識別器のデータセットでは正例と負例を区別するためにラベル付けが行われる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqLks309v_EJ"
   },
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9AUckZpv_EJ"
   },
   "outputs": [],
   "source": [
    "#生成器のデータセットを作成する\n",
    "\n",
    "def dataset_for_generator(data_file, batch_size):\n",
    "    dataset = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        # 一行ずつ読み取る\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            parse_line = [int(x) for x in line]\n",
    "            if len(parse_line) == 15: # 一行で15単語\n",
    "                dataset.append(parse_line)\n",
    "    output = tf.data.Dataset.from_tensor_slices(dataset).shuffle(len(dataset)).batch(batch_size)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo052Q_0v_EK"
   },
   "source": [
    "バッチサイズの仕組み\n",
    "\n",
    "tf.data.Dataset.batch(10)ってやると、データをバッチサイズごとに分割してくれる\n",
    "\n",
    "例えば、100個分のデータがあったとして、そのままだと、二次元配列に一つだけのデータ\u001c\n",
    "(１００個)が入った状態になる。\n",
    "\n",
    "[[１００個のデータ]]\n",
    "\n",
    "これを.batch(10)ってすると、\n",
    "\n",
    "[[１０個のデータ]]\n",
    "\n",
    "[[１０個のデータ]]\n",
    "\n",
    "...\n",
    "\n",
    "[[１０個のデータ]]\n",
    "\n",
    "みたいに、100/10　個に分けてデータを扱うことができるようになる。　\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcw0l4p0v_EK"
   },
   "outputs": [],
   "source": [
    "#識別器のデータセットを作成\n",
    "\n",
    "def dataset_for_discriminator(positive_file, negative_file, batch_size):\n",
    "\n",
    "    examples = [] # データを入れる配列\n",
    "    labels = [] # データがtrueかfalseかのラベルを付ける\n",
    "\n",
    "    with open(positive_file) as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            parse_line = [int(x) for x in line]\n",
    "            if len(parse_line) == 15:\n",
    "                examples.append(parse_line)\n",
    "                labels.append([0, 1])\n",
    "\n",
    "    with open(negative_file) as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            parse_line = [int(x) for x in line]\n",
    "            if len(parse_line) == 15:\n",
    "                examples.append(parse_line)\n",
    "                labels.append([1, 0])\n",
    "    output = tf.data.Dataset.from_tensor_slices((examples, labels)).shuffle(len(examples)).batch(batch_size).repeat(6)\n",
    "    return output\n",
    "\n",
    "# tf.data.Datasetの形式は配列ではないのでそのままでは内容が確認できない\n",
    "# for文とかnumpy()を使えば確認できるらしい\n",
    "# もしくはデータの形式を変えたら確認できるのかな\n",
    "\n",
    "# listの形式でいけたわ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVOuz77_v_EM"
   },
   "source": [
    "# rnnlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成器のモデルを作成している\n",
    "\n",
    "のちのtarget_lstmではここで作ったクラスを継承している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vKmhbuHv_EM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Flatten\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu5xnh-Mv_EN"
   },
   "outputs": [],
   "source": [
    "class RNNLM(object):\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, learning_rate=0.01):\n",
    "        self.num_emb = num_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.start_token_vec = tf.constant([start_token] * self.batch_size, dtype=tf.int32)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = 5.0\n",
    "\n",
    "        # 生成器のモデルを作る\n",
    "        # 入力サイズ(Input)は固定の単語数\n",
    "        # モデルの構造は、　input embedding LSTM Dense って流れ\n",
    "\n",
    "        self.g_model = tf.keras.models.Sequential([\n",
    "            Input((self.sequence_length,), dtype=tf.int32),\n",
    "            Embedding(self.num_emb, self.emb_dim, embeddings_initializer=tf.random_normal_initializer(stddev=0.1)),\n",
    "            LSTM(self.hidden_dim, kernel_initializer=tf.random_normal_initializer(stddev=0.1), recurrent_initializer=tf.random_normal_initializer(stddev=0.1), return_sequences=True),\n",
    "            Dense(self.num_emb, kernel_initializer=tf.random_normal_initializer(stddev=0.1), activation=\"softmax\")\n",
    "        ])\n",
    "\n",
    "        # optimizer(最適化関数)を自分で作ってる\n",
    "        # 最適化関数　→　損失関数をどの方向にどのくらい更新するかを決める\n",
    "        # learing_rate →　どの程度更新するか\n",
    "        # 一回の学習で更新しすぎないように制限するパラメータも存在する\n",
    "        # clipnorm →　ベクトルの長さ\n",
    "        # clipvalue →　絶対値\n",
    "\n",
    "        # 損失関数　→　sparse_categorical_crossentropy\n",
    "        # これは、クラス分類に使われる損失関数\n",
    "        # categorical_crossentropyだと、one-hotベクトルでの分類を行うが、sparse_categorical_crossentropyでは整数で分類可能\n",
    "        # つまり[001][010][100]って分類か[0][1][2]って分類か\n",
    "        # lstmだと次に来る単語の候補の確率から、どの候補かを分類してるって感じなのかな\n",
    "        \n",
    "\n",
    "        self.g_optimizer = self._create_optimizer(learning_rate, clipnorm=self.grad_clip)\n",
    "        if self.g_optimizer is not None:\n",
    "            self.g_model.compile(\n",
    "                optimizer=self.g_optimizer,\n",
    "                loss=\"sparse_categorical_crossentropy\")\n",
    "        else:\n",
    "            self.g_model.compile(\n",
    "                loss=\"sparse_categorical_crossentropy\")\n",
    "        self.g_embeddings = self.g_model.trainable_weights[0]\n",
    "\n",
    "\n",
    "    # データセットを入れて、損失を出力する\n",
    "    def target_loss(self, dataset):\n",
    "        # dataset: each element has [self.batch_size, self.sequence_length]\n",
    "        # outputs are 1 timestep ahead\n",
    "        ds = dataset.map(lambda x: (tf.pad(x[:, 0:-1], ([0, 0], [1, 0]), \"CONSTANT\", self.start_token), x))\n",
    "        loss = self.g_model.evaluate(ds, verbose=1)\n",
    "\n",
    "        # print(\"\\n\")\n",
    "        # print(\"生成したデータセットから損失を計算しました\")\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @tf.function # これを付けると、これ以降は処理速度が早くなるらしい\n",
    "\n",
    "    # バッチサイズひとつ分だけを生成する場合\n",
    "    def generate_one_batch(self):\n",
    "        # パラメータの初期設定\n",
    "        # h0とc0は隠れ層でのパラメータ\n",
    "        # gen_xは\n",
    "        h0 = c0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
    "        gen_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
    "                               dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        def _g_recurrence(i, x_t, h_tm1, gen_x):\n",
    "            # o_t: batch x vocab, probability\n",
    "            # h_t: hidden_memory_tuple\n",
    "            o_t, h_t = self.g_model.layers[1].cell(x_t, h_tm1, training=False) # layers[1]: LSTM\n",
    "            o_t = self.g_model.layers[2](o_t) # layers[2]: Dense\n",
    "            log_prob = tf.math.log(o_t)\n",
    "            next_token = tf.cast(tf.reshape(tf.random.categorical(log_prob, 1), [self.batch_size]), tf.int32)\n",
    "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
    "            return i + 1, x_tp1, h_t, gen_x\n",
    "\n",
    "        # while_loop\n",
    "        # cond →　ループの条件\n",
    "        # body →　状態の更新\n",
    "        # loop_vars →　初期状態\n",
    "        _, _, _, gen_x = tf.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
    "            body=_g_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token_vec), [h0, c0], gen_x))\n",
    "\n",
    "        gen_x = gen_x.stack()  # seq_length x batch_size\n",
    "        outputs = tf.transpose(gen_x, perm=[1, 0])  # batch_size x seq_length\n",
    "        return outputs\n",
    "\n",
    "    # 生成を実行\n",
    "    def generate_samples(self, num_batches, output_file):\n",
    "        # Generate Samples\n",
    "        with open(output_file, 'w') as fout:\n",
    "            for _ in range(num_batches):\n",
    "                generated_samples = self.generate_one_batch().numpy()\n",
    "\n",
    "                # print(\"\\n\")\n",
    "                # print(\"生成結果 : \",generated_samples) # 生成結果を表示\n",
    "\n",
    "                # generated_samplesはバッチサイズ個作られているので、それぞれをfor文で回して出力する\n",
    "                for poem in generated_samples:\n",
    "                    print(' '.join([str(x) for x in poem]), file=fout)\n",
    "\n",
    "    # __init__で使われてる\n",
    "    def _create_optimizer(self, *args, **kwargs):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlTEODRPv_EO"
   },
   "source": [
    "# target_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstmの層のサイズを設定している\n",
    "\n",
    "元の論文にてtarget_params.pklという既に調整されたパラメータのファイルを読み込む形になっていたが、今回は使用しないため、入力サイズを自分で調整する必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GRot2Rtv_EO"
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LJwdFTRv_EP"
   },
   "outputs": [],
   "source": [
    "# lstmのパラメータが入っているpickleファイルを読み取る時に使う　→　今回は使わない\n",
    "# モデルのサイズはパラメータファイルで決められるよ\n",
    "# パラメータファイルと自分のデータのファイルで違うので自分のファイルに合わせるように変数を調整する\n",
    "\n",
    "class TARGET_LSTM(RNNLM):\n",
    "    # function for reading save/target_params.pkl\n",
    "    \n",
    "    # voc_sizeを追加 num_embの値をvoc_sizeに変更\n",
    "    def __init__(self, batch_size, sequence_length, start_token, params, voc_size):\n",
    "\n",
    "#         num_emb = params[0].shape[0]\n",
    "        num_emb = voc_size\n",
    "        emb_dim = params[0].shape[1]\n",
    "        hidden_dim = params[1].shape[1]\n",
    "        \n",
    "        param_0 = np.zeros((num_emb, emb_dim)) # 作成\n",
    "        param_13 = np.zeros((emb_dim, num_emb)) # 作成\n",
    "        param_14 = np.zeros(num_emb) # 作成\n",
    "\n",
    "        super(TARGET_LSTM, self).__init__(num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token)\n",
    "        weights = [\n",
    "            # Embedding\n",
    "            param_0,\n",
    "            # LSTM\n",
    "            np.c_[params[1], params[4], params[10], params[7]], # kernel (i, f, c, o)\n",
    "            np.c_[params[2], params[5], params[11], params[8]], # recurrent_kernel\n",
    "            np.r_[params[3], params[6], params[12], params[9]], # bias\n",
    "            # Dense\n",
    "            param_13,\n",
    "            param_14\n",
    "        ]\n",
    "        self.g_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxv4ogz1v_EP"
   },
   "source": [
    "# generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seqGANでは、生成器は事前学習と敵対学習の２つのステップがある。\n",
    "\n",
    "まず、事前学習にてある程度のキャッチコピーが作れるようになった上で、敵対学習を実施する。\n",
    "\n",
    "どの程度の事前学習を行うかは、後のsequence_ganにて調整する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXI5YFpZv_EP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSoBUbP-v_EQ"
   },
   "outputs": [],
   "source": [
    "# さっきのtarget_lstmと今回のgeneratorはrnnlmで構築したモデルを利用している\n",
    "# ので、わからない関数があったときはrnnlmを見返すと良い\n",
    "# 関数の中に別の関数があってその中にまた別の関数があるので、理解がちょっと難しい\n",
    "\n",
    "\n",
    "class Generator(RNNLM):\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, learning_rate=0.01):\n",
    "        super(Generator, self).__init__(num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, learning_rate)\n",
    "\n",
    "        # prepare model for GAN training\n",
    "        # rnnlmでもしたけど、またコンパイルをしている　→　別物じゃん(g_modelとg_model_temporal)\n",
    "        # sample_weight_modeってのが増えたのかな\n",
    "\n",
    "        # sample_weight →　入力サンプルと同じ長さのnumpy配列で、訓練のサンプルに対する重みを格納する\n",
    "        # これは損失関数をスケーリングするために、訓練中だけ使用する\n",
    "        # あるいは系列データの場合において，2次元配列の(samples, sequence_length)という形式で， \n",
    "        # すべてのサンプルの各時間において異なる重みを適用できます． \n",
    "        # この場合，compile()の中でsample_weight_mode=\"temporal\"と確実に明記すべき\n",
    "\n",
    "        self.g_model_temporal = tf.keras.models.Sequential(self.g_model.layers)\n",
    "        self.g_optimizer_temporal = self._create_optimizer(\n",
    "            learning_rate, clipnorm=self.grad_clip)\n",
    "        self.g_model_temporal.compile(\n",
    "            optimizer=self.g_optimizer_temporal,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            sample_weight_mode=\"temporal\")\n",
    "\n",
    "    # 事前学習にて実行\n",
    "\n",
    "    def pretrain(self, dataset, target_lstm, num_epochs, num_steps, eval_file):\n",
    "        # dataset: each element has [self.batch_size, self.sequence_length]\n",
    "        # outputs are 1 timestep ahead\n",
    "\n",
    "        # 5エポックごとに損失を求めている\n",
    "        def pretrain_callback(epoch, logs):\n",
    "            if epoch % 5 == 0:\n",
    "                self.generate_samples(num_steps, eval_file) # ここで生成\n",
    "                likelihood_dataset = dataset_for_generator(eval_file, self.batch_size) # データセットはここで作成\n",
    "\n",
    "                # print(\"損失に用いたデータセットを表示\")  \n",
    "                # for i in likelihood_dataset:\n",
    "                #     print(i.numpy())\n",
    "\n",
    "                test_loss = target_lstm.target_loss(likelihood_dataset) # 作成したデータセットの損失を求める\n",
    "\n",
    "\n",
    "                # print('pre-train epoch ', epoch, 'test_loss ', test_loss) # 損失を表示する\n",
    "                buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "                log.write(buffer)\n",
    "\n",
    "        # データセットを作成\n",
    "        # データの先頭にstart_tokenの数字を付ける処理を行なっている\n",
    "        # start_tokenが0だったため、paddingの0と被っていたのが問題かも →　start_tokenの値を変える or paddingの値を変える\n",
    "        ds = dataset.map(lambda x: (tf.pad(x[:, 0:-1], ([0, 0], [1, 0]), \"CONSTANT\", self.start_token), x)).repeat(num_epochs)\n",
    "\n",
    "        # 生成器のモデルの学習を実施\n",
    "        # callbacks →　訓練中のモデルの状態を可視化するために使われる\n",
    "        # model.fitの中で使われるよ\n",
    "        # tf.keras.callbacks.LambdaCallback →　シンプルな自作コールバックを作れる\n",
    "        # 今回のon_epoch_endではepochとlogの二つの位置引数が必要(他にも色々ある模様)\n",
    "\n",
    "\n",
    "\n",
    "        # pretrain_loss = self.g_model.fit(ds, verbose=1, epochs=num_epochs, steps_per_epoch=num_steps,\n",
    "        #                                  callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=pretrain_callback)])\n",
    "        \n",
    "        pretrain_loss = self.g_model.fit(ds, verbose=1, epochs=num_epochs, steps_per_epoch=num_steps) # callbackなしでちょっと様子を見る\n",
    "        return pretrain_loss\n",
    "\n",
    "    def train_step(self, x, rewards):\n",
    "        # x: [self.batch_size, self.sequence_length]\n",
    "        # rewards: [self.batch_size, self.sequence_length] (sample_weight)\n",
    "        # outputs are 1 timestep ahead\n",
    "\n",
    "        # __init__でコンパイルしてたモデル\n",
    "        # train_on_batch　→　サンプル中の一つのバッチで勾配を更新する\n",
    "        # やってることは.fit()と.train_on_batch()で同じなのかも(違いは、使うバッチが一つか複数か)\n",
    "        # sample_weight →　サンプルの重み、numpy配列\n",
    "\n",
    "        train_loss = self.g_model_temporal.train_on_batch(\n",
    "            np.pad(x[:, 0:-1], ([0, 0], [1, 0]), \"constant\", constant_values=self.start_token), x,\n",
    "            # sparse_categorical_crossentropy returns mean loss\n",
    "            # here we multiply (batch_size * sequence_length) to use weighted \"sum\"\n",
    "            sample_weight=rewards * self.batch_size * self.sequence_length)\n",
    "        return train_loss\n",
    "\n",
    "    def _create_optimizer(self, *args, **kwargs):\n",
    "        return tf.keras.optimizers.Adam(*args, **kwargs)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.g_model.save_weights(filename, save_format=\"h5\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.g_model.load_weights(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行時に表示されるETA は estimated time of arrival の略で、1エポックあたりのトレーニングにかかる時間の予測のこと。\n",
    "\n",
    "エポック内の処理の進捗と残りのデータ量を使ってkerasが自動で予測して出力する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhCgdNyIv_EY"
   },
   "source": [
    "# discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "識別器も生成器と同様に、事前学習と敵対学習の２ステップを行う。\n",
    "\n",
    "また、今回は生成器と識別器で扱うデータの種類が異なっている\n",
    "\n",
    "- 生成器は事前学習にて、全ての観光地の575が与えられる　⇨　語彙や表現の幅を増やす\n",
    "- 識別器は正解データ（正例）に対象の観光地の575が与えられる　⇨　生成器は識別器に与えられた観光地の575に似せるように生成を行うため、目的の観光地にあったキャッチコピーが生成される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OS5ucB_kv_EZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Embedding, Conv1D, MaxPool1D, Concatenate, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SpDIHxwv_EZ"
   },
   "outputs": [],
   "source": [
    "# どこで使っているのか\n",
    "\n",
    "class Highway(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Highway, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        output_dim = input_shape[-1]\n",
    "        self.dense_g = Dense(output_dim, activation=\"relu\")\n",
    "        self.dense_t = Dense(output_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        g = self.dense_g(input_tensor, training=training)\n",
    "        t = self.dense_t(input_tensor, training=training)\n",
    "        o = t * g + (1. - t) * input_tensor\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXtgIX2ov_EZ"
   },
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, sequence_length, num_classes, vocab_size,\n",
    "            embedding_size, filter_sizes, num_filters, dropout_keep_prob, l2_reg_lambda=0.2):\n",
    "      \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        layer_input = Input((sequence_length,), dtype=tf.int32)\n",
    "        layer_emb = Embedding(vocab_size, embedding_size, embeddings_initializer=tf.random_uniform_initializer(-1.0, 1.0))(layer_input)\n",
    "        # (None, sequence_length, embedding_size)\n",
    "\n",
    "\n",
    "        # filter_sizes →　畳み込みのフィルターの大きさ(カーネルサイズのこと)\n",
    "        # num_filters →　フィルターの数(畳み込みにおける出力フィルタの数)\n",
    "        pooled_outputs = []\n",
    "        for filter_size, num_filter in zip(filter_sizes, num_filters):\n",
    "            x = Conv1D(num_filter, filter_size)(layer_emb) # (None, sequence_length - filter_size + 1, num_filter)\n",
    "            x = MaxPool1D(sequence_length - filter_size + 1)(x) # (None, 1, num_filter)\n",
    "            pooled_outputs.append(x)\n",
    "\n",
    "        x = Concatenate()(pooled_outputs)\n",
    "        x = Flatten()(x) # (None, sum(num_filters))\n",
    "        x = Highway()(x)\n",
    "        x = Dropout(1.0 - dropout_keep_prob)(x)\n",
    "        layer_output = Dense(num_classes,\n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(l2_reg_lambda),\n",
    "                             bias_regularizer=tf.keras.regularizers.l2(l2_reg_lambda),\n",
    "                             activation=\"softmax\")(x)\n",
    "\n",
    "        self.d_model = Model(layer_input, layer_output)\n",
    "        d_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        self.d_model.compile(optimizer=d_optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    def train(self, dataset, num_epochs, num_steps, **kwargs):\n",
    "        # dataset: ([None, sequence_length], [None, num_classes])\n",
    "        return self.d_model.fit(dataset.repeat(num_epochs), verbose=1, epochs=num_epochs, steps_per_epoch=num_steps, **kwargs)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.d_model.save_weights(filename, save_format=\"h5\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.d_model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISvYb9_8v_Eb"
   },
   "source": [
    "# rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "敵対学習にてパラメータの調整を行う部分\n",
    "\n",
    "また、生成器に対する強化学習の手法によるパラメータの更新の部分のここで設定している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DflKvAZNv_Eb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dytrx_K2v_Eb"
   },
   "outputs": [],
   "source": [
    "class ROLLOUT(RNNLM):\n",
    "    def __init__(self, lstm, update_rate):\n",
    "        super(ROLLOUT, self).__init__(lstm.num_emb, lstm.batch_size, lstm.emb_dim, lstm.hidden_dim, lstm.sequence_length, lstm.start_token)\n",
    "\n",
    "        # generatorのモデルをlstmとして読み込む\n",
    "        # update_rate = 0.8がデフォルトの設定\n",
    "        # .get_weights()でモデルの重みを取得して、set_weightsで別のモデルの重みとして設定\n",
    "        self.lstm = lstm\n",
    "        self.update_rate = update_rate\n",
    "        self.g_model.set_weights(lstm.g_model.get_weights())\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_batch(self, x_orig, given_num):\n",
    "        # Initial states\n",
    "        h0 = c0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
    "        h0 = [h0, c0]\n",
    "\n",
    "        # tf.transpose →　元の行列をpermで指定した順番に変える\n",
    "        # 今回はperm=[1, 0, 2] なので、index[0] →　index[1]になり、index[1] →　index[0]になる\n",
    "        processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, x_orig), perm=[1, 0, 2])  # seq_length x batch_size x emb_dim\n",
    "\n",
    "        # tf.TensorArray →　Tensor Arrayを生成する。多分配列みたいなものだと思う\n",
    "        # size →　tensorarrayのサイズ\n",
    "        # dynamic_size →　sizeを動的に変更できよるようにするか\n",
    "        # clear_after_read →　readでデータを取得した後に初期化するか\n",
    "\n",
    "        # .wirte →　データを書き込む(index →　書き込む位置, value →　書き込む値)\n",
    "        # .read →　要素を一つ見る\n",
    "        # .stack　→　全ての要素を見る\n",
    "        # .unstack →　配列を一気に書き込む\n",
    "        gen_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
    "                                             dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        ta_emb_x = tf.TensorArray(dtype=tf.float32, size=self.sequence_length)\n",
    "        ta_emb_x = ta_emb_x.unstack(processed_x)\n",
    "        ta_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length)\n",
    "        ta_x = ta_x.unstack(tf.transpose(x_orig, perm=[1, 0]))\n",
    "\n",
    "        # When current index i < given_num, use the provided tokens as the input at each time step\n",
    "        # 最初は既存のlstmのモデルからパラメータを取得しているが、given_numを越えると、パラメータを元にnext_tokenの確率を計算し直して算出している\n",
    "\n",
    "        def _g_recurrence_1(i, x_t, h_tm1, given_num, gen_x):\n",
    "            # h_t: hidden_memory_tuple\n",
    "            _, h_t = self.g_model.layers[1].cell(x_t, h_tm1, training=False) # layers[1]: LSTM\n",
    "            x_tp1 = ta_emb_x.read(i)\n",
    "            next_token = ta_x.read(i)\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
    "            return i + 1, x_tp1, h_t, given_num, gen_x\n",
    "\n",
    "        # When current index i >= given_num, start roll-out, use the output as time step t as the input at time step t+1\n",
    "        def _g_recurrence_2(i, x_t, h_tm1, given_num, gen_x):\n",
    "            # o_t: batch x vocab, probability\n",
    "            # h_t: hidden_memory_tuple\n",
    "            o_t, h_t = self.g_model.layers[1].cell(x_t, h_tm1, training=False) # layers[1]: LSTM\n",
    "            o_t = self.g_model.layers[2](o_t) # layers[2]: Dense\n",
    "            log_prob = tf.math.log(o_t)\n",
    "            next_token = tf.cast(tf.reshape(tf.random.categorical(log_prob, 1), [self.batch_size]), tf.int32)\n",
    "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
    "            return i + 1, x_tp1, h_t, given_num, gen_x\n",
    "\n",
    "        # tf.while_loopでループさせる\n",
    "        # i < given_numの場合と、given_num < i < sequence_lengthの場合で、異なるループを回す\n",
    "        i, x_t, h_tm1, given_num, gen_x = tf.while_loop(\n",
    "            cond=lambda i, _1, _2, given_num, _4: i < given_num,\n",
    "            body=_g_recurrence_1,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token_vec), h0, given_num, gen_x))\n",
    "\n",
    "        _, _, _, _, gen_x = tf.while_loop(\n",
    "            cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
    "            body=_g_recurrence_2,\n",
    "            loop_vars=(i, x_t, h_tm1, given_num, gen_x))\n",
    "\n",
    "        # gen_x: seq_length x batch_size\n",
    "        # 最終的にgen_xをoutputする\n",
    "        # gen_xには、単語の順番と次の単語の確率が入っているはず\n",
    "        outputs = tf.transpose(gen_x.stack(), perm=[1, 0])  # batch_size x seq_length\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "    # 一度generatorでワンバッチ分のsampleを生成し、それを入力している\n",
    "    # 生成したサンプルからnext_tokenの確率をもう一度算出し直している\n",
    "    # その後、算出した確率をdiscriminatorのモデルに入れてypred_for_accを算出し、rewordsに追加していく\n",
    "    def get_reward(self, input_x, rollout_num, discriminator):\n",
    "        rewards = []\n",
    "        for i in range(rollout_num):\n",
    "            # given_num between 1 to sequence_length - 1 for a part completed sentence\n",
    "            for given_num in tf.range(1, self.sequence_length):\n",
    "                samples = self.generate_one_batch(input_x, given_num) # 文章をバッチサイズ個だけ生成\n",
    "                ypred_for_auc = discriminator.d_model(samples).numpy() # 入力した文章に対して[falseの確率 trueの確率]を返す\n",
    "                ypred = ypred_for_auc[:, 1] # prob for outputting 1 (True) つまり　Trueの確率のみを取り出す\n",
    "                if i == 0:\n",
    "                    rewards.append(ypred)\n",
    "                else:\n",
    "                    rewards[given_num - 1] += ypred\n",
    "\n",
    "            # the last token reward\n",
    "            ypred_for_auc = discriminator.d_model(input_x).numpy()\n",
    "            ypred = ypred_for_auc[:, 1]\n",
    "            if i == 0:\n",
    "                rewards.append(ypred)\n",
    "            else:\n",
    "                # completed sentence reward\n",
    "                rewards[self.sequence_length - 1] += ypred\n",
    "\n",
    "        # rollout_num回for文で回してから、rollout_numで割ってるってことは、平均を出してるってことかも\n",
    "        rewards = np.transpose(np.array(rewards)) / (1.0 * rollout_num)  # batch_size x seq_length\n",
    "        return rewards\n",
    "\n",
    "    def update_params(self):\n",
    "        # Weights and Bias for input and hidden tensor\n",
    "        # self: Rollout, self.lstm: Original generator\n",
    "\n",
    "        # The embedding layer: directly (fully) transferred\n",
    "        # The other layers: transferred with the ratio of (1 - self.update_rate)\n",
    "\n",
    "        # 生成器で構築したg_modelとrolloutで再び構築したlstm.g_modelのパラメータを掛け合わせて調節し新しい重みを算出している\n",
    "        # g_modelが0.8でlstm.g_modelが0.2って感じ\n",
    "        new_weights = [self.update_rate * w1 + (1 - self.update_rate) * w2 if i > 0 else w2\n",
    "                       for i, (w1, w2) in enumerate(zip(self.g_model.get_weights(), self.lstm.g_model.get_weights()))]\n",
    "        self.g_model.set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTg3PiFqd7hK"
   },
   "source": [
    "# file_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習のモデルの重みや生成結果、識別器が算出した本物かどうかの確率などを出力する部分\n",
    "\n",
    "ここは自作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV7L3UeTeB84"
   },
   "outputs": [],
   "source": [
    "def file_output(epoch):\n",
    "    temp_output_list = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        temp_output = generator.generate_one_batch()\n",
    "\n",
    "        prob = discriminator.d_model(temp_output).numpy()\n",
    "        ypred = prob[:, 1]\n",
    "\n",
    "\n",
    "        # numpy形式のリストに変換したのち、pythonのリストの形式に変換\n",
    "        temp_output = temp_output.numpy()\n",
    "        temp_output = temp_output.tolist()\n",
    "\n",
    "        # print(temp_output)\n",
    "        # print(ypred)\n",
    "\n",
    "\n",
    "        for o, y in zip(temp_output, ypred):\n",
    "          temp = [o,y]\n",
    "          temp_output_list.append(temp)\n",
    "\n",
    "    temp_output_list.sort(reverse=True, key=lambda x:x[1])\n",
    "\n",
    "    # 生成結果をテキスト形式にする\n",
    "    output_text = \"\"\n",
    "    for i in temp_output_list:\n",
    "      temp_text = \"\"\n",
    "      for out in i[0]:\n",
    "        temp_text += str(out)\n",
    "        temp_text += \" \"\n",
    "      \n",
    "      output_text += temp_text\n",
    "      output_text += \"\\n\"\n",
    "\n",
    "    # print(output_text)\n",
    "\n",
    "    # 生成結果を出力\n",
    "    with open('data/generated/output_text_{}.txt'.format(epoch), 'w') as f:\n",
    "      f.write(output_text)\n",
    "\n",
    "\n",
    "    # 生成結果の確率をテキスト形式する\n",
    "    output_text = \"\"\n",
    "    for i in temp_output_list:\n",
    "      output_text += str(i[1])\n",
    "      output_text += \"\\n\"\n",
    "\n",
    "    # print(output_text)\n",
    "\n",
    "    # 生成結果を出力\n",
    "    with open('data/generated/prob_{}.txt'.format(epoch), 'w') as f:\n",
    "      f.write(output_text)\n",
    "\n",
    "    # 生成器と識別器の重みを保存\n",
    "    generator.save(\"data/generated/generator_{}.h5\".format(epoch))\n",
    "    discriminator.save(\"data/generated/discriminator_{}.h5\".format(epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy1GpvoEv_Ec"
   },
   "source": [
    "# sequence_gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行部分\n",
    "\n",
    "パラメータやエポック数の調整はここで行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8RpxnCtv_Ec"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#tf.disable_v2_behavior()\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VaVZJrdv_Ec"
   },
   "source": [
    "# 変えるポイント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj2aZsywv_Ed"
   },
   "source": [
    "START_TOKEN → vocab_size以内で使用していないidの数（0はパディングと被るのでやめたほうが良い）　\n",
    "\n",
    "SEQ_LENGTH 20 →　15 今回の入力は15単語で固定\n",
    "\n",
    "dis_num_filtersとdis_filter_sizesの配列の最後を消去\n",
    "\n",
    "generated_num →　入力データの数と合わせる\n",
    "\n",
    "vocab_size →　観光地の575での語彙数より大きくなるように調整\n",
    "\n",
    "rewards = rollout.get_reward(samples, 16 →　12, discriminator)\n",
    "\n",
    "PRE_EPOCH_NUM と　識別器の事前学習の回数は変わりうる（結果をみながら調整が必要）\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "今回は博多駅を対象にしており、他の観光地にて生成を行う場合は、適当に合わせること"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wy5ZR-_av_Ed"
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  生成器のパラメータ\n",
    "######################################################################################\n",
    "EMB_DIM = 32 # embedding dimension\n",
    "HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n",
    "SEQ_LENGTH = 15 # sequence length\n",
    "START_TOKEN = 1699\n",
    "PRE_EPOCH_NUM = 301 # supervise (maximum likelihood estimation) epochs\n",
    "SEED = 88\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#########################################################################################\n",
    "#  識別器のパラメータ\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "\n",
    "#########################################################################################\n",
    "#  Basic Training Parameters\n",
    "# GANの学習を実行していく\n",
    "#########################################################################################\n",
    "\n",
    "TOTAL_BATCH = 300 # 生成器と識別器の更新を300回する\n",
    "\n",
    "# 学習で使用するデータ\n",
    "# 最初は存在しないので、lstmで作るらしい\n",
    "\n",
    "# 生成器の事前学習で使用\n",
    "generator_file = 'data/id/all_575_id.txt' \n",
    "eval_file = 'data/generated/eval_file.txt' # 使えていない気がする\n",
    "\n",
    "# 識別器の学習で使用\n",
    "positive_file = 'data/id/hakataeki_575_id.txt' # 対象とする観光地の575のid\n",
    "negative_file = 'data/generated/hakataeki/paramater/generator_sample.txt' # 生成器が作成した偽物を使用する\n",
    "\n",
    "output_file = 'data/generated/hakataeki/output/output_file.txt'\n",
    "\n",
    "# 生成された数\n",
    "generated_num = 1024 # kgeneratorのデータ数と合わせている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFN9BImuv_Ed",
    "outputId": "6bd024a3-f467-45f2-d33b-2032d9b77fb1"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "assert START_TOKEN == 1699\n",
    "\n",
    "vocab_size = 1700\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"GPUが使えるよ\")\n",
    "    # for dev in physical_devices:\n",
    "    #     tf.config.experimental.set_memory_growth(dev, True)\n",
    "else:\n",
    "    print(\"GPUは使えないよ,ごめんね\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBhGn2lcv_Ed",
    "outputId": "2d59feb2-98db-4384-fa57-c7aed33aa628"
   },
   "outputs": [],
   "source": [
    "# 生成器で学習\n",
    "generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n",
    "target_params = pickle.load(open('data/generated/target_params_py3.pkl', 'rb'))\n",
    "target_lstm = TARGET_LSTM(BATCH_SIZE, SEQ_LENGTH, START_TOKEN, target_params, vocab_size) # The oracle model\n",
    "\n",
    "# 識別器で学習\n",
    "discriminator = Discriminator(sequence_length=SEQ_LENGTH, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim,\n",
    "                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, dropout_keep_prob=dis_dropout_keep_prob,\n",
    "                                l2_reg_lambda=dis_l2_reg_lambda)\n",
    "\n",
    "# First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n",
    "# GANの学習で使用する正解データを作成する\n",
    "if not os.path.exists(generator_file):\n",
    "    target_lstm.generate_samples(generated_num // BATCH_SIZE, generator_file)\n",
    "gen_dataset = dataset_for_generator(generator_file, BATCH_SIZE)\n",
    "log = open('data/generated/experiment-log.txt', 'w')\n",
    "\n",
    "\n",
    "#  事前学習での文章生成をlstmで行い、生成器の重みを保存する\n",
    "if not os.path.exists(\"data/generated/hakataeki/paramater/generator_pretrained.h5\"):\n",
    "    print('Start pre-training...')\n",
    "    log.write('pre-training...\\n')\n",
    "    generator.pretrain(gen_dataset, target_lstm, PRE_EPOCH_NUM, generated_num // BATCH_SIZE, eval_file)\n",
    "    generator.save(\"data/generated/hakataeki/paramater/generator_pretrained.h5\")\n",
    "else:\n",
    "    generator.load(\"data/generated/hakataeki/paramater/generator_pretrained.h5\")\n",
    "\n",
    "# 識別器の事前学習での重み\n",
    "if not os.path.exists(\"data/generated/hakataeki/paramater/discriminator_pretrained.h5\"):\n",
    "    print('Start pre-training discriminator...')\n",
    "    # 1エポックの識別器の訓練を15回繰り返す\n",
    "    for _ in range(15):\n",
    "        print(\"Dataset\", _)\n",
    "\n",
    "        # まず生成器が偽物を作成\n",
    "        generator.generate_samples(generated_num // BATCH_SIZE, negative_file)\n",
    "\n",
    "        # 偽物と本物を混ぜたデータセットを作成\n",
    "        dis_dataset = dataset_for_discriminator(positive_file, negative_file, BATCH_SIZE)\n",
    "\n",
    "        # 識別器を学習させる\n",
    "        discriminator.train(dis_dataset, 1, (generated_num // BATCH_SIZE) * 2)\n",
    "    discriminator.save(\"data/generated/hakataeki/paramater/discriminator_pretrained.h5\")\n",
    "else:\n",
    "    discriminator.load(\"data/generated/hakataeki/paramater/discriminator_pretrained.h5\")\n",
    "\n",
    "rollout = ROLLOUT(generator, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zr1EcYpkv_Ed",
    "outputId": "a6b3a5bf-f7f1-43a0-b8c9-28995926be6d"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('#########################################################################')\n",
    "print('Start Adversarial Training...')\n",
    "log.write('adversarial training...\\n')\n",
    "\n",
    "# 学習の実行\n",
    "# 今回は200回の訓練を行う\n",
    "\n",
    "# accとlossのグラフ描画のための配列\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "\n",
    "for total_batch in range(TOTAL_BATCH):\n",
    "    print(\"Generator\", total_batch)\n",
    "    # Train the generator for one step\n",
    "    for _ in range(5):\n",
    "        samples = generator.generate_one_batch()\n",
    "        rewards = rollout.get_reward(samples, 12, discriminator)\n",
    "        generator.train_step(samples, rewards)\n",
    "\n",
    "    # target_lossが機能していないと思われるので実行しない\n",
    "    # # Test \n",
    "    # if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
    "    #     generator.generate_samples(generated_num // BATCH_SIZE, eval_file)\n",
    "    #     likelihood_dataset = dataset_for_generator(eval_file, BATCH_SIZE)\n",
    "    #     test_loss = target_lstm.target_loss(likelihood_dataset)\n",
    "    #     buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "    #     print('total_batch: ', total_batch, 'test_loss: ', test_loss)\n",
    "    #     log.write(buffer)\n",
    "\n",
    "    # Update roll-out parameters\n",
    "    rollout.update_params()\n",
    "\n",
    "    # Train the discriminator\n",
    "    print(\"Discriminator\", total_batch)\n",
    "    for _ in range(1):\n",
    "        generator.generate_samples(generated_num // BATCH_SIZE, negative_file)\n",
    "        dis_dataset = dataset_for_discriminator(positive_file, negative_file, BATCH_SIZE)\n",
    "        history = discriminator.train(dis_dataset, 1, (generated_num // BATCH_SIZE) * 2)\n",
    "        \n",
    "        acc_list.append(history.history['accuracy'])\n",
    "        loss_list .append(history.history['loss'])\n",
    "\n",
    "\n",
    "file_output(TOTAL_BATCH)\n",
    "\n",
    "with open('data/generated/hakataeki/paramater/acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(acc_list, f)\n",
    "\n",
    "with open('data/generated/hakataeki/paramater/loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(loss_list, f)\n",
    "\n",
    "\n",
    "\n",
    "log.close()\n"
   ]
  },
  {
   "source": [
    "google colabにてフォルダをダウンロードするときに使用"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "I0ypqDICv_Ef",
    "outputId": "b5c03f8f-8b1f-4c04-b9cd-d10967e6b974"
   },
   "outputs": [],
   "source": [
    "# ダウンロードしたいフォルダを zip 圧縮する\n",
    "!zip -r /content/data/generated.zip /content/data/generated\n",
    "\n",
    "# 圧縮した zip ファイルをダウンロードする\n",
    "from google.colab import files\n",
    "files.download(\"/content/data/generated.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0qENGlUhUbZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aysGHjUov_EI",
    "HlTEODRPv_EO",
    "hxv4ogz1v_EP",
    "NhCgdNyIv_EY",
    "ISvYb9_8v_Eb",
    "PTg3PiFqd7hK",
    "Gy1GpvoEv_Ec"
   ],
   "name": "seqGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python387jvsc74a57bd09f1e5f8df66e0355b93d6a73e8e18cceb2fedad000b7b1dd4a514e55097c6a9d",
   "display_name": "Python 3.8.7 64-bit ('3.8.7')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "9f1e5f8df66e0355b93d6a73e8e18cceb2fedad000b7b1dd4a514e55097c6a9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
