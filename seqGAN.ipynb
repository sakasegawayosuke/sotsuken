{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQw0EF5av_D_"
   },
   "source": [
    "# seqGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjPzDI2fv_EH"
   },
   "source": [
    "ここでは、用意した学習データを用いて、seqGANによるキャッチコピーの生成を行う"
    "\n",
    "\n",
    "参考サイト（https://qiita.com/everylittle/items/19c4988a135d36150dc0）\n",
    "\n",
    "こちらの方のコードにほぼ乗っかる形で、ところどころで自分のデータに合わせながら作成しました。\n",
    "\n",
    "とてもお世話になりました。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWYN51zIv_EH"
   },
   "source": [
    "# seqGANのコード一覧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWlrTpVYv_EI"
   },
   "source": [
    "generator.py ：生成器について\n",
    "\n",
    "discriminator.py：識別器について\n",
    "\n",
    "dataloader.py：入力データを読み込んでリストを作る\n",
    "\n",
    "rnnlm.py：生成器で使用するlstmの雛形を作ってる感じ、ワンバッチとか\n",
    "\n",
    "rollout.py：生成器と識別器のパラメータを調整している\n",
    "\n",
    "target_lstm.py：lstmのモデルを作っている\n",
    "\n",
    "utils.py：ボキャブラリーの作成、生成器の事前学習、識別器と生成器の掛け合いについて、正直一番よくわかっていない部分\n",
    "\n",
    "sequence_gan.py：メイン関数がある、全体の統括、ここでパラメータをいじればいいって書いてあった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCB-X9TCv_EI"
   },
   "source": [
    "# それぞれのコードの中身について調べてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aysGHjUov_EI"
   },
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込む部分\n",
    "\n",
    "今回は、観光地レビューの575文字列をid化したものを読み込ませる\n",
    "\n",
    "データセットは生成器用と識別器用のふたつを作る\n",
    "\n",
    "識別器のデータセットでは正例と負例を区別するためにラベル付けが行われる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqLks309v_EJ"
   },
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R9AUckZpv_EJ"
   },
   "outputs": [],
   "source": [
    "#生成器のデータセットを作成する\n",
    "\n",
    "def dataset_for_generator(data_file, batch_size):\n",
    "    dataset = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        # 一行ずつ読み取る\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            parse_line = [int(x) for x in line]\n",
    "            if len(parse_line) == 15: # 一行で15単語\n",
    "                dataset.append(parse_line)\n",
    "    output = tf.data.Dataset.from_tensor_slices(dataset).shuffle(len(dataset)).batch(batch_size)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo052Q_0v_EK"
   },
   "source": [
    "バッチサイズの仕組み\n",
    "\n",
    "tf.data.Dataset.batch(10)ってやると、データをバッチサイズごとに分割してくれる\n",
    "\n",
    "例えば、100個分のデータがあったとして、そのままだと、二次元配列に一つだけのデータ\u001c",
    "(１００個)が入った状態になる。\n",
    "\n",
    "[[１００個のデータ]]\n",
    "\n",
    "これを.batch(10)ってすると、\n",
    "\n",
    "[[１０個のデータ]]\n",
    "\n",
    "[[１０個のデータ]]\n",
    "\n",
    "...\n",
    "\n",
    "[[１０個のデータ]]\n",
    "\n",
    "みたいに、100/10　個に分けてデータを扱うことができるようになる。　\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rcw0l4p0v_EK"
   },
   "outputs": [],
   "source": [
    "#識別器のデータセットを作成\n",
    "\n",
    "def dataset_for_discriminator(positive_file, negative_file, batch_size):\n",
    "\n",
    "    examples = [] # データを入れる配列\n",
    "    labels = [] # データがtrueかfalseかのラベルを付ける\n",
    "\n",
    "    with open(positive_file) as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            parse_line = [int(x) for x in line]\n",
    "            if len(parse_line) == 15:\n",
    "                examples.append(parse_line)\n",
    "                labels.append([0, 1])\n",
    "\n",
    "    with open(negative_file) as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            parse_line = [int(x) for x in line]\n",
    "            if len(parse_line) == 15:\n",
    "                examples.append(parse_line)\n",
    "                labels.append([1, 0])\n",
    "    output = tf.data.Dataset.from_tensor_slices((examples, labels)).shuffle(len(examples)).batch(batch_size)\n",
    "    return output\n",
    "\n",
    "# tf.data.Datasetの形式は配列ではないのでそのままでは内容が確認できない\n",
    "# for文とかnumpy()を使えば確認できるらしい\n",
    "# もしくはデータの形式を変えたら確認できるのかな\n",
    "\n",
    "# listの形式でいけたわ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVOuz77_v_EM"
   },
   "source": [
    "# rnnlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成器のモデルを作成している\n",
    "\n",
    "のちのtarget_lstmではここで作ったクラスを継承している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9vKmhbuHv_EM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Flatten\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wu5xnh-Mv_EN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-75326a35a2cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-75326a35a2cb>\u001b[0m in \u001b[0;36mRNNLM\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;31m# これを付けると、これ以降は処理速度が早くなるらしい\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# バッチサイズひとつ分だけを生成する場合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class RNNLM(object):\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, learning_rate=0.01):\n",
    "        self.num_emb = num_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.start_token_vec = tf.constant([start_token] * self.batch_size, dtype=tf.int32)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = 5.0\n",
    "\n",
    "        # 生成器のモデルを作る\n",
    "        # 入力サイズ(Input)は固定の単語数\n",
    "        # モデルの構造は、　input embedding LSTM Dense って流れ\n",
    "\n",
    "        self.g_model = tf.keras.models.Sequential([\n",
    "            Input((self.sequence_length,), dtype=tf.int32),\n",
    "            Embedding(self.num_emb, self.emb_dim, embeddings_initializer=tf.random_normal_initializer(stddev=0.1)),\n",
    "            LSTM(self.hidden_dim, kernel_initializer=tf.random_normal_initializer(stddev=0.1), recurrent_initializer=tf.random_normal_initializer(stddev=0.1), return_sequences=True),\n",
    "            Dense(self.num_emb, kernel_initializer=tf.random_normal_initializer(stddev=0.1), activation=\"softmax\")\n",
    "        ])\n",
    "\n",
    "        # optimizer(最適化関数)を自分で作ってる\n",
    "        # 最適化関数　→　損失関数をどの方向にどのくらい更新するかを決める\n",
    "        # learing_rate →　どの程度更新するか\n",
    "        # 一回の学習で更新しすぎないように制限するパラメータも存在する\n",
    "        # clipnorm →　ベクトルの長さ\n",
    "        # clipvalue →　絶対値\n",
    "\n",
    "        # 損失関数　→　sparse_categorical_crossentropy\n",
    "        # これは、クラス分類に使われる損失関数\n",
    "        # categorical_crossentropyだと、one-hotベクトルでの分類を行うが、sparse_categorical_crossentropyでは整数で分類可能\n",
    "        # つまり[001][010][100]って分類か[0][1][2]って分類か\n",
    "        # lstmだと次に来る単語の候補の確率から、どの候補かを分類してるって感じなのかな\n",
    "        \n",
    "\n",
    "        self.g_optimizer = self._create_optimizer(learning_rate, clipnorm=self.grad_clip)\n",
    "        if self.g_optimizer is not None:\n",
    "            self.g_model.compile(\n",
    "                optimizer=self.g_optimizer,\n",
    "                loss=\"sparse_categorical_crossentropy\")\n",
    "        else:\n",
    "            self.g_model.compile(\n",
    "                loss=\"sparse_categorical_crossentropy\")\n",
    "        self.g_embeddings = self.g_model.trainable_weights[0]\n",
    "\n",
    "\n",
    "    # データセットを入れて、損失を出力する\n",
    "    def target_loss(self, dataset):\n",
    "        # dataset: each element has [self.batch_size, self.sequence_length]\n",
    "        # outputs are 1 timestep ahead\n",
    "        ds = dataset.map(lambda x: (tf.pad(x[:, 0:-1], ([0, 0], [1, 0]), \"CONSTANT\", self.start_token), x))\n",
    "        loss = self.g_model.evaluate(ds, verbose=1)\n",
    "\n",
    "        # print(\"\\n\")\n",
    "        # print(\"生成したデータセットから損失を計算しました\")\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @tf.function # これを付けると、これ以降は処理速度が早くなるらしい\n",
    "\n",
    "    # バッチサイズひとつ分だけを生成する場合\n",
    "    def generate_one_batch(self):\n",
    "        # パラメータの初期設定\n",
    "        # h0とc0は隠れ層でのパラメータ\n",
    "        # gen_xは\n",
    "        h0 = c0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
    "        gen_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
    "                               dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        def _g_recurrence(i, x_t, h_tm1, gen_x):\n",
    "            # o_t: batch x vocab, probability\n",
    "            # h_t: hidden_memory_tuple\n",
    "            o_t, h_t = self.g_model.layers[1].cell(x_t, h_tm1, training=False) # layers[1]: LSTM\n",
    "            o_t = self.g_model.layers[2](o_t) # layers[2]: Dense\n",
    "            log_prob = tf.math.log(o_t)\n",
    "            next_token = tf.cast(tf.reshape(tf.random.categorical(log_prob, 1), [self.batch_size]), tf.int32)\n",
    "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
    "            return i + 1, x_tp1, h_t, gen_x\n",
    "\n",
    "        # while_loop\n",
    "        # cond →　ループの条件\n",
    "        # body →　状態の更新\n",
    "        # loop_vars →　初期状態\n",
    "        _, _, _, gen_x = tf.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
    "            body=_g_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token_vec), [h0, c0], gen_x))\n",
    "\n",
    "        gen_x = gen_x.stack()  # seq_length x batch_size\n",
    "        outputs = tf.transpose(gen_x, perm=[1, 0])  # batch_size x seq_length\n",
    "        return outputs\n",
    "\n",
    "    # 生成を実行\n",
    "    def generate_samples(self, num_batches, output_file):\n",
    "        # Generate Samples\n",
    "        with open(output_file, 'w') as fout:\n",
    "            for _ in range(num_batches):\n",
    "                generated_samples = self.generate_one_batch().numpy()\n",
    "\n",
    "                # print(\"\\n\")\n",
    "                # print(\"生成結果 : \",generated_samples) # 生成結果を表示\n",
    "\n",
    "                # generated_samplesはバッチサイズ個作られているので、それぞれをfor文で回して出力する\n",
    "                for poem in generated_samples:\n",
    "                    print(' '.join([str(x) for x in poem]), file=fout)\n",
    "\n",
    "    # __init__で使われてる\n",
    "    def _create_optimizer(self, *args, **kwargs):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlTEODRPv_EO"
   },
   "source": [
    "# target_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstmの層のサイズを設定している\n",
    "\n",
    "元の論文にてtarget_params.pklという既に調整されたパラメータのファイルを読み込む形になっていたが、今回は使用しないため、入力サイズを自分で調整する必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GRot2Rtv_EO"
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LJwdFTRv_EP"
   },
   "outputs": [],
   "source": [
    "# lstmのパラメータが入っているpickleファイルを読み取る時に使う　→　今回は使わない\n",
    "# モデルのサイズはパラメータファイルで決められるよ\n",
    "# パラメータファイルと自分のデータのファイルで違うので自分のファイルに合わせるように変数を調整する\n",
    "\n",
    "class TARGET_LSTM(RNNLM):\n",
    "    # function for reading save/target_params.pkl\n",
    "    \n",
    "    # voc_sizeを追加 num_embの値をvoc_sizeに変更\n",
    "    def __init__(self, batch_size, sequence_length, start_token, params, voc_size):\n",
    "\n",
    "#         num_emb = params[0].shape[0]\n",
    "        num_emb = voc_size\n",
    "        emb_dim = params[0].shape[1]\n",
    "        hidden_dim = params[1].shape[1]\n",
    "        \n",
    "        param_0 = np.zeros((num_emb, emb_dim)) # 作成\n",
    "        param_13 = np.zeros((emb_dim, num_emb)) # 作成\n",
    "        param_14 = np.zeros(num_emb) # 作成\n",
    "\n",
    "        super(TARGET_LSTM, self).__init__(num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token)\n",
    "        weights = [\n",
    "            # Embedding\n",
    "            param_0,\n",
    "            # LSTM\n",
    "            np.c_[params[1], params[4], params[10], params[7]], # kernel (i, f, c, o)\n",
    "            np.c_[params[2], params[5], params[11], params[8]], # recurrent_kernel\n",
    "            np.r_[params[3], params[6], params[12], params[9]], # bias\n",
    "            # Dense\n",
    "            param_13,\n",
    "            param_14\n",
    "        ]\n",
    "        self.g_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxv4ogz1v_EP"
   },
   "source": [
    "# generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seqGANでは、生成器は事前学習と敵対学習の２つのステップがある。\n",
    "\n",
    "まず、事前学習にてある程度のキャッチコピーが作れるようになった上で、敵対学習を実施する。\n",
    "\n",
    "どの程度の事前学習を行うかは、後のsequence_ganにて調整する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXI5YFpZv_EP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSoBUbP-v_EQ"
   },
   "outputs": [],
   "source": [
    "# さっきのtarget_lstmと今回のgeneratorはrnnlmで構築したモデルを利用している\n",
    "# ので、わからない関数があったときはrnnlmを見返すと良い\n",
    "# 関数の中に別の関数があってその中にまた別の関数があるので、理解がちょっと難しい\n",
    "\n",
    "\n",
    "class Generator(RNNLM):\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, learning_rate=0.01):\n",
    "        super(Generator, self).__init__(num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, learning_rate)\n",
    "\n",
    "        # prepare model for GAN training\n",
    "        # rnnlmでもしたけど、またコンパイルをしている　→　別物じゃん(g_modelとg_model_temporal)\n",
    "        # sample_weight_modeってのが増えたのかな\n",
    "\n",
    "        # sample_weight →　入力サンプルと同じ長さのnumpy配列で、訓練のサンプルに対する重みを格納する\n",
    "        # これは損失関数をスケーリングするために、訓練中だけ使用する\n",
    "        # あるいは系列データの場合において，2次元配列の(samples, sequence_length)という形式で， \n",
    "        # すべてのサンプルの各時間において異なる重みを適用できます． \n",
    "        # この場合，compile()の中でsample_weight_mode=\"temporal\"と確実に明記すべき\n",
    "\n",
    "        self.g_model_temporal = tf.keras.models.Sequential(self.g_model.layers)\n",
    "        self.g_optimizer_temporal = self._create_optimizer(\n",
    "            learning_rate, clipnorm=self.grad_clip)\n",
    "        self.g_model_temporal.compile(\n",
    "            optimizer=self.g_optimizer_temporal,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            sample_weight_mode=\"temporal\")\n",
    "\n",
    "    # 事前学習にて実行\n",
    "\n",
    "    def pretrain(self, dataset, target_lstm, num_epochs, num_steps, eval_file):\n",
    "        # dataset: each element has [self.batch_size, self.sequence_length]\n",
    "        # outputs are 1 timestep ahead\n",
    "\n",
    "        # 5エポックごとに損失を求めている\n",
    "        def pretrain_callback(epoch, logs):\n",
    "            if epoch % 5 == 0:\n",
    "                self.generate_samples(num_steps, eval_file) # ここで生成\n",
    "                likelihood_dataset = dataset_for_generator(eval_file, self.batch_size) # データセットはここで作成\n",
    "\n",
    "                # print(\"損失に用いたデータセットを表示\")  \n",
    "                # for i in likelihood_dataset:\n",
    "                #     print(i.numpy())\n",
    "\n",
    "                test_loss = target_lstm.target_loss(likelihood_dataset) # 作成したデータセットの損失を求める\n",
    "\n",
    "\n",
    "                # print('pre-train epoch ', epoch, 'test_loss ', test_loss) # 損失を表示する\n",
    "                buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "                log.write(buffer)\n",
    "\n",
    "        # データセットを作成\n",
    "        # データの先頭にstart_tokenの数字を付ける処理を行なっている\n",
    "        # start_tokenが0だったため、paddingの0と被っていたのが問題かも →　start_tokenの値を変える or paddingの値を変える\n",
    "        ds = dataset.map(lambda x: (tf.pad(x[:, 0:-1], ([0, 0], [1, 0]), \"CONSTANT\", self.start_token), x)).repeat(num_epochs)\n",
    "\n",
    "        # 生成器のモデルの学習を実施\n",
    "        # callbacks →　訓練中のモデルの状態を可視化するために使われる\n",
    "        # model.fitの中で使われるよ\n",
    "        # tf.keras.callbacks.LambdaCallback →　シンプルな自作コールバックを作れる\n",
    "        # 今回のon_epoch_endではepochとlogの二つの位置引数が必要(他にも色々ある模様)\n",
    "\n",
    "\n",
    "\n",
    "        # pretrain_loss = self.g_model.fit(ds, verbose=1, epochs=num_epochs, steps_per_epoch=num_steps,\n",
    "        #                                  callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=pretrain_callback)])\n",
    "        \n",
    "        pretrain_loss = self.g_model.fit(ds, verbose=1, epochs=num_epochs, steps_per_epoch=num_steps) # callbackなしでちょっと様子を見る\n",
    "        return pretrain_loss\n",
    "\n",
    "    def train_step(self, x, rewards):\n",
    "        # x: [self.batch_size, self.sequence_length]\n",
    "        # rewards: [self.batch_size, self.sequence_length] (sample_weight)\n",
    "        # outputs are 1 timestep ahead\n",
    "\n",
    "        # __init__でコンパイルしてたモデル\n",
    "        # train_on_batch　→　サンプル中の一つのバッチで勾配を更新する\n",
    "        # やってることは.fit()と.train_on_batch()で同じなのかも(違いは、使うバッチが一つか複数か)\n",
    "        # sample_weight →　サンプルの重み、numpy配列\n",
    "\n",
    "        train_loss = self.g_model_temporal.train_on_batch(\n",
    "            np.pad(x[:, 0:-1], ([0, 0], [1, 0]), \"constant\", constant_values=self.start_token), x,\n",
    "            # sparse_categorical_crossentropy returns mean loss\n",
    "            # here we multiply (batch_size * sequence_length) to use weighted \"sum\"\n",
    "            sample_weight=rewards * self.batch_size * self.sequence_length)\n",
    "        return train_loss\n",
    "\n",
    "    def _create_optimizer(self, *args, **kwargs):\n",
    "        return tf.keras.optimizers.Adam(*args, **kwargs)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.g_model.save_weights(filename, save_format=\"h5\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.g_model.load_weights(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行時に表示されるETA は estimated time of arrival の略で、1エポックあたりのトレーニングにかかる時間の予測のこと。\n",
    "\n",
    "エポック内の処理の進捗と残りのデータ量を使ってkerasが自動で予測して出力する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhCgdNyIv_EY"
   },
   "source": [
    "# discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "識別器も生成器と同様に、事前学習と敵対学習の２ステップを行う。\n",
    "\n",
    "また、今回は生成器と識別器で扱うデータの種類が異なっている\n",
    "\n",
    "- 生成器は事前学習にて、全ての観光地の575が与えられる　⇨　語彙や表現の幅を増やす\n",
    "- 識別器は正解データ（正例）に対象の観光地の575が与えられる　⇨　生成器は識別器に与えられた観光地の575に似せるように生成を行うため、目的の観光地にあったキャッチコピーが生成される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OS5ucB_kv_EZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Embedding, Conv1D, MaxPool1D, Concatenate, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SpDIHxwv_EZ"
   },
   "outputs": [],
   "source": [
    "# どこで使っているのか\n",
    "\n",
    "class Highway(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Highway, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        output_dim = input_shape[-1]\n",
    "        self.dense_g = Dense(output_dim, activation=\"relu\")\n",
    "        self.dense_t = Dense(output_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        g = self.dense_g(input_tensor, training=training)\n",
    "        t = self.dense_t(input_tensor, training=training)\n",
    "        o = t * g + (1. - t) * input_tensor\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXtgIX2ov_EZ"
   },
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, sequence_length, num_classes, vocab_size,\n",
    "            embedding_size, filter_sizes, num_filters, dropout_keep_prob, l2_reg_lambda=0.2):\n",
    "      \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        layer_input = Input((sequence_length,), dtype=tf.int32)\n",
    "        layer_emb = Embedding(vocab_size, embedding_size, embeddings_initializer=tf.random_uniform_initializer(-1.0, 1.0))(layer_input)\n",
    "        # (None, sequence_length, embedding_size)\n",
    "\n",
    "\n",
    "        # filter_sizes →　畳み込みのフィルターの大きさ(カーネルサイズのこと)\n",
    "        # num_filters →　フィルターの数(畳み込みにおける出力フィルタの数)\n",
    "        pooled_outputs = []\n",
    "        for filter_size, num_filter in zip(filter_sizes, num_filters):\n",
    "            x = Conv1D(num_filter, filter_size)(layer_emb) # (None, sequence_length - filter_size + 1, num_filter)\n",
    "            x = MaxPool1D(sequence_length - filter_size + 1)(x) # (None, 1, num_filter)\n",
    "            pooled_outputs.append(x)\n",
    "\n",
    "        x = Concatenate()(pooled_outputs)\n",
    "        x = Flatten()(x) # (None, sum(num_filters))\n",
    "        x = Highway()(x)\n",
    "        x = Dropout(1.0 - dropout_keep_prob)(x)\n",
    "        layer_output = Dense(num_classes,\n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(l2_reg_lambda),\n",
    "                             bias_regularizer=tf.keras.regularizers.l2(l2_reg_lambda),\n",
    "                             activation=\"softmax\")(x)\n",
    "\n",
    "        self.d_model = Model(layer_input, layer_output)\n",
    "        d_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        self.d_model.compile(optimizer=d_optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    def train(self, dataset, num_epochs, num_steps, **kwargs):\n",
    "        # dataset: ([None, sequence_length], [None, num_classes])\n",
    "        return self.d_model.fit(dataset.repeat(num_epochs), verbose=1, epochs=num_epochs, steps_per_epoch=num_steps, **kwargs)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.d_model.save_weights(filename, save_format=\"h5\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.d_model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISvYb9_8v_Eb"
   },
   "source": [
    "# rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "敵対学習にてパラメータの調整を行う部分\n",
    "\n",
    "また、生成器に対する強化学習の手法によるパラメータの更新の部分のここで設定している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DflKvAZNv_Eb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dytrx_K2v_Eb"
   },
   "outputs": [],
   "source": [
    "class ROLLOUT(RNNLM):\n",
    "    def __init__(self, lstm, update_rate):\n",
    "        super(ROLLOUT, self).__init__(lstm.num_emb, lstm.batch_size, lstm.emb_dim, lstm.hidden_dim, lstm.sequence_length, lstm.start_token)\n",
    "\n",
    "        # generatorのモデルをlstmとして読み込む\n",
    "        # update_rate = 0.8がデフォルトの設定\n",
    "        # .get_weights()でモデルの重みを取得して、set_weightsで別のモデルの重みとして設定\n",
    "        self.lstm = lstm\n",
    "        self.update_rate = update_rate\n",
    "        self.g_model.set_weights(lstm.g_model.get_weights())\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_batch(self, x_orig, given_num):\n",
    "        # Initial states\n",
    "        h0 = c0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
    "        h0 = [h0, c0]\n",
    "\n",
    "        # tf.transpose →　元の行列をpermで指定した順番に変える\n",
    "        # 今回はperm=[1, 0, 2] なので、index[0] →　index[1]になり、index[1] →　index[0]になる\n",
    "        processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, x_orig), perm=[1, 0, 2])  # seq_length x batch_size x emb_dim\n",
    "\n",
    "        # tf.TensorArray →　Tensor Arrayを生成する。多分配列みたいなものだと思う\n",
    "        # size →　tensorarrayのサイズ\n",
    "        # dynamic_size →　sizeを動的に変更できよるようにするか\n",
    "        # clear_after_read →　readでデータを取得した後に初期化するか\n",
    "\n",
    "        # .wirte →　データを書き込む(index →　書き込む位置, value →　書き込む値)\n",
    "        # .read →　要素を一つ見る\n",
    "        # .stack　→　全ての要素を見る\n",
    "        # .unstack →　配列を一気に書き込む\n",
    "        gen_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
    "                                             dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        ta_emb_x = tf.TensorArray(dtype=tf.float32, size=self.sequence_length)\n",
    "        ta_emb_x = ta_emb_x.unstack(processed_x)\n",
    "        ta_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length)\n",
    "        ta_x = ta_x.unstack(tf.transpose(x_orig, perm=[1, 0]))\n",
    "\n",
    "        # When current index i < given_num, use the provided tokens as the input at each time step\n",
    "        # 最初は既存のlstmのモデルからパラメータを取得しているが、given_numを越えると、パラメータを元にnext_tokenの確率を計算し直して算出している\n",
    "\n",
    "        def _g_recurrence_1(i, x_t, h_tm1, given_num, gen_x):\n",
    "            # h_t: hidden_memory_tuple\n",
    "            _, h_t = self.g_model.layers[1].cell(x_t, h_tm1, training=False) # layers[1]: LSTM\n",
    "            x_tp1 = ta_emb_x.read(i)\n",
    "            next_token = ta_x.read(i)\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
    "            return i + 1, x_tp1, h_t, given_num, gen_x\n",
    "\n",
    "        # When current index i >= given_num, start roll-out, use the output as time step t as the input at time step t+1\n",
    "        def _g_recurrence_2(i, x_t, h_tm1, given_num, gen_x):\n",
    "            # o_t: batch x vocab, probability\n",
    "            # h_t: hidden_memory_tuple\n",
    "            o_t, h_t = self.g_model.layers[1].cell(x_t, h_tm1, training=False) # layers[1]: LSTM\n",
    "            o_t = self.g_model.layers[2](o_t) # layers[2]: Dense\n",
    "            log_prob = tf.math.log(o_t)\n",
    "            next_token = tf.cast(tf.reshape(tf.random.categorical(log_prob, 1), [self.batch_size]), tf.int32)\n",
    "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
    "            return i + 1, x_tp1, h_t, given_num, gen_x\n",
    "\n",
    "        # tf.while_loopでループさせる\n",
    "        # i < given_numの場合と、given_num < i < sequence_lengthの場合で、異なるループを回す\n",
    "        i, x_t, h_tm1, given_num, gen_x = tf.while_loop(\n",
    "            cond=lambda i, _1, _2, given_num, _4: i < given_num,\n",
    "            body=_g_recurrence_1,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token_vec), h0, given_num, gen_x))\n",
    "\n",
    "        _, _, _, _, gen_x = tf.while_loop(\n",
    "            cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
    "            body=_g_recurrence_2,\n",
    "            loop_vars=(i, x_t, h_tm1, given_num, gen_x))\n",
    "\n",
    "        # gen_x: seq_length x batch_size\n",
    "        # 最終的にgen_xをoutputする\n",
    "        # gen_xには、単語の順番と次の単語の確率が入っているはず\n",
    "        outputs = tf.transpose(gen_x.stack(), perm=[1, 0])  # batch_size x seq_length\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "    # 一度generatorでワンバッチ分のsampleを生成し、それを入力している\n",
    "    # 生成したサンプルからnext_tokenの確率をもう一度算出し直している\n",
    "    # その後、算出した確率をdiscriminatorのモデルに入れてypred_for_accを算出し、rewordsに追加していく\n",
    "    def get_reward(self, input_x, rollout_num, discriminator):\n",
    "        rewards = []\n",
    "        for i in range(rollout_num):\n",
    "            # given_num between 1 to sequence_length - 1 for a part completed sentence\n",
    "            for given_num in tf.range(1, self.sequence_length):\n",
    "                samples = self.generate_one_batch(input_x, given_num) # 文章をバッチサイズ個だけ生成\n",
    "                ypred_for_auc = discriminator.d_model(samples).numpy() # 入力した文章に対して[falseの確率 trueの確率]を返す\n",
    "                ypred = ypred_for_auc[:, 1] # prob for outputting 1 (True) つまり　Trueの確率のみを取り出す\n",
    "                if i == 0:\n",
    "                    rewards.append(ypred)\n",
    "                else:\n",
    "                    rewards[given_num - 1] += ypred\n",
    "\n",
    "            # the last token reward\n",
    "            ypred_for_auc = discriminator.d_model(input_x).numpy()\n",
    "            ypred = ypred_for_auc[:, 1]\n",
    "            if i == 0:\n",
    "                rewards.append(ypred)\n",
    "            else:\n",
    "                # completed sentence reward\n",
    "                rewards[self.sequence_length - 1] += ypred\n",
    "\n",
    "        # rollout_num回for文で回してから、rollout_numで割ってるってことは、平均を出してるってことかも\n",
    "        rewards = np.transpose(np.array(rewards)) / (1.0 * rollout_num)  # batch_size x seq_length\n",
    "        return rewards\n",
    "\n",
    "    def update_params(self):\n",
    "        # Weights and Bias for input and hidden tensor\n",
    "        # self: Rollout, self.lstm: Original generator\n",
    "\n",
    "        # The embedding layer: directly (fully) transferred\n",
    "        # The other layers: transferred with the ratio of (1 - self.update_rate)\n",
    "\n",
    "        # 生成器で構築したg_modelとrolloutで再び構築したlstm.g_modelのパラメータを掛け合わせて調節し新しい重みを算出している\n",
    "        # g_modelが0.8でlstm.g_modelが0.2って感じ\n",
    "        new_weights = [self.update_rate * w1 + (1 - self.update_rate) * w2 if i > 0 else w2\n",
    "                       for i, (w1, w2) in enumerate(zip(self.g_model.get_weights(), self.lstm.g_model.get_weights()))]\n",
    "        self.g_model.set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTg3PiFqd7hK"
   },
   "source": [
    "# file_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習のモデルの重みや生成結果、識別器が算出した本物かどうかの確率などを出力する部分\n",
    "\n",
    "ここは自作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV7L3UeTeB84"
   },
   "outputs": [],
   "source": [
    "def file_output(epoch):\n",
    "    temp_output_list = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        temp_output = generator.generate_one_batch()\n",
    "\n",
    "        prob = discriminator.d_model(temp_output).numpy()\n",
    "        ypred = prob[:, 1]\n",
    "\n",
    "\n",
    "        # numpy形式のリストに変換したのち、pythonのリストの形式に変換\n",
    "        temp_output = temp_output.numpy()\n",
    "        temp_output = temp_output.tolist()\n",
    "\n",
    "        # print(temp_output)\n",
    "        # print(ypred)\n",
    "\n",
    "\n",
    "        for o, y in zip(temp_output, ypred):\n",
    "          temp = [o,y]\n",
    "          temp_output_list.append(temp)\n",
    "\n",
    "    temp_output_list.sort(reverse=True, key=lambda x:x[1])\n",
    "\n",
    "    # 生成結果をテキスト形式にする\n",
    "    output_text = \"\"\n",
    "    for i in temp_output_list:\n",
    "      temp_text = \"\"\n",
    "      for out in i[0]:\n",
    "        temp_text += str(out)\n",
    "        temp_text += \" \"\n",
    "      \n",
    "      output_text += temp_text\n",
    "      output_text += \"\\n\"\n",
    "\n",
    "    print(output_text)\n",
    "\n",
    "    # 生成結果を出力\n",
    "    with open('data/generated/output_text_{}.txt'.format(epoch), 'w') as f:\n",
    "      f.write(output_text)\n",
    "\n",
    "\n",
    "    # 生成結果の確率をテキスト形式する\n",
    "    output_text = \"\"\n",
    "    for i in temp_output_list:\n",
    "      output_text += str(i[1])\n",
    "      output_text += \"\\n\"\n",
    "\n",
    "    print(output_text)\n",
    "\n",
    "    # 生成結果を出力\n",
    "    with open('data/generated/prob_{}.txt'.format(epoch), 'w') as f:\n",
    "      f.write(output_text)\n",
    "\n",
    "    # 生成器と識別器の重みを保存\n",
    "    generator.save(\"data/generated/generator_{}.h5\".format(epoch))\n",
    "    discriminator.save(\"data/generated/discriminator_{}.h5\".format(epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy1GpvoEv_Ec"
   },
   "source": [
    "# sequence_gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行部分\n",
    "\n",
    "パラメータやエポック数の調整はここで行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8RpxnCtv_Ec"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#tf.disable_v2_behavior()\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VaVZJrdv_Ec"
   },
   "source": [
    "# 変えるポイント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj2aZsywv_Ed"
   },
   "source": [
    "START_TOKEN → vocab_size以内で使用していないidの数（0はパディングと被るのでやめたほうが良い）　\n",
    "\n",
    "SEQ_LENGTH 20 →　15 今回の入力は15単語で固定\n",
    "\n",
    "dis_num_filtersとdis_filter_sizesの配列の最後を消去\n",
    "\n",
    "generated_num →　入力データの数と合わせる\n",
    "\n",
    "vocab_size →　観光地の575での語彙数より大きくなるように調整\n",
    "\n",
    "rewards = rollout.get_reward(samples, 16 →　12, discriminator)\n",
    "\n",
    "PRE_EPOCH_NUM と　識別器の事前学習の回数は変わりうる（結果をみながら調整が必要）\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wy5ZR-_av_Ed"
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  生成器のパラメータ\n",
    "######################################################################################\n",
    "EMB_DIM = 32 # embedding dimension\n",
    "HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n",
    "SEQ_LENGTH = 15 # sequence length\n",
    "START_TOKEN = 1699\n",
    "PRE_EPOCH_NUM = 601 # supervise (maximum likelihood estimation) epochs\n",
    "SEED = 88\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#########################################################################################\n",
    "#  識別器のパラメータ\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "\n",
    "#########################################################################################\n",
    "#  Basic Training Parameters\n",
    "# GANの学習を実行していく\n",
    "#########################################################################################\n",
    "\n",
    "TOTAL_BATCH = 300 # 生成器と識別器の更新を300回する\n",
    "\n",
    "# 学習で使用するデータ\n",
    "# 最初は存在しないので、lstmで作るらしい\n",
    "\n",
    "# 生成器の事前学習で使用\n",
    "generator_file = 'data/id/all_575_id.txt' \n",
    "eval_file = 'data/generated/eval_file.txt' # 使えていない気がする\n",
    "\n",
    "# 識別器の学習で使用\n",
    "positive_file = 'data/id/hakataeki_575_id.txt' # 対象とする観光地の575のid\n",
    "negative_file = 'data/generated/generator_sample.txt' # 生成器が作成した偽物を使用する\n",
    "\n",
    "output_file = 'data/generated/output_file.txt'\n",
    "\n",
    "# 生成された数\n",
    "generated_num = 1024 # kgeneratorのデータ数と合わせている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFN9BImuv_Ed",
    "outputId": "6bd024a3-f467-45f2-d33b-2032d9b77fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUが使えるよ\n"
     ]
    }
   ],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "assert START_TOKEN == 1699\n",
    "\n",
    "vocab_size = 1700\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"GPUが使えるよ\")\n",
    "    # for dev in physical_devices:\n",
    "    #     tf.config.experimental.set_memory_growth(dev, True)\n",
    "else:\n",
    "    print(\"GPUは使えないよ,ごめんね\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBhGn2lcv_Ed",
    "outputId": "2d59feb2-98db-4384-fa57-c7aed33aa628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pre-training...\n",
      "Epoch 1/300\n",
      "63/63 [==============================] - 7s 7ms/step - loss: 5.6533\n",
      "Epoch 2/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.5448\n",
      "Epoch 3/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.2370\n",
      "Epoch 4/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.0891\n",
      "Epoch 5/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.9457\n",
      "Epoch 6/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.7909\n",
      "Epoch 7/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.6555\n",
      "Epoch 8/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.5599\n",
      "Epoch 9/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.4450\n",
      "Epoch 10/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.3793\n",
      "Epoch 11/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2866\n",
      "Epoch 12/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.2249\n",
      "Epoch 13/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.1851\n",
      "Epoch 14/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.1139\n",
      "Epoch 15/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0795\n",
      "Epoch 16/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0062\n",
      "Epoch 17/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.9593\n",
      "Epoch 18/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.9400\n",
      "Epoch 19/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.8777\n",
      "Epoch 20/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.8434\n",
      "Epoch 21/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.7963\n",
      "Epoch 22/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.7824\n",
      "Epoch 23/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.7561\n",
      "Epoch 24/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.7007\n",
      "Epoch 25/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.6853\n",
      "Epoch 26/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.6540\n",
      "Epoch 27/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6354\n",
      "Epoch 28/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.5905\n",
      "Epoch 29/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.5679\n",
      "Epoch 30/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5584\n",
      "Epoch 31/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.5345\n",
      "Epoch 32/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.5167\n",
      "Epoch 33/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.4971\n",
      "Epoch 34/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4816\n",
      "Epoch 35/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.4689\n",
      "Epoch 36/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.4637\n",
      "Epoch 37/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.4269\n",
      "Epoch 38/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.4119\n",
      "Epoch 39/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.4147\n",
      "Epoch 40/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.3703\n",
      "Epoch 41/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.3685\n",
      "Epoch 42/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.3557\n",
      "Epoch 43/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.3339\n",
      "Epoch 44/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.3255\n",
      "Epoch 45/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.3146\n",
      "Epoch 46/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.3100\n",
      "Epoch 47/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2957\n",
      "Epoch 48/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2824\n",
      "Epoch 49/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2645\n",
      "Epoch 50/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2664\n",
      "Epoch 51/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2580\n",
      "Epoch 52/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2448\n",
      "Epoch 53/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2306\n",
      "Epoch 54/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2154\n",
      "Epoch 55/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2082\n",
      "Epoch 56/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1924\n",
      "Epoch 57/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.2000\n",
      "Epoch 58/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1953\n",
      "Epoch 59/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1827\n",
      "Epoch 60/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1556\n",
      "Epoch 61/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1560\n",
      "Epoch 62/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1598\n",
      "Epoch 63/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1499\n",
      "Epoch 64/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1404\n",
      "Epoch 65/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1464\n",
      "Epoch 66/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.1348\n",
      "Epoch 67/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1309\n",
      "Epoch 68/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1349\n",
      "Epoch 69/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1309\n",
      "Epoch 70/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1221\n",
      "Epoch 71/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1190\n",
      "Epoch 72/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1145\n",
      "Epoch 73/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1192\n",
      "Epoch 74/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1203\n",
      "Epoch 75/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1038\n",
      "Epoch 76/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0942\n",
      "Epoch 77/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1041\n",
      "Epoch 78/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0957\n",
      "Epoch 79/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.1030\n",
      "Epoch 80/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0946\n",
      "Epoch 81/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0868\n",
      "Epoch 82/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0871\n",
      "Epoch 83/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0748\n",
      "Epoch 84/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0662\n",
      "Epoch 85/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0674\n",
      "Epoch 86/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0712\n",
      "Epoch 87/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0673\n",
      "Epoch 88/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0649\n",
      "Epoch 89/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0795\n",
      "Epoch 90/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0540\n",
      "Epoch 91/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0730\n",
      "Epoch 92/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0569\n",
      "Epoch 93/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0542\n",
      "Epoch 94/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0425\n",
      "Epoch 95/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0429\n",
      "Epoch 96/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0336\n",
      "Epoch 97/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0349\n",
      "Epoch 98/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0327\n",
      "Epoch 99/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0273\n",
      "Epoch 100/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0480\n",
      "Epoch 101/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0288\n",
      "Epoch 102/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0221\n",
      "Epoch 103/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0241\n",
      "Epoch 104/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0199\n",
      "Epoch 105/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0181\n",
      "Epoch 106/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0092\n",
      "Epoch 107/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0125\n",
      "Epoch 108/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0024\n",
      "Epoch 109/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0153\n",
      "Epoch 110/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.0187\n",
      "Epoch 111/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.0013\n",
      "Epoch 112/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9890\n",
      "Epoch 113/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9871\n",
      "Epoch 114/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9934\n",
      "Epoch 115/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9770\n",
      "Epoch 116/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9759\n",
      "Epoch 117/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9770\n",
      "Epoch 118/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9882\n",
      "Epoch 119/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9861\n",
      "Epoch 120/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9797\n",
      "Epoch 121/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9799\n",
      "Epoch 122/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9670\n",
      "Epoch 123/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9502\n",
      "Epoch 124/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9584\n",
      "Epoch 125/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9562\n",
      "Epoch 126/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9603\n",
      "Epoch 127/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9548\n",
      "Epoch 128/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9453\n",
      "Epoch 129/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9472\n",
      "Epoch 130/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9418\n",
      "Epoch 131/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9561\n",
      "Epoch 132/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9326\n",
      "Epoch 133/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9501\n",
      "Epoch 134/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9508\n",
      "Epoch 135/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9567\n",
      "Epoch 136/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9540\n",
      "Epoch 137/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9562\n",
      "Epoch 138/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9492\n",
      "Epoch 139/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9550\n",
      "Epoch 140/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9341\n",
      "Epoch 141/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9407\n",
      "Epoch 142/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9641\n",
      "Epoch 143/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9448\n",
      "Epoch 144/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9401\n",
      "Epoch 145/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9559\n",
      "Epoch 146/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9418\n",
      "Epoch 147/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9545\n",
      "Epoch 148/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9484\n",
      "Epoch 149/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9391\n",
      "Epoch 150/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9452\n",
      "Epoch 151/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9413\n",
      "Epoch 152/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9410\n",
      "Epoch 153/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9468\n",
      "Epoch 154/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9473\n",
      "Epoch 155/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9488\n",
      "Epoch 156/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9387\n",
      "Epoch 157/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9412\n",
      "Epoch 158/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9353\n",
      "Epoch 159/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9319\n",
      "Epoch 160/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9444\n",
      "Epoch 161/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9368\n",
      "Epoch 162/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9214\n",
      "Epoch 163/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9220\n",
      "Epoch 164/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9291\n",
      "Epoch 165/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9313\n",
      "Epoch 166/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9256\n",
      "Epoch 167/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9189\n",
      "Epoch 168/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9344\n",
      "Epoch 169/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9265\n",
      "Epoch 170/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9412\n",
      "Epoch 171/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9137\n",
      "Epoch 172/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9250\n",
      "Epoch 173/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9147\n",
      "Epoch 174/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9087\n",
      "Epoch 175/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9022\n",
      "Epoch 176/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9035\n",
      "Epoch 177/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9112\n",
      "Epoch 178/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9110\n",
      "Epoch 179/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9086\n",
      "Epoch 180/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9037\n",
      "Epoch 181/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9169\n",
      "Epoch 182/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9016\n",
      "Epoch 183/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9018\n",
      "Epoch 184/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8985\n",
      "Epoch 185/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8964\n",
      "Epoch 186/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9030\n",
      "Epoch 187/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8988\n",
      "Epoch 188/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8921\n",
      "Epoch 189/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9026\n",
      "Epoch 190/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8896\n",
      "Epoch 191/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8894\n",
      "Epoch 192/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8850\n",
      "Epoch 193/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8828\n",
      "Epoch 194/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8835\n",
      "Epoch 195/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8963\n",
      "Epoch 196/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8896\n",
      "Epoch 197/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8907\n",
      "Epoch 198/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8848\n",
      "Epoch 199/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9041\n",
      "Epoch 200/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8903\n",
      "Epoch 201/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9016\n",
      "Epoch 202/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9032\n",
      "Epoch 203/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8904\n",
      "Epoch 204/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9025\n",
      "Epoch 205/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9023\n",
      "Epoch 206/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8843\n",
      "Epoch 207/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9005\n",
      "Epoch 208/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8927\n",
      "Epoch 209/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8987\n",
      "Epoch 210/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8865\n",
      "Epoch 211/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8862\n",
      "Epoch 212/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8954\n",
      "Epoch 213/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9102\n",
      "Epoch 214/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9057\n",
      "Epoch 215/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9020\n",
      "Epoch 216/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8923\n",
      "Epoch 217/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8937\n",
      "Epoch 218/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8916\n",
      "Epoch 219/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9009\n",
      "Epoch 220/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9095\n",
      "Epoch 221/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8929\n",
      "Epoch 222/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8906\n",
      "Epoch 223/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.9060\n",
      "Epoch 224/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.9028\n",
      "Epoch 225/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8975\n",
      "Epoch 226/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8845\n",
      "Epoch 227/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8822\n",
      "Epoch 228/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8797\n",
      "Epoch 229/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8893\n",
      "Epoch 230/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8715\n",
      "Epoch 231/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8706\n",
      "Epoch 232/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8688\n",
      "Epoch 233/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8817\n",
      "Epoch 234/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8812\n",
      "Epoch 235/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8783\n",
      "Epoch 236/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8625\n",
      "Epoch 237/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8788\n",
      "Epoch 238/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8796\n",
      "Epoch 239/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8780\n",
      "Epoch 240/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8720\n",
      "Epoch 241/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8779\n",
      "Epoch 242/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8768\n",
      "Epoch 243/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8878\n",
      "Epoch 244/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8755\n",
      "Epoch 245/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8969\n",
      "Epoch 246/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8932\n",
      "Epoch 247/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8823\n",
      "Epoch 248/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8738\n",
      "Epoch 249/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8629\n",
      "Epoch 250/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8550\n",
      "Epoch 251/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8566\n",
      "Epoch 252/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8553\n",
      "Epoch 253/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8650\n",
      "Epoch 254/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8620\n",
      "Epoch 255/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8666\n",
      "Epoch 256/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8566\n",
      "Epoch 257/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8423\n",
      "Epoch 258/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8437\n",
      "Epoch 259/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8390\n",
      "Epoch 260/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8509\n",
      "Epoch 261/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8814\n",
      "Epoch 262/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8724\n",
      "Epoch 263/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8697\n",
      "Epoch 264/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8716\n",
      "Epoch 265/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8631\n",
      "Epoch 266/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8719\n",
      "Epoch 267/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8725\n",
      "Epoch 268/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8606\n",
      "Epoch 269/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8636\n",
      "Epoch 270/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8672\n",
      "Epoch 271/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8529\n",
      "Epoch 272/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8600\n",
      "Epoch 273/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8636\n",
      "Epoch 274/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8567\n",
      "Epoch 275/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8612\n",
      "Epoch 276/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8533\n",
      "Epoch 277/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8603\n",
      "Epoch 278/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8809\n",
      "Epoch 279/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8974\n",
      "Epoch 280/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8800\n",
      "Epoch 281/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8763\n",
      "Epoch 282/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8705\n",
      "Epoch 283/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8563\n",
      "Epoch 284/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8544\n",
      "Epoch 285/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8582\n",
      "Epoch 286/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8569\n",
      "Epoch 287/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8743\n",
      "Epoch 288/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8632\n",
      "Epoch 289/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8696\n",
      "Epoch 290/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8738\n",
      "Epoch 291/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8732\n",
      "Epoch 292/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8684\n",
      "Epoch 293/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8745\n",
      "Epoch 294/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8669\n",
      "Epoch 295/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8648\n",
      "Epoch 296/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8716\n",
      "Epoch 297/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8570\n",
      "Epoch 298/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8503\n",
      "Epoch 299/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.8432\n",
      "Epoch 300/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.8511\n",
      "Start pre-training discriminator...\n",
      "Dataset 0\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 3s 11ms/step - loss: 1.5369 - accuracy: 0.4972\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 1.3207 - accuracy: 0.5266\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 1.1604 - accuracy: 0.5680\n",
      "Dataset 1\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 1.0524 - accuracy: 0.5448\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.9580 - accuracy: 0.5670\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.8778 - accuracy: 0.6135\n",
      "Dataset 2\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.8493 - accuracy: 0.5644\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.7991 - accuracy: 0.5918\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.7620 - accuracy: 0.6100\n",
      "Dataset 3\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.7577 - accuracy: 0.5801\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.7183 - accuracy: 0.6247\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.6930 - accuracy: 0.6498\n",
      "Dataset 4\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.6956 - accuracy: 0.6224\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.6672 - accuracy: 0.6576\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.6344 - accuracy: 0.6937\n",
      "Dataset 5\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.6551 - accuracy: 0.6581\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.6087 - accuracy: 0.7113\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.5720 - accuracy: 0.7405\n",
      "Dataset 6\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.6157 - accuracy: 0.6874\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.5555 - accuracy: 0.7476\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.5080 - accuracy: 0.7860\n",
      "Dataset 7\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.5741 - accuracy: 0.7238\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.5022 - accuracy: 0.7789\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4671 - accuracy: 0.8043\n",
      "Dataset 8\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.5273 - accuracy: 0.7655\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4588 - accuracy: 0.8114\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4152 - accuracy: 0.8365\n",
      "Dataset 9\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4971 - accuracy: 0.7770\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4296 - accuracy: 0.8193\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 0.3936 - accuracy: 0.8398\n",
      "Dataset 10\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4755 - accuracy: 0.7904\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 0.4082 - accuracy: 0.8317\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3699 - accuracy: 0.8490\n",
      "Dataset 11\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4574 - accuracy: 0.7966\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4011 - accuracy: 0.8254\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3659 - accuracy: 0.8479\n",
      "Dataset 12\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4395 - accuracy: 0.8051\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3758 - accuracy: 0.8408\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3482 - accuracy: 0.8572\n",
      "Dataset 13\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4276 - accuracy: 0.8137\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3635 - accuracy: 0.8449\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3311 - accuracy: 0.8616\n",
      "Dataset 14\n",
      "Epoch 1/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.4042 - accuracy: 0.8203\n",
      "Epoch 2/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3507 - accuracy: 0.8546\n",
      "Epoch 3/3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3339 - accuracy: 0.8573\n"
     ]
    }
   ],
   "source": [
    "# 生成器で学習\n",
    "generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n",
    "target_params = pickle.load(open('data/generated/target_params_py3.pkl', 'rb'))\n",
    "target_lstm = TARGET_LSTM(BATCH_SIZE, SEQ_LENGTH, START_TOKEN, target_params, vocab_size) # The oracle model\n",
    "\n",
    "# 識別器で学習\n",
    "discriminator = Discriminator(sequence_length=SEQ_LENGTH, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim,\n",
    "                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, dropout_keep_prob=dis_dropout_keep_prob,\n",
    "                                l2_reg_lambda=dis_l2_reg_lambda)\n",
    "\n",
    "# First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n",
    "# GANの学習で使用する正解データを作成する\n",
    "if not os.path.exists(generator_file):\n",
    "    target_lstm.generate_samples(generated_num // BATCH_SIZE, generator_file)\n",
    "gen_dataset = dataset_for_generator(generator_file, BATCH_SIZE)\n",
    "log = open('data/generated/experiment-log.txt', 'w')\n",
    "\n",
    "\n",
    "#  事前学習での文章生成をlstmで行い、生成器の重みを保存する\n",
    "if not os.path.exists(\"data/generated/generator_pretrained.h5\"):\n",
    "    print('Start pre-training...')\n",
    "    log.write('pre-training...\\n')\n",
    "    generator.pretrain(gen_dataset, target_lstm, PRE_EPOCH_NUM, generated_num // BATCH_SIZE, eval_file)\n",
    "    generator.save(\"data/generated/generator_pretrained.h5\")\n",
    "else:\n",
    "    generator.load(\"data/generated/generator_pretrained.h5\")\n",
    "\n",
    "# 識別器の事前学習での重み\n",
    "if not os.path.exists(\"data/generated/discriminator_pretrained.h5\"):\n",
    "    print('Start pre-training discriminator...')\n",
    "    # Train 3 epoch on the generated data and do this for 50 times\n",
    "    # 3エポックの識別器の訓練を５０回繰り返す\n",
    "    for _ in range(15):\n",
    "        print(\"Dataset\", _)\n",
    "\n",
    "        # まず生成器が偽物を作成\n",
    "        generator.generate_samples(generated_num // BATCH_SIZE, negative_file)\n",
    "\n",
    "        # 偽物と本物を混ぜたデータセットを作成\n",
    "        dis_dataset = dataset_for_discriminator(positive_file, negative_file, BATCH_SIZE)\n",
    "\n",
    "        # 識別器を学習させる\n",
    "        discriminator.train(dis_dataset, 3, (generated_num // BATCH_SIZE) * 2)\n",
    "    discriminator.save(\"data/generated/discriminator_pretrained.h5\")\n",
    "else:\n",
    "    discriminator.load(\"data/generated/discriminator_pretrained.h5\")\n",
    "\n",
    "rollout = ROLLOUT(generator, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zr1EcYpkv_Ed",
    "outputId": "a6b3a5bf-f7f1-43a0-b8c9-28995926be6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "Start Adversarial Training...\n",
      "Generator 0\n",
      "Discriminator 0\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3593 - accuracy: 0.8497\n",
      "2888 2874 961 974 479 1659 3102 1593 0 0 0 0 0 0 0 \n",
      "2540 177 1159 7 1255 1744 1904 347 1593 0 0 0 0 0 0 \n",
      "1048 1744 3653 1744 156 1297 1634 372 1744 0 0 0 0 0 0 \n",
      "1916 372 350 2097 177 2150 2756 900 0 0 0 0 0 0 0 \n",
      "856 2854 2599 177 1659 1395 847 1744 0 0 0 0 0 0 0 \n",
      "432 2868 2059 1504 717 372 0 0 0 0 0 0 0 0 0 \n",
      "900 2696 1297 2027 1744 1314 2756 0 0 0 0 0 0 0 0 \n",
      "3148 1518 2505 1549 1631 177 0 0 0 0 0 0 0 0 0 \n",
      "3402 900 1593 961 3476 843 1509 248 0 0 0 0 0 0 0 \n",
      "2745 7 461 900 3175 1092 1719 2751 0 0 0 0 0 0 0 \n",
      "1104 961 885 2020 961 2130 2756 0 0 0 0 0 0 0 0 \n",
      "900 1599 416 758 1659 885 1950 0 0 0 0 0 0 0 0 \n",
      "643 2868 1985 2228 2893 339 1744 0 0 0 0 0 0 0 0 \n",
      "765 1744 3358 2756 900 961 2470 446 0 0 0 0 0 0 0 \n",
      "1225 1297 1754 3215 177 2893 1000 1593 0 0 0 0 0 0 0 \n",
      "27 372 1744 2655 3288 1870 3263 1297 0 0 0 0 0 0 0 \n",
      "2301 177 193 2874 961 428 2756 0 0 0 0 0 0 0 0 \n",
      "1104 7 2657 1728 2175 1728 2153 1950 0 0 0 0 0 0 0 \n",
      "1225 2505 3569 1870 3429 7 876 0 0 0 0 0 0 0 0 \n",
      "1853 221 177 1659 3328 1886 847 2756 900 0 0 0 0 0 0 \n",
      "1769 1744 3653 2756 900 1599 1352 1189 2505 2568 1659 0 0 0 0 \n",
      "3666 7 372 1744 2762 1728 1126 0 0 0 0 0 0 0 0 \n",
      "1104 2505 133 1068 843 2172 974 0 0 0 0 0 0 0 0 \n",
      "3563 2505 1933 2893 2835 2937 0 0 0 0 0 0 0 0 0 \n",
      "182 381 1297 2762 3526 1297 0 0 0 0 0 0 0 0 0 \n",
      "1225 2505 1482 1870 2998 0 0 0 0 0 0 0 0 0 0 \n",
      "3286 3665 2893 762 2138 7 0 0 0 0 0 0 0 0 0 \n",
      "885 654 2505 2247 2505 1147 7 1773 327 0 0 0 0 0 0 \n",
      "1601 7 372 1744 1904 2247 961 643 1593 0 0 0 0 0 0 \n",
      "2247 1728 1827 1759 7 1947 3165 0 0 0 0 0 0 0 0 \n",
      "109 372 1744 3101 7 2992 1297 1769 0 0 0 0 0 0 0 \n",
      "1017 2505 975 2479 900 961 885 900 3476 0 0 0 0 0 0 \n",
      "2870 1297 2976 1297 3321 1719 2868 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 4 177 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 4 177 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 4 177 0 0 0 0 0 0 0 0 \n",
      "3402 900 1593 388 204 3691 1870 885 0 0 0 0 0 0 0 \n",
      "26 1318 900 2505 2701 7 2268 0 0 0 0 0 0 0 0 \n",
      "1050 961 2073 900 2505 3148 36 2089 0 0 0 0 0 0 0 \n",
      "2888 2874 961 1707 1780 177 2371 3295 3035 0 0 0 0 0 0 \n",
      "3448 1728 1149 2527 1413 961 428 1744 1950 1488 2566 1744 0 0 0 \n",
      "2510 961 1274 1719 3268 2893 827 2237 0 0 0 0 0 0 0 \n",
      "2508 961 1359 2893 373 2756 900 0 0 0 0 0 0 0 0 \n",
      "870 961 885 2056 1477 2505 2056 0 0 0 0 0 0 0 0 \n",
      "2795 2505 2247 961 1651 2505 2247 2505 2919 961 0 0 0 0 0 \n",
      "2409 177 1365 1297 1761 900 2505 1593 961 1953 0 0 0 0 0 \n",
      "2467 2022 372 1744 1904 793 961 327 0 0 0 0 0 0 0 \n",
      "267 54 2870 7 1769 177 386 2756 0 0 0 0 0 0 0 \n",
      "3445 1744 1761 1744 1659 403 843 0 0 0 0 0 0 0 0 \n",
      "721 1297 3194 635 177 1130 164 0 0 0 0 0 0 0 0 \n",
      "1111 2938 7 3596 1744 3467 1744 0 0 0 0 0 0 0 0 \n",
      "2870 2505 1540 1659 885 1359 2505 1258 177 0 0 0 0 0 0 \n",
      "2741 1744 1406 2401 2183 961 1400 1593 0 0 0 0 0 0 0 \n",
      "2751 381 1297 3079 1744 192 2520 1670 0 0 0 0 0 0 0 \n",
      "421 2505 70 3194 1389 177 3579 54 0 0 0 0 0 0 0 \n",
      "2736 177 327 2983 1719 2868 2223 2854 2237 0 0 0 0 0 0 \n",
      "3505 372 2756 900 961 2175 2063 1593 0 0 0 0 0 0 0 \n",
      "793 1646 2609 1712 1744 286 2505 0 0 0 0 0 0 0 0 \n",
      "3333 1659 1651 1984 2505 3268 1659 0 0 0 0 0 0 0 0 \n",
      "1629 2505 2247 961 1651 1113 2169 0 0 0 0 0 0 0 0 \n",
      "913 2505 1630 961 498 1828 2505 2250 2763 0 0 0 0 0 0 \n",
      "258 3428 1297 2000 2505 961 2305 1719 388 2711 2961 3532 0 0 0 \n",
      "877 2893 2000 1242 3467 900 2505 961 1603 177 1131 0 0 0 0 \n",
      "2336 900 1593 961 449 1719 3323 961 643 1593 0 0 0 0 0 \n",
      "\n",
      "0.8949751\n",
      "0.8575037\n",
      "0.8376699\n",
      "0.8367136\n",
      "0.7271977\n",
      "0.6665015\n",
      "0.6559915\n",
      "0.6285328\n",
      "0.609946\n",
      "0.58251697\n",
      "0.57518893\n",
      "0.48575857\n",
      "0.47236493\n",
      "0.4449658\n",
      "0.44475713\n",
      "0.434988\n",
      "0.42510086\n",
      "0.42296022\n",
      "0.30127537\n",
      "0.2644324\n",
      "0.25687462\n",
      "0.24653506\n",
      "0.23885666\n",
      "0.21012345\n",
      "0.18810806\n",
      "0.17072178\n",
      "0.16105795\n",
      "0.14088823\n",
      "0.12923175\n",
      "0.12590843\n",
      "0.116936535\n",
      "0.08034529\n",
      "0.07919001\n",
      "0.07447398\n",
      "0.07447398\n",
      "0.07447398\n",
      "0.06869535\n",
      "0.060391977\n",
      "0.058544897\n",
      "0.055827126\n",
      "0.05183334\n",
      "0.041676234\n",
      "0.04092458\n",
      "0.03633238\n",
      "0.030528717\n",
      "0.029689597\n",
      "0.028917545\n",
      "0.027125103\n",
      "0.02563257\n",
      "0.024296116\n",
      "0.019741477\n",
      "0.01888213\n",
      "0.01749083\n",
      "0.014993218\n",
      "0.012585553\n",
      "0.0096761\n",
      "0.008283361\n",
      "0.0070702294\n",
      "0.006201295\n",
      "0.0024783488\n",
      "0.0023400057\n",
      "0.0007263404\n",
      "0.0004584549\n",
      "0.00027299556\n",
      "\n",
      "Generator 1\n",
      "Discriminator 1\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.3219 - accuracy: 0.8726\n",
      "Generator 2\n",
      "Discriminator 2\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.2983 - accuracy: 0.8884\n",
      "Generator 3\n",
      "Discriminator 3\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.2850 - accuracy: 0.8989\n",
      "Generator 4\n",
      "Discriminator 4\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.2741 - accuracy: 0.8981\n",
      "Generator 5\n",
      "Discriminator 5\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.2471 - accuracy: 0.9182\n",
      "Generator 6\n",
      "Discriminator 6\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.2440 - accuracy: 0.9220\n",
      "Generator 7\n",
      "Discriminator 7\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.2312 - accuracy: 0.9266\n",
      "Generator 8\n",
      "Discriminator 8\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.2077 - accuracy: 0.9402\n",
      "Generator 9\n",
      "Discriminator 9\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1840 - accuracy: 0.9479\n",
      "Generator 10\n",
      "Discriminator 10\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1858 - accuracy: 0.9490\n",
      "Generator 11\n",
      "Discriminator 11\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1864 - accuracy: 0.9515\n",
      "Generator 12\n",
      "Discriminator 12\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1839 - accuracy: 0.9495\n",
      "Generator 13\n",
      "Discriminator 13\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1774 - accuracy: 0.9524\n",
      "Generator 14\n",
      "Discriminator 14\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1628 - accuracy: 0.9562\n",
      "Generator 15\n",
      "Discriminator 15\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1670 - accuracy: 0.9547\n",
      "Generator 16\n",
      "Discriminator 16\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1460 - accuracy: 0.9613\n",
      "Generator 17\n",
      "Discriminator 17\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.1469 - accuracy: 0.9637\n",
      "Generator 18\n",
      "Discriminator 18\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1503 - accuracy: 0.9618\n",
      "Generator 19\n",
      "Discriminator 19\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1501 - accuracy: 0.9599\n",
      "Generator 20\n",
      "Discriminator 20\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1415 - accuracy: 0.9623\n",
      "Generator 21\n",
      "Discriminator 21\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1453 - accuracy: 0.9622\n",
      "Generator 22\n",
      "Discriminator 22\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1413 - accuracy: 0.9639\n",
      "Generator 23\n",
      "Discriminator 23\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1333 - accuracy: 0.9648\n",
      "Generator 24\n",
      "Discriminator 24\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1399 - accuracy: 0.9627\n",
      "Generator 25\n",
      "Discriminator 25\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1398 - accuracy: 0.9638\n",
      "Generator 26\n",
      "Discriminator 26\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1434 - accuracy: 0.9630\n",
      "Generator 27\n",
      "Discriminator 27\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1447 - accuracy: 0.9629\n",
      "Generator 28\n",
      "Discriminator 28\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.1542 - accuracy: 0.9594\n",
      "Generator 29\n",
      "Discriminator 29\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1411 - accuracy: 0.9622\n",
      "Generator 30\n",
      "Discriminator 30\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1335 - accuracy: 0.9668\n",
      "Generator 31\n",
      "Discriminator 31\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1297 - accuracy: 0.9666\n",
      "Generator 32\n",
      "Discriminator 32\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1291 - accuracy: 0.9669\n",
      "Generator 33\n",
      "Discriminator 33\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1201 - accuracy: 0.9712\n",
      "Generator 34\n",
      "Discriminator 34\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.1162 - accuracy: 0.9702\n",
      "Generator 35\n",
      "Discriminator 35\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1267 - accuracy: 0.9652\n",
      "Generator 36\n",
      "Discriminator 36\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1302 - accuracy: 0.9679\n",
      "Generator 37\n",
      "Discriminator 37\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1244 - accuracy: 0.9660\n",
      "Generator 38\n",
      "Discriminator 38\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1181 - accuracy: 0.9707\n",
      "Generator 39\n",
      "Discriminator 39\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.1066 - accuracy: 0.9728\n",
      "Generator 40\n",
      "Discriminator 40\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.1120 - accuracy: 0.9709\n",
      "Generator 41\n",
      "Discriminator 41\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1059 - accuracy: 0.9747\n",
      "Generator 42\n",
      "Discriminator 42\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1075 - accuracy: 0.9756\n",
      "Generator 43\n",
      "Discriminator 43\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1014 - accuracy: 0.9740\n",
      "Generator 44\n",
      "Discriminator 44\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1008 - accuracy: 0.9751\n",
      "Generator 45\n",
      "Discriminator 45\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1031 - accuracy: 0.9747\n",
      "Generator 46\n",
      "Discriminator 46\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1036 - accuracy: 0.9759\n",
      "Generator 47\n",
      "Discriminator 47\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1030 - accuracy: 0.9769\n",
      "Generator 48\n",
      "Discriminator 48\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0962 - accuracy: 0.9769\n",
      "Generator 49\n",
      "Discriminator 49\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0998 - accuracy: 0.9768\n",
      "Generator 50\n",
      "Discriminator 50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0979 - accuracy: 0.9781\n",
      "2185 372 1744 1904 2247 1297 3717 1744 0 0 0 0 0 0 0 \n",
      "159 3072 177 2034 3245 1562 177 0 0 0 0 0 0 0 0 \n",
      "3688 2505 1711 1870 2185 372 0 0 0 0 0 0 0 0 0 \n",
      "2141 1659 885 853 1728 2153 1950 0 0 0 0 0 0 0 0 \n",
      "3467 900 2247 961 1651 2467 2022 885 0 0 0 0 0 0 0 \n",
      "72 2505 2948 961 1904 2868 0 0 0 0 0 0 0 0 0 \n",
      "1406 2588 2588 2588 2617 2505 947 2505 0 0 0 0 0 0 0 \n",
      "817 103 2505 1622 7 1773 372 0 0 0 0 0 0 0 0 \n",
      "1104 1297 1622 7 1491 350 0 0 0 0 0 0 0 0 0 \n",
      "2428 248 2505 3550 3485 1593 0 0 0 0 0 0 0 0 0 \n",
      "1431 2893 383 7 2596 372 0 0 0 0 0 0 0 0 0 \n",
      "72 2505 2948 961 643 2868 0 0 0 0 0 0 0 0 0 \n",
      "1225 2505 1693 1728 64 1006 2508 0 0 0 0 0 0 0 0 \n",
      "2350 2163 2505 3149 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1769 177 892 1744 1904 1719 1728 3028 0 0 0 0 0 0 0 \n",
      "417 1659 2801 1744 557 3674 1581 177 0 0 0 0 0 0 0 \n",
      "84 1728 2114 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1930 961 1756 1744 1904 2868 2810 0 0 0 0 0 0 0 0 \n",
      "2581 961 3108 1728 668 1744 437 7 231 0 0 0 0 0 0 \n",
      "2768 1719 1622 961 3402 900 0 0 0 0 0 0 0 0 0 \n",
      "332 372 1744 1904 3268 961 643 1593 0 0 0 0 0 0 0 \n",
      "1287 1659 2655 821 999 961 885 0 0 0 0 0 0 0 0 \n",
      "1104 2505 1622 7 3563 1297 0 0 0 0 0 0 0 0 0 \n",
      "790 2505 1842 286 1034 177 0 0 0 0 0 0 0 0 0 \n",
      "790 2505 1842 286 1034 177 0 0 0 0 0 0 0 0 0 \n",
      "885 2371 2505 823 721 1719 1072 0 0 0 0 0 0 0 0 \n",
      "1104 2893 3714 1593 961 2889 2893 0 0 0 0 0 0 0 0 \n",
      "2017 1870 2923 1022 3679 1728 3207 0 0 0 0 0 0 0 0 \n",
      "1104 7 1721 350 3015 1933 0 0 0 0 0 0 0 0 0 \n",
      "1739 843 248 1659 511 372 1744 1761 1950 0 0 0 0 0 0 \n",
      "2185 372 1744 1904 2247 961 643 1593 0 0 0 0 0 0 0 \n",
      "2185 372 1744 1904 2247 961 643 1593 0 0 0 0 0 0 0 \n",
      "1104 7 1721 350 521 372 0 0 0 0 0 0 0 0 0 \n",
      "817 103 2505 1622 7 1491 350 0 0 0 0 0 0 0 0 \n",
      "817 103 2505 1622 7 1491 350 0 0 0 0 0 0 0 0 \n",
      "1104 7 3428 1190 3634 1728 0 0 0 0 0 0 0 0 0 \n",
      "2581 961 3108 1728 668 1297 2530 1719 876 0 0 0 0 0 0 \n",
      "2568 7 152 2169 1728 2474 1593 961 1254 0 0 0 0 0 0 \n",
      "825 961 885 1011 177 1314 164 0 0 0 0 0 0 0 0 \n",
      "1911 2505 175 1006 1654 1337 0 0 0 0 0 0 0 0 0 \n",
      "955 1870 3520 1886 847 1744 2756 900 0 0 0 0 0 0 0 \n",
      "2351 177 3365 177 2817 961 0 0 0 0 0 0 0 0 0 \n",
      "2351 177 3365 177 2817 961 0 0 0 0 0 0 0 0 0 \n",
      "404 2505 1739 961 974 3620 0 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 1072 0 0 0 0 0 0 0 0 0 \n",
      "2768 1719 1622 961 643 1593 0 0 0 0 0 0 0 0 0 \n",
      "1551 1886 847 2756 900 3448 3448 1728 2153 0 0 0 0 0 0 \n",
      "2871 2505 2410 1297 609 722 2724 0 0 0 0 0 0 0 0 \n",
      "1406 1100 2505 2453 7 765 1744 3358 2756 900 0 0 0 0 0 \n",
      "1713 177 961 2854 2237 372 0 0 0 0 0 0 0 0 0 \n",
      "1104 2893 3714 1593 961 2889 0 0 0 0 0 0 0 0 0 \n",
      "3114 2346 324 342 2034 734 1319 0 0 0 0 0 0 0 0 \n",
      "1761 1744 2474 961 1836 3079 1744 2108 0 0 0 0 0 0 0 \n",
      "424 1711 1870 2505 3461 177 0 0 0 0 0 0 0 0 0 \n",
      "2141 961 885 1950 1599 1599 2390 0 0 0 0 0 0 0 0 \n",
      "2925 2505 1706 7 177 308 1744 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 2983 0 0 0 0 0 0 0 0 0 \n",
      "817 103 2505 1622 961 3402 900 0 0 0 0 0 0 0 0 \n",
      "2808 2983 961 974 2983 1593 0 0 0 0 0 0 0 0 0 \n",
      "428 1289 3194 3078 177 308 1744 0 0 0 0 0 0 0 0 \n",
      "3359 2893 142 2505 3646 0 0 0 0 0 0 0 0 0 0 \n",
      "3057 520 7 2237 2169 2505 3448 961 0 0 0 0 0 0 0 \n",
      "2315 177 372 1744 1922 372 2065 961 1707 0 0 0 0 0 0 \n",
      "2836 2505 2726 2960 2997 1744 1761 1950 1870 0 0 0 0 0 0 \n",
      "\n",
      "0.9932781\n",
      "0.9017692\n",
      "0.40192327\n",
      "0.36234632\n",
      "0.3360863\n",
      "0.32880893\n",
      "0.25293666\n",
      "0.23657621\n",
      "0.20921087\n",
      "0.20167673\n",
      "0.16716257\n",
      "0.1642995\n",
      "0.1459879\n",
      "0.14304817\n",
      "0.12476146\n",
      "0.1135351\n",
      "0.11185976\n",
      "0.1036113\n",
      "0.10138931\n",
      "0.083384916\n",
      "0.08289426\n",
      "0.08115332\n",
      "0.07697231\n",
      "0.07557391\n",
      "0.07557391\n",
      "0.070645966\n",
      "0.06357184\n",
      "0.05449105\n",
      "0.050641477\n",
      "0.04505\n",
      "0.037416868\n",
      "0.037416868\n",
      "0.036903642\n",
      "0.033733007\n",
      "0.033733007\n",
      "0.03149098\n",
      "0.029066032\n",
      "0.027990352\n",
      "0.024145259\n",
      "0.02306454\n",
      "0.0216473\n",
      "0.013142979\n",
      "0.013142979\n",
      "0.012914856\n",
      "0.011984463\n",
      "0.011593641\n",
      "0.011238064\n",
      "0.010999364\n",
      "0.010939695\n",
      "0.010117318\n",
      "0.009882514\n",
      "0.0077914256\n",
      "0.0059099793\n",
      "0.0054139076\n",
      "0.0036177144\n",
      "0.0032042384\n",
      "0.0023677151\n",
      "0.0023542526\n",
      "0.002269203\n",
      "0.0011945479\n",
      "0.001137079\n",
      "0.0007364793\n",
      "0.00025595858\n",
      "0.00012934314\n",
      "\n",
      "Generator 51\n",
      "Discriminator 51\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.1008 - accuracy: 0.9773\n",
      "Generator 52\n",
      "Discriminator 52\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.1024 - accuracy: 0.9776\n",
      "Generator 53\n",
      "Discriminator 53\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0992 - accuracy: 0.9769\n",
      "Generator 54\n",
      "Discriminator 54\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.1046 - accuracy: 0.9750\n",
      "Generator 55\n",
      "Discriminator 55\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0913 - accuracy: 0.9779\n",
      "Generator 56\n",
      "Discriminator 56\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0965 - accuracy: 0.9777\n",
      "Generator 57\n",
      "Discriminator 57\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0903 - accuracy: 0.9797\n",
      "Generator 58\n",
      "Discriminator 58\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0924 - accuracy: 0.9778\n",
      "Generator 59\n",
      "Discriminator 59\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0872 - accuracy: 0.9804\n",
      "Generator 60\n",
      "Discriminator 60\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0919 - accuracy: 0.9794\n",
      "Generator 61\n",
      "Discriminator 61\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0953 - accuracy: 0.9787\n",
      "Generator 62\n",
      "Discriminator 62\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0920 - accuracy: 0.9789\n",
      "Generator 63\n",
      "Discriminator 63\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0878 - accuracy: 0.9789\n",
      "Generator 64\n",
      "Discriminator 64\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0879 - accuracy: 0.9773\n",
      "Generator 65\n",
      "Discriminator 65\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0903 - accuracy: 0.9782\n",
      "Generator 66\n",
      "Discriminator 66\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0905 - accuracy: 0.9777\n",
      "Generator 67\n",
      "Discriminator 67\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0862 - accuracy: 0.9787\n",
      "Generator 68\n",
      "Discriminator 68\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0882 - accuracy: 0.9814\n",
      "Generator 69\n",
      "Discriminator 69\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0844 - accuracy: 0.9813\n",
      "Generator 70\n",
      "Discriminator 70\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0885 - accuracy: 0.9809\n",
      "Generator 71\n",
      "Discriminator 71\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0903 - accuracy: 0.9787\n",
      "Generator 72\n",
      "Discriminator 72\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0868 - accuracy: 0.9816\n",
      "Generator 73\n",
      "Discriminator 73\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0868 - accuracy: 0.9819\n",
      "Generator 74\n",
      "Discriminator 74\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0878 - accuracy: 0.9809\n",
      "Generator 75\n",
      "Discriminator 75\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0841 - accuracy: 0.9804\n",
      "Generator 76\n",
      "Discriminator 76\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0862 - accuracy: 0.9807\n",
      "Generator 77\n",
      "Discriminator 77\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0898 - accuracy: 0.9803\n",
      "Generator 78\n",
      "Discriminator 78\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0824 - accuracy: 0.9819\n",
      "Generator 79\n",
      "Discriminator 79\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0881 - accuracy: 0.9810\n",
      "Generator 80\n",
      "Discriminator 80\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0890 - accuracy: 0.9803\n",
      "Generator 81\n",
      "Discriminator 81\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0838 - accuracy: 0.9825\n",
      "Generator 82\n",
      "Discriminator 82\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0883 - accuracy: 0.9820\n",
      "Generator 83\n",
      "Discriminator 83\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0817 - accuracy: 0.9814\n",
      "Generator 84\n",
      "Discriminator 84\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0852 - accuracy: 0.9808\n",
      "Generator 85\n",
      "Discriminator 85\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0854 - accuracy: 0.9804\n",
      "Generator 86\n",
      "Discriminator 86\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0852 - accuracy: 0.9810\n",
      "Generator 87\n",
      "Discriminator 87\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0877 - accuracy: 0.9807\n",
      "Generator 88\n",
      "Discriminator 88\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0862 - accuracy: 0.9807\n",
      "Generator 89\n",
      "Discriminator 89\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0853 - accuracy: 0.9805\n",
      "Generator 90\n",
      "Discriminator 90\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0836 - accuracy: 0.9808\n",
      "Generator 91\n",
      "Discriminator 91\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0872 - accuracy: 0.9797\n",
      "Generator 92\n",
      "Discriminator 92\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0822 - accuracy: 0.9799\n",
      "Generator 93\n",
      "Discriminator 93\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0805 - accuracy: 0.9812\n",
      "Generator 94\n",
      "Discriminator 94\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0774 - accuracy: 0.9831\n",
      "Generator 95\n",
      "Discriminator 95\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0726 - accuracy: 0.9823\n",
      "Generator 96\n",
      "Discriminator 96\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0714 - accuracy: 0.9824\n",
      "Generator 97\n",
      "Discriminator 97\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0712 - accuracy: 0.9831\n",
      "Generator 98\n",
      "Discriminator 98\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0661 - accuracy: 0.9833\n",
      "Generator 99\n",
      "Discriminator 99\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0661 - accuracy: 0.9830\n",
      "Generator 100\n",
      "Discriminator 100\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0649 - accuracy: 0.9847\n",
      "2473 2691 2505 1208 2505 1706 2505 2139 0 0 0 0 0 0 0 \n",
      "2315 372 1591 121 2065 961 3414 1950 961 0 0 0 0 0 0 \n",
      "2390 2893 1449 1644 2505 3188 1659 0 0 0 0 0 0 0 0 \n",
      "1722 1297 612 1566 1744 1000 1593 0 0 0 0 0 0 0 0 \n",
      "2291 2505 2948 1297 339 1744 0 0 0 0 0 0 0 0 0 \n",
      "1739 1068 248 7 1974 305 2763 2505 0 0 0 0 0 0 0 \n",
      "1593 1870 3448 1728 3013 2153 2756 900 0 0 0 0 0 0 0 \n",
      "3332 1728 2079 961 1405 177 0 0 0 0 0 0 0 0 0 \n",
      "1310 2505 347 177 3140 2490 177 1767 0 0 0 0 0 0 0 \n",
      "72 2505 2948 961 643 2868 0 0 0 0 0 0 0 0 0 \n",
      "72 2505 2948 961 643 2868 0 0 0 0 0 0 0 0 0 \n",
      "1104 2893 3714 1593 961 2889 2893 0 0 0 0 0 0 0 0 \n",
      "1104 2893 1374 2352 961 643 372 0 0 0 0 0 0 0 0 \n",
      "3114 2604 765 177 2893 2167 1728 2153 0 0 0 0 0 0 0 \n",
      "1104 2893 1374 2352 1297 1739 0 0 0 0 0 0 0 0 0 \n",
      "1104 2893 1374 2352 1297 1739 0 0 0 0 0 0 0 0 0 \n",
      "2810 2505 2059 1213 164 900 2874 1659 511 372 900 0 0 0 0 \n",
      "489 1719 2868 1738 2505 3331 164 0 0 0 0 0 0 0 0 \n",
      "661 1659 3187 177 3390 2756 900 0 0 0 0 0 0 0 0 \n",
      "72 2505 2948 961 3402 900 0 0 0 0 0 0 0 0 0 \n",
      "2898 961 3003 3448 1870 2011 1034 177 0 0 0 0 0 0 0 \n",
      "1104 961 885 2020 961 2130 2756 0 0 0 0 0 0 0 0 \n",
      "2568 1659 1128 177 885 2756 900 0 0 0 0 0 0 0 0 \n",
      "1739 843 248 1659 3518 885 1000 0 0 0 0 0 0 0 0 \n",
      "2305 1297 2390 2472 1954 1744 1314 0 0 0 0 0 0 0 0 \n",
      "2351 177 3365 177 2817 961 0 0 0 0 0 0 0 0 0 \n",
      "2351 177 3365 177 2817 961 0 0 0 0 0 0 0 0 0 \n",
      "2351 177 3365 177 2817 961 0 0 0 0 0 0 0 0 0 \n",
      "1104 7 1721 350 521 372 0 0 0 0 0 0 0 0 0 \n",
      "3359 2893 142 1870 2505 1353 0 0 0 0 0 0 0 0 0 \n",
      "3364 2893 2167 2874 1659 2766 2153 2756 0 0 0 0 0 0 0 \n",
      "1104 7 1721 350 3015 1933 0 0 0 0 0 0 0 0 0 \n",
      "1104 7 1721 350 3015 1933 0 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 4 177 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 4 177 0 0 0 0 0 0 0 0 \n",
      "2088 1719 306 900 2568 7 0 0 0 0 0 0 0 0 0 \n",
      "3334 7 866 961 2130 2756 0 0 0 0 0 0 0 0 0 \n",
      "3047 3285 1659 2030 586 1744 1666 0 0 0 0 0 0 0 0 \n",
      "1329 1719 2751 1659 2608 847 1744 1761 1950 0 0 0 0 0 0 \n",
      "1915 7 217 1886 847 1744 1761 1950 0 0 0 0 0 0 0 \n",
      "2568 7 2900 1744 3032 900 0 0 0 0 0 0 0 0 0 \n",
      "1629 2505 1111 961 885 2858 2868 1245 177 0 0 0 0 0 0 \n",
      "788 2868 1934 8 2505 2663 1297 996 0 0 0 0 0 0 0 \n",
      "1739 1068 248 7 2022 372 0 0 0 0 0 0 0 0 0 \n",
      "3334 7 866 961 2373 1719 1728 0 0 0 0 0 0 0 0 \n",
      "1970 1659 122 3675 2505 3268 2505 1149 0 0 0 0 0 0 0 \n",
      "36 1861 3688 486 1979 1297 0 0 0 0 0 0 0 0 0 \n",
      "3518 2505 2247 2893 854 961 643 1593 0 0 0 0 0 0 0 \n",
      "3535 177 2893 2171 511 372 1744 1904 2017 0 0 0 0 0 0 \n",
      "3121 2034 2331 2505 789 1297 0 0 0 0 0 0 0 0 0 \n",
      "1935 1659 3616 917 7 1491 350 0 0 0 0 0 0 0 0 \n",
      "766 1744 1859 2505 1149 2878 961 885 1540 961 885 0 0 0 0 \n",
      "1970 1659 817 999 1416 7 3389 177 0 0 0 0 0 0 0 \n",
      "2303 2505 1863 1659 2599 586 47 0 0 0 0 0 0 0 0 \n",
      "2410 1659 1738 961 307 372 0 0 0 0 0 0 0 0 0 \n",
      "520 177 1728 2948 961 643 2868 2831 0 0 0 0 0 0 0 \n",
      "1739 7 3445 1744 2828 1593 0 0 0 0 0 0 0 0 0 \n",
      "482 2684 900 2751 1659 1651 807 1999 372 0 0 0 0 0 0 \n",
      "1534 1728 1706 366 36 3000 1297 2237 372 0 0 0 0 0 0 \n",
      "2726 3072 2684 1744 1954 2910 1728 743 900 1593 0 0 0 0 0 \n",
      "1400 1719 1728 1711 286 177 0 0 0 0 0 0 0 0 0 \n",
      "1830 961 2073 900 1728 3375 1337 511 1728 0 0 0 0 0 0 \n",
      "282 1744 2711 1297 1659 1072 2505 2474 1728 0 0 0 0 0 0 \n",
      "482 366 177 3599 2910 529 2337 1670 0 0 0 0 0 0 0 \n",
      "\n",
      "0.23829609\n",
      "0.096622\n",
      "0.09468489\n",
      "0.08769777\n",
      "0.079259515\n",
      "0.059083227\n",
      "0.053570393\n",
      "0.05186903\n",
      "0.04851079\n",
      "0.035675664\n",
      "0.035675664\n",
      "0.032569733\n",
      "0.02308337\n",
      "0.021626156\n",
      "0.02149733\n",
      "0.02149733\n",
      "0.021051327\n",
      "0.017905282\n",
      "0.017046964\n",
      "0.016902767\n",
      "0.016089348\n",
      "0.015638597\n",
      "0.013915111\n",
      "0.009682018\n",
      "0.008828758\n",
      "0.008455156\n",
      "0.008455156\n",
      "0.008455156\n",
      "0.008196232\n",
      "0.0061372574\n",
      "0.005234403\n",
      "0.004913636\n",
      "0.004913636\n",
      "0.0047969176\n",
      "0.0047969176\n",
      "0.003634693\n",
      "0.0035041661\n",
      "0.0032770915\n",
      "0.0030462255\n",
      "0.002840684\n",
      "0.0028243447\n",
      "0.0021618311\n",
      "0.0021040274\n",
      "0.0019956077\n",
      "0.0019292524\n",
      "0.0019003177\n",
      "0.0018862555\n",
      "0.0013483583\n",
      "0.0011529229\n",
      "0.00072484574\n",
      "0.00068373105\n",
      "0.0006623044\n",
      "0.0005995358\n",
      "0.0005397507\n",
      "0.00051089004\n",
      "0.0004565913\n",
      "0.0004048692\n",
      "0.0003935919\n",
      "0.00017536561\n",
      "8.1918624e-05\n",
      "5.8395304e-05\n",
      "4.392932e-05\n",
      "3.5381043e-05\n",
      "2.1759395e-05\n",
      "\n",
      "Generator 101\n",
      "Discriminator 101\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0790 - accuracy: 0.9825\n",
      "Generator 102\n",
      "Discriminator 102\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0719 - accuracy: 0.9849\n",
      "Generator 103\n",
      "Discriminator 103\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0744 - accuracy: 0.9838\n",
      "Generator 104\n",
      "Discriminator 104\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0688 - accuracy: 0.9862\n",
      "Generator 105\n",
      "Discriminator 105\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0727 - accuracy: 0.9839\n",
      "Generator 106\n",
      "Discriminator 106\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0732 - accuracy: 0.9829\n",
      "Generator 107\n",
      "Discriminator 107\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0711 - accuracy: 0.9831\n",
      "Generator 108\n",
      "Discriminator 108\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0726 - accuracy: 0.9839\n",
      "Generator 109\n",
      "Discriminator 109\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0754 - accuracy: 0.9839\n",
      "Generator 110\n",
      "Discriminator 110\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0762 - accuracy: 0.9839\n",
      "Generator 111\n",
      "Discriminator 111\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0732 - accuracy: 0.9847\n",
      "Generator 112\n",
      "Discriminator 112\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0722 - accuracy: 0.9836\n",
      "Generator 113\n",
      "Discriminator 113\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0724 - accuracy: 0.9843\n",
      "Generator 114\n",
      "Discriminator 114\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0740 - accuracy: 0.9841\n",
      "Generator 115\n",
      "Discriminator 115\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0701 - accuracy: 0.9845\n",
      "Generator 116\n",
      "Discriminator 116\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0710 - accuracy: 0.9857\n",
      "Generator 117\n",
      "Discriminator 117\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0667 - accuracy: 0.9846\n",
      "Generator 118\n",
      "Discriminator 118\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0694 - accuracy: 0.9850\n",
      "Generator 119\n",
      "Discriminator 119\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0642 - accuracy: 0.9860\n",
      "Generator 120\n",
      "Discriminator 120\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0651 - accuracy: 0.9864\n",
      "Generator 121\n",
      "Discriminator 121\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0729 - accuracy: 0.9844\n",
      "Generator 122\n",
      "Discriminator 122\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0745 - accuracy: 0.9845\n",
      "Generator 123\n",
      "Discriminator 123\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0694 - accuracy: 0.9852\n",
      "Generator 124\n",
      "Discriminator 124\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0704 - accuracy: 0.9846\n",
      "Generator 125\n",
      "Discriminator 125\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0680 - accuracy: 0.9862\n",
      "Generator 126\n",
      "Discriminator 126\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0666 - accuracy: 0.9854\n",
      "Generator 127\n",
      "Discriminator 127\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0643 - accuracy: 0.9864\n",
      "Generator 128\n",
      "Discriminator 128\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0659 - accuracy: 0.9855\n",
      "Generator 129\n",
      "Discriminator 129\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0646 - accuracy: 0.9871\n",
      "Generator 130\n",
      "Discriminator 130\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0648 - accuracy: 0.9865\n",
      "Generator 131\n",
      "Discriminator 131\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0655 - accuracy: 0.9857\n",
      "Generator 132\n",
      "Discriminator 132\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0659 - accuracy: 0.9861\n",
      "Generator 133\n",
      "Discriminator 133\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0647 - accuracy: 0.9870\n",
      "Generator 134\n",
      "Discriminator 134\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0667 - accuracy: 0.9856\n",
      "Generator 135\n",
      "Discriminator 135\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0645 - accuracy: 0.9860\n",
      "Generator 136\n",
      "Discriminator 136\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0609 - accuracy: 0.9878\n",
      "Generator 137\n",
      "Discriminator 137\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0631 - accuracy: 0.9875\n",
      "Generator 138\n",
      "Discriminator 138\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0625 - accuracy: 0.9877\n",
      "Generator 139\n",
      "Discriminator 139\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0683 - accuracy: 0.9860\n",
      "Generator 140\n",
      "Discriminator 140\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0693 - accuracy: 0.9843\n",
      "Generator 141\n",
      "Discriminator 141\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0634 - accuracy: 0.9865\n",
      "Generator 142\n",
      "Discriminator 142\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0612 - accuracy: 0.9872\n",
      "Generator 143\n",
      "Discriminator 143\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0623 - accuracy: 0.9869\n",
      "Generator 144\n",
      "Discriminator 144\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0659 - accuracy: 0.9865\n",
      "Generator 145\n",
      "Discriminator 145\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0603 - accuracy: 0.9876\n",
      "Generator 146\n",
      "Discriminator 146\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0627 - accuracy: 0.9857\n",
      "Generator 147\n",
      "Discriminator 147\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0605 - accuracy: 0.9871\n",
      "Generator 148\n",
      "Discriminator 148\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0603 - accuracy: 0.9867\n",
      "Generator 149\n",
      "Discriminator 149\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0629 - accuracy: 0.9861\n",
      "Generator 150\n",
      "Discriminator 150\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0608 - accuracy: 0.9872\n",
      "2934 1068 248 7 1773 7 1199 7 0 0 0 0 0 0 0 \n",
      "1231 2505 2948 961 3402 900 0 0 0 0 0 0 0 0 0 \n",
      "1974 7 2805 350 1979 1252 0 0 0 0 0 0 0 0 0 \n",
      "1739 961 2855 1289 2169 2874 1659 885 0 0 0 0 0 0 0 \n",
      "1739 1068 248 1659 511 372 1744 1761 1950 0 0 0 0 0 0 \n",
      "3101 961 1779 1870 3233 900 1301 0 0 0 0 0 0 0 0 \n",
      "100 1659 3353 2608 847 1744 1761 1950 0 0 0 0 0 0 0 \n",
      "100 1659 3353 2608 847 1744 1761 1950 0 0 0 0 0 0 0 \n",
      "2390 2893 1449 1644 2505 3188 1659 0 0 0 0 0 0 0 0 \n",
      "2390 2893 1449 1644 2505 3188 1659 0 0 0 0 0 0 0 0 \n",
      "2803 1728 2833 2071 3389 0 0 0 0 0 0 0 0 0 0 \n",
      "2803 1728 2833 2071 3389 0 0 0 0 0 0 0 0 0 0 \n",
      "1225 2505 1693 1728 64 1006 995 0 0 0 0 0 0 0 0 \n",
      "1287 177 3452 2505 2340 0 0 0 0 0 0 0 0 0 0 \n",
      "1739 1068 248 843 1838 1068 248 2763 0 0 0 0 0 0 0 \n",
      "838 2505 1622 961 1115 1297 0 0 0 0 0 0 0 0 0 \n",
      "1911 961 2024 847 1744 450 961 0 0 0 0 0 0 0 0 \n",
      "3364 2893 3078 1821 3222 0 0 0 0 0 0 0 0 0 0 \n",
      "479 3368 647 256 1744 1761 0 0 0 0 0 0 0 0 0 \n",
      "1254 900 1728 194 1912 2505 0 0 0 0 0 0 0 0 0 \n",
      "1104 2893 1374 2352 1297 1739 0 0 0 0 0 0 0 0 0 \n",
      "2445 1419 1786 961 1670 0 0 0 0 0 0 0 0 0 0 \n",
      "1104 2893 3714 1593 961 2889 2893 0 0 0 0 0 0 0 0 \n",
      "1931 3345 3612 44 2505 3633 0 0 0 0 0 0 0 0 0 \n",
      "1931 3345 3612 44 2505 3633 0 0 0 0 0 0 0 0 0 \n",
      "1722 1297 612 1566 1744 1000 1593 0 0 0 0 0 0 0 0 \n",
      "1722 1297 612 1566 1744 1000 1593 0 0 0 0 0 0 0 0 \n",
      "1908 7 362 1744 1870 1531 177 0 0 0 0 0 0 0 0 \n",
      "421 2505 70 1297 3194 1389 1593 0 0 0 0 0 0 0 0 \n",
      "1214 2147 2505 2599 0 0 0 0 0 0 0 0 0 0 0 \n",
      "234 2505 99 2604 765 164 900 0 0 0 0 0 0 0 0 \n",
      "19 843 234 1870 2505 99 2505 0 0 0 0 0 0 0 0 \n",
      "51 2696 1066 2289 1006 630 3035 0 0 0 0 0 0 0 0 \n",
      "817 999 1870 1659 2237 372 0 0 0 0 0 0 0 0 0 \n",
      "142 2505 2948 961 643 2505 0 0 0 0 0 0 0 0 0 \n",
      "432 2948 1297 2857 2505 0 0 0 0 0 0 0 0 0 0 \n",
      "2351 177 3365 177 2817 961 0 0 0 0 0 0 0 0 0 \n",
      "861 2192 2868 1031 177 735 350 0 0 0 0 0 0 0 0 \n",
      "1739 7 209 177 150 2874 961 428 0 0 0 0 0 0 0 \n",
      "234 2505 922 1607 3709 177 0 0 0 0 0 0 0 0 0 \n",
      "1348 2410 2223 661 1702 961 0 0 0 0 0 0 0 0 0 \n",
      "817 999 1870 1659 2854 2237 0 0 0 0 0 0 0 0 0 \n",
      "1728 918 2505 3 2367 0 0 0 0 0 0 0 0 0 0 \n",
      "234 2505 3474 2505 877 164 900 2868 0 0 0 0 0 0 0 \n",
      "1072 7 2380 2874 961 428 2756 900 0 0 0 0 0 0 0 \n",
      "37 2893 1938 961 2095 1719 2870 0 0 0 0 0 0 0 0 \n",
      "286 7 3494 1744 1904 3337 961 0 0 0 0 0 0 0 0 \n",
      "1974 2505 1622 2893 838 0 0 0 0 0 0 0 0 0 0 \n",
      "72 2505 2948 961 643 1593 0 0 0 0 0 0 0 0 0 \n",
      "327 1870 2948 961 643 2868 0 0 0 0 0 0 0 0 0 \n",
      "432 2948 2505 1072 961 0 0 0 0 0 0 0 0 0 0 \n",
      "1935 2505 2948 961 1178 1593 0 0 0 0 0 0 0 0 0 \n",
      "2910 1593 109 1711 1711 0 0 0 0 0 0 0 0 0 0 \n",
      "3388 961 2139 3035 2756 900 0 0 0 0 0 0 0 0 0 \n",
      "3505 1870 2923 1022 1063 588 3413 177 1314 0 0 0 0 0 0 \n",
      "838 2505 2183 961 428 2474 1689 177 0 0 0 0 0 0 0 \n",
      "58 1593 961 3265 2505 2706 7 0 0 0 0 0 0 0 0 \n",
      "825 961 885 1011 177 732 1297 0 0 0 0 0 0 0 0 \n",
      "1769 177 1325 2108 3617 372 2756 900 0 0 0 0 0 0 0 \n",
      "8 2505 1119 177 171 550 2243 1886 847 1744 0 0 0 0 0 \n",
      "2153 2756 900 1599 1599 2247 1297 2662 177 0 0 0 0 0 0 \n",
      "1935 2505 1630 1659 2914 177 3664 961 2335 0 0 0 0 0 0 \n",
      "2167 366 2893 817 999 961 3622 0 0 0 0 0 0 0 0 \n",
      "2305 1297 2390 2472 1954 3022 372 2756 0 0 0 0 0 0 0 \n",
      "\n",
      "0.070282124\n",
      "0.06683775\n",
      "0.054362644\n",
      "0.048237197\n",
      "0.031299543\n",
      "0.03085855\n",
      "0.02934914\n",
      "0.02934914\n",
      "0.029146276\n",
      "0.029146276\n",
      "0.028698301\n",
      "0.028698301\n",
      "0.027982825\n",
      "0.025865711\n",
      "0.023472253\n",
      "0.023438878\n",
      "0.022445844\n",
      "0.022182211\n",
      "0.021527974\n",
      "0.020230616\n",
      "0.01995292\n",
      "0.019134644\n",
      "0.019001497\n",
      "0.017249519\n",
      "0.017249519\n",
      "0.017202962\n",
      "0.017202962\n",
      "0.016842593\n",
      "0.015614547\n",
      "0.014055048\n",
      "0.013978939\n",
      "0.011987635\n",
      "0.011418786\n",
      "0.01133419\n",
      "0.010776015\n",
      "0.010032794\n",
      "0.009212186\n",
      "0.009181618\n",
      "0.0073604756\n",
      "0.006228608\n",
      "0.0061584148\n",
      "0.0060871183\n",
      "0.005996389\n",
      "0.005949845\n",
      "0.0053966986\n",
      "0.0052864016\n",
      "0.005249939\n",
      "0.0051418054\n",
      "0.004922585\n",
      "0.004747324\n",
      "0.004677316\n",
      "0.0036305527\n",
      "0.003525805\n",
      "0.0032636484\n",
      "0.0032356372\n",
      "0.0021670538\n",
      "0.002116221\n",
      "0.0011258299\n",
      "0.0011159424\n",
      "0.0007664166\n",
      "0.0006166456\n",
      "0.00028557048\n",
      "0.00018108629\n",
      "2.0556993e-06\n",
      "\n",
      "Generator 151\n",
      "Discriminator 151\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0647 - accuracy: 0.9867\n",
      "Generator 152\n",
      "Discriminator 152\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0625 - accuracy: 0.9872\n",
      "Generator 153\n",
      "Discriminator 153\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0596 - accuracy: 0.9870\n",
      "Generator 154\n",
      "Discriminator 154\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0617 - accuracy: 0.9874\n",
      "Generator 155\n",
      "Discriminator 155\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0653 - accuracy: 0.9861\n",
      "Generator 156\n",
      "Discriminator 156\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0670 - accuracy: 0.9850\n",
      "Generator 157\n",
      "Discriminator 157\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0638 - accuracy: 0.9851\n",
      "Generator 158\n",
      "Discriminator 158\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0637 - accuracy: 0.9854\n",
      "Generator 159\n",
      "Discriminator 159\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0637 - accuracy: 0.9857\n",
      "Generator 160\n",
      "Discriminator 160\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0627 - accuracy: 0.9865\n",
      "Generator 161\n",
      "Discriminator 161\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0625 - accuracy: 0.9865\n",
      "Generator 162\n",
      "Discriminator 162\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 0.0658 - accuracy: 0.9854\n",
      "Generator 163\n",
      "Discriminator 163\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0651 - accuracy: 0.9862\n",
      "Generator 164\n",
      "Discriminator 164\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0618 - accuracy: 0.9870\n",
      "Generator 165\n",
      "Discriminator 165\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0606 - accuracy: 0.9860\n",
      "Generator 166\n",
      "Discriminator 166\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0625 - accuracy: 0.9867\n",
      "Generator 167\n",
      "Discriminator 167\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0590 - accuracy: 0.9871\n",
      "Generator 168\n",
      "Discriminator 168\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0633 - accuracy: 0.9860\n",
      "Generator 169\n",
      "Discriminator 169\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0604 - accuracy: 0.9864\n",
      "Generator 170\n",
      "Discriminator 170\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0618 - accuracy: 0.9861\n",
      "Generator 171\n",
      "Discriminator 171\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0620 - accuracy: 0.9866\n",
      "Generator 172\n",
      "Discriminator 172\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0652 - accuracy: 0.9859\n",
      "Generator 173\n",
      "Discriminator 173\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0599 - accuracy: 0.9865\n",
      "Generator 174\n",
      "Discriminator 174\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0603 - accuracy: 0.9865\n",
      "Generator 175\n",
      "Discriminator 175\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0638 - accuracy: 0.9856\n",
      "Generator 176\n",
      "Discriminator 176\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0617 - accuracy: 0.9864\n",
      "Generator 177\n",
      "Discriminator 177\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0624 - accuracy: 0.9856\n",
      "Generator 178\n",
      "Discriminator 178\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0586 - accuracy: 0.9861\n",
      "Generator 179\n",
      "Discriminator 179\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0586 - accuracy: 0.9872\n",
      "Generator 180\n",
      "Discriminator 180\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0598 - accuracy: 0.9866\n",
      "Generator 181\n",
      "Discriminator 181\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0599 - accuracy: 0.9871\n",
      "Generator 182\n",
      "Discriminator 182\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0555 - accuracy: 0.9877\n",
      "Generator 183\n",
      "Discriminator 183\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0545 - accuracy: 0.9877\n",
      "Generator 184\n",
      "Discriminator 184\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0575 - accuracy: 0.9871\n",
      "Generator 185\n",
      "Discriminator 185\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0567 - accuracy: 0.9876\n",
      "Generator 186\n",
      "Discriminator 186\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0604 - accuracy: 0.9869\n",
      "Generator 187\n",
      "Discriminator 187\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0568 - accuracy: 0.9881\n",
      "Generator 188\n",
      "Discriminator 188\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0606 - accuracy: 0.9881\n",
      "Generator 189\n",
      "Discriminator 189\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0591 - accuracy: 0.9880\n",
      "Generator 190\n",
      "Discriminator 190\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0568 - accuracy: 0.9874\n",
      "Generator 191\n",
      "Discriminator 191\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0587 - accuracy: 0.9876\n",
      "Generator 192\n",
      "Discriminator 192\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 0.0562 - accuracy: 0.9882\n",
      "Generator 193\n",
      "Discriminator 193\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0542 - accuracy: 0.9886\n",
      "Generator 194\n",
      "Discriminator 194\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0580 - accuracy: 0.9877\n",
      "Generator 195\n",
      "Discriminator 195\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0533 - accuracy: 0.9881\n",
      "Generator 196\n",
      "Discriminator 196\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0566 - accuracy: 0.9878\n",
      "Generator 197\n",
      "Discriminator 197\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0553 - accuracy: 0.9875\n",
      "Generator 198\n",
      "Discriminator 198\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0571 - accuracy: 0.9875\n",
      "Generator 199\n",
      "Discriminator 199\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0581 - accuracy: 0.9870\n",
      "Generator 200\n",
      "Discriminator 200\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0579 - accuracy: 0.9877\n",
      "1629 2505 2948 1297 339 1744 0 0 0 0 0 0 0 0 0 \n",
      "36 1861 3688 486 1979 1297 313 0 0 0 0 0 0 0 0 \n",
      "36 1861 3688 486 1979 1297 313 0 0 0 0 0 0 0 0 \n",
      "234 2505 2948 961 3402 900 0 0 0 0 0 0 0 0 0 \n",
      "51 2696 1066 2289 1006 630 3035 163 0 0 0 0 0 0 0 \n",
      "51 2696 1066 2289 1006 630 3035 163 0 0 0 0 0 0 0 \n",
      "1931 3345 3612 44 2505 3633 0 0 0 0 0 0 0 0 0 \n",
      "1931 3345 3612 44 2505 3633 0 0 0 0 0 0 0 0 0 \n",
      "1722 1297 612 1566 1744 1000 1593 0 0 0 0 0 0 0 0 \n",
      "1254 900 1728 194 1912 2505 0 0 0 0 0 0 0 0 0 \n",
      "2803 1728 2446 7 569 1297 0 0 0 0 0 0 0 0 0 \n",
      "2803 1728 2446 7 569 1297 0 0 0 0 0 0 0 0 0 \n",
      "1214 2147 2505 2599 0 0 0 0 0 0 0 0 0 0 0 \n",
      "790 2505 1842 286 1034 177 0 0 0 0 0 0 0 0 0 \n",
      "790 2505 1842 286 1034 177 0 0 0 0 0 0 0 0 0 \n",
      "3518 2505 2948 1297 339 1744 0 0 0 0 0 0 0 0 0 \n",
      "149 177 2745 7 461 900 3175 0 0 0 0 0 0 0 0 \n",
      "661 1659 3187 177 3390 2756 900 0 0 0 0 0 0 0 0 \n",
      "661 1659 3187 177 3390 2756 900 0 0 0 0 0 0 0 0 \n",
      "661 1659 3187 177 3390 2756 900 0 0 0 0 0 0 0 0 \n",
      "1974 7 2805 350 1979 1252 0 0 0 0 0 0 0 0 0 \n",
      "175 1006 1874 3627 177 2153 2433 2505 961 0 0 0 0 0 0 \n",
      "175 1006 1874 3627 177 2153 2433 2505 961 0 0 0 0 0 0 \n",
      "2568 2505 1738 961 307 372 0 0 0 0 0 0 0 0 0 \n",
      "2568 2505 1738 961 307 372 0 0 0 0 0 0 0 0 0 \n",
      "2351 177 3365 177 2817 961 0 0 0 0 0 0 0 0 0 \n",
      "1739 1068 248 843 1974 305 2763 2505 0 0 0 0 0 0 0 \n",
      "1911 961 2024 847 1744 450 961 0 0 0 0 0 0 0 0 \n",
      "1911 961 2024 847 1744 450 961 0 0 0 0 0 0 0 0 \n",
      "1739 1068 248 1659 3518 885 1950 372 0 0 0 0 0 0 0 \n",
      "1974 2505 1622 1659 2335 0 0 0 0 0 0 0 0 0 0 \n",
      "1974 2505 1622 7 1491 350 0 0 0 0 0 0 0 0 0 \n",
      "1974 2505 1622 7 1491 350 0 0 0 0 0 0 0 0 0 \n",
      "3286 3665 2893 762 2138 7 0 0 0 0 0 0 0 0 0 \n",
      "1231 2505 2948 961 3402 900 0 0 0 0 0 0 0 0 0 \n",
      "2390 2893 1449 1644 2505 3188 1659 0 0 0 0 0 0 0 0 \n",
      "2390 2893 1449 1644 2505 3188 1659 0 0 0 0 0 0 0 0 \n",
      "2390 2893 1449 1644 2505 3188 1659 0 0 0 0 0 0 0 0 \n",
      "2034 2247 3366 1693 961 2162 1719 2868 0 0 0 0 0 0 0 \n",
      "2529 2505 2985 78 2768 1719 1009 0 0 0 0 0 0 0 0 \n",
      "2871 2505 1693 1728 64 1006 995 0 0 0 0 0 0 0 0 \n",
      "1739 1068 248 961 511 372 1591 2247 0 0 0 0 0 0 0 \n",
      "2697 1886 1670 2022 0 0 0 0 0 0 0 0 0 0 0 \n",
      "100 1659 3353 2608 847 1744 1761 1950 0 0 0 0 0 0 0 \n",
      "100 1659 3353 2608 847 1744 1761 1950 0 0 0 0 0 0 0 \n",
      "100 1659 3353 2608 847 1744 1761 1950 0 0 0 0 0 0 0 \n",
      "152 961 2474 433 177 3390 900 0 0 0 0 0 0 0 0 \n",
      "152 177 483 1318 900 900 1593 0 0 0 0 0 0 0 0 \n",
      "1853 3366 1693 2505 877 164 900 0 0 0 0 0 0 0 0 \n",
      "1237 1744 1761 1744 2247 1297 1531 2505 2474 2153 0 0 0 0 0 \n",
      "1739 177 3255 3136 366 2100 177 3057 0 0 0 0 0 0 0 \n",
      "2376 2505 881 177 2853 847 0 0 0 0 0 0 0 0 0 \n",
      "109 482 177 3 2505 3646 0 0 0 0 0 0 0 0 0 \n",
      "838 2505 3291 2505 2948 961 3402 900 0 0 0 0 0 0 0 \n",
      "661 961 3087 900 372 1744 2756 900 0 0 0 0 0 0 0 \n",
      "661 1659 3518 885 2056 2505 1689 0 0 0 0 0 0 0 0 \n",
      "1187 2247 1297 386 2756 900 961 0 0 0 0 0 0 0 0 \n",
      "1310 2505 347 1870 902 3001 7 3163 1297 0 0 0 0 0 0 \n",
      "2303 2505 2751 1659 2608 847 1744 3194 0 0 0 0 0 0 0 \n",
      "100 2505 1630 961 1839 3539 372 1551 0 0 0 0 0 0 0 \n",
      "997 159 2505 1593 2225 900 1719 2017 2505 1853 0 0 0 0 0 \n",
      "100 961 2465 2927 2505 3438 7 247 3263 3263 0 0 0 0 0 \n",
      "234 2505 891 388 630 3035 2756 0 0 0 0 0 0 0 0 \n",
      "3321 1719 2192 1400 1719 636 961 0 0 0 0 0 0 0 0 \n",
      "\n",
      "0.027118504\n",
      "0.023307083\n",
      "0.023307083\n",
      "0.023294136\n",
      "0.019862533\n",
      "0.019862533\n",
      "0.019519016\n",
      "0.019519016\n",
      "0.019100593\n",
      "0.01900659\n",
      "0.018203778\n",
      "0.018203778\n",
      "0.018022\n",
      "0.017565869\n",
      "0.017565869\n",
      "0.017280975\n",
      "0.016879681\n",
      "0.016832458\n",
      "0.016832458\n",
      "0.016832458\n",
      "0.016469147\n",
      "0.016248958\n",
      "0.016248958\n",
      "0.015221121\n",
      "0.015221121\n",
      "0.0144425025\n",
      "0.013301106\n",
      "0.012475045\n",
      "0.012475045\n",
      "0.012326517\n",
      "0.011536628\n",
      "0.011294802\n",
      "0.011294802\n",
      "0.011049331\n",
      "0.00805027\n",
      "0.0069481363\n",
      "0.0069481363\n",
      "0.0069481363\n",
      "0.0068006907\n",
      "0.00600819\n",
      "0.005603055\n",
      "0.0047844606\n",
      "0.0044555613\n",
      "0.0042114463\n",
      "0.0042114463\n",
      "0.0042114463\n",
      "0.003917878\n",
      "0.0037370052\n",
      "0.0036581533\n",
      "0.003590247\n",
      "0.0028780415\n",
      "0.0027920483\n",
      "0.0015617849\n",
      "0.001450706\n",
      "0.0013424946\n",
      "0.00089864765\n",
      "0.0006492157\n",
      "0.000540943\n",
      "0.0005302384\n",
      "0.00017379121\n",
      "8.814501e-05\n",
      "6.951521e-05\n",
      "4.1135558e-05\n",
      "4.5867705e-06\n",
      "\n",
      "Generator 201\n",
      "Discriminator 201\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0551 - accuracy: 0.9874\n",
      "Generator 202\n",
      "Discriminator 202\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0662 - accuracy: 0.9862\n",
      "Generator 203\n",
      "Discriminator 203\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0568 - accuracy: 0.9878\n",
      "Generator 204\n",
      "Discriminator 204\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0551 - accuracy: 0.9878\n",
      "Generator 205\n",
      "Discriminator 205\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0594 - accuracy: 0.9871\n",
      "Generator 206\n",
      "Discriminator 206\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0552 - accuracy: 0.9869\n",
      "Generator 207\n",
      "Discriminator 207\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0543 - accuracy: 0.9886\n",
      "Generator 208\n",
      "Discriminator 208\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0581 - accuracy: 0.9874\n",
      "Generator 209\n",
      "Discriminator 209\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0577 - accuracy: 0.9875\n",
      "Generator 210\n",
      "Discriminator 210\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0579 - accuracy: 0.9871\n",
      "Generator 211\n",
      "Discriminator 211\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0551 - accuracy: 0.9883\n",
      "Generator 212\n",
      "Discriminator 212\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0537 - accuracy: 0.9886\n",
      "Generator 213\n",
      "Discriminator 213\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0589 - accuracy: 0.9876\n",
      "Generator 214\n",
      "Discriminator 214\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0543 - accuracy: 0.9883\n",
      "Generator 215\n",
      "Discriminator 215\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0592 - accuracy: 0.9874\n",
      "Generator 216\n",
      "Discriminator 216\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0608 - accuracy: 0.9866\n",
      "Generator 217\n",
      "Discriminator 217\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0541 - accuracy: 0.9885\n",
      "Generator 218\n",
      "Discriminator 218\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0551 - accuracy: 0.9885\n",
      "Generator 219\n",
      "Discriminator 219\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0569 - accuracy: 0.9877\n",
      "Generator 220\n",
      "Discriminator 220\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0571 - accuracy: 0.9877\n",
      "Generator 221\n",
      "Discriminator 221\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0547 - accuracy: 0.9876\n",
      "Generator 222\n",
      "Discriminator 222\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0545 - accuracy: 0.9887\n",
      "Generator 223\n",
      "Discriminator 223\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0548 - accuracy: 0.9880\n",
      "Generator 224\n",
      "Discriminator 224\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0601 - accuracy: 0.9861\n",
      "Generator 225\n",
      "Discriminator 225\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0574 - accuracy: 0.9875\n",
      "Generator 226\n",
      "Discriminator 226\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0582 - accuracy: 0.9871\n",
      "Generator 227\n",
      "Discriminator 227\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0574 - accuracy: 0.9877\n",
      "Generator 228\n",
      "Discriminator 228\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0600 - accuracy: 0.9876\n",
      "Generator 229\n",
      "Discriminator 229\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0540 - accuracy: 0.9880\n",
      "Generator 230\n",
      "Discriminator 230\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0548 - accuracy: 0.9883\n",
      "Generator 231\n",
      "Discriminator 231\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0523 - accuracy: 0.9883\n",
      "Generator 232\n",
      "Discriminator 232\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0530 - accuracy: 0.9887\n",
      "Generator 233\n",
      "Discriminator 233\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0551 - accuracy: 0.9885\n",
      "Generator 234\n",
      "Discriminator 234\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0548 - accuracy: 0.9877\n",
      "Generator 235\n",
      "Discriminator 235\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0550 - accuracy: 0.9880\n",
      "Generator 236\n",
      "Discriminator 236\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0555 - accuracy: 0.9883\n",
      "Generator 237\n",
      "Discriminator 237\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0538 - accuracy: 0.9887\n",
      "Generator 238\n",
      "Discriminator 238\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0553 - accuracy: 0.9880\n",
      "Generator 239\n",
      "Discriminator 239\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0560 - accuracy: 0.9871\n",
      "Generator 240\n",
      "Discriminator 240\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0530 - accuracy: 0.9890\n",
      "Generator 241\n",
      "Discriminator 241\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0562 - accuracy: 0.9887\n",
      "Generator 242\n",
      "Discriminator 242\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0544 - accuracy: 0.9875\n",
      "Generator 243\n",
      "Discriminator 243\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0584 - accuracy: 0.9876\n",
      "Generator 244\n",
      "Discriminator 244\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0564 - accuracy: 0.9885\n",
      "Generator 245\n",
      "Discriminator 245\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0510 - accuracy: 0.9886\n",
      "Generator 246\n",
      "Discriminator 246\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0508 - accuracy: 0.9893\n",
      "Generator 247\n",
      "Discriminator 247\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0554 - accuracy: 0.9880\n",
      "Generator 248\n",
      "Discriminator 248\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0556 - accuracy: 0.9885\n",
      "Generator 249\n",
      "Discriminator 249\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0548 - accuracy: 0.9883\n",
      "Generator 250\n",
      "Discriminator 250\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0599 - accuracy: 0.9872\n",
      "109 372 1744 1904 2247 1659 0 0 0 0 0 0 0 0 0 \n",
      "2658 961 137 177 109 0 0 0 0 0 0 0 0 0 0 \n",
      "1931 3345 3612 44 2505 3633 0 0 0 0 0 0 0 0 0 \n",
      "2351 177 3365 177 2817 961 0 0 0 0 0 0 0 0 0 \n",
      "2351 177 3365 177 2817 961 0 0 0 0 0 0 0 0 0 \n",
      "2803 1728 2446 7 569 1297 0 0 0 0 0 0 0 0 0 \n",
      "479 3368 647 256 1744 1761 0 0 0 0 0 0 0 0 0 \n",
      "776 177 3486 961 1670 0 0 0 0 0 0 0 0 0 0 \n",
      "776 177 3486 961 1670 0 0 0 0 0 0 0 0 0 0 \n",
      "1688 961 1645 1744 1761 1950 961 3265 0 0 0 0 0 0 0 \n",
      "1739 961 2855 1289 2169 2874 1659 885 0 0 0 0 0 0 0 \n",
      "3476 2893 2739 2958 164 900 2016 1719 0 0 0 0 0 0 0 \n",
      "1911 961 2024 847 1744 450 961 0 0 0 0 0 0 0 0 \n",
      "1911 961 2024 847 1744 450 961 0 0 0 0 0 0 0 0 \n",
      "2568 2505 1738 961 307 372 0 0 0 0 0 0 0 0 0 \n",
      "175 1006 1874 3627 177 2153 2433 2505 961 0 0 0 0 0 0 \n",
      "175 1006 1874 3627 177 2153 2433 2505 961 0 0 0 0 0 0 \n",
      "175 1006 1874 3627 177 2153 2433 2505 961 0 0 0 0 0 0 \n",
      "1629 2505 2948 1297 339 1744 0 0 0 0 0 0 0 0 0 \n",
      "1629 2505 2948 1297 339 1744 0 0 0 0 0 0 0 0 0 \n",
      "100 1659 974 3331 177 1659 0 0 0 0 0 0 0 0 0 \n",
      "3594 54 2874 7 2596 2179 1950 0 0 0 0 0 0 0 0 \n",
      "2390 2893 1449 1644 2505 3188 1659 0 0 0 0 0 0 0 0 \n",
      "1028 372 900 1728 718 2505 3267 2479 0 0 0 0 0 0 0 \n",
      "1028 372 900 1728 718 2505 3267 2479 0 0 0 0 0 0 0 \n",
      "1214 2147 2505 2599 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1214 2147 2505 2599 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1739 1068 248 1659 511 372 1744 1761 1950 0 0 0 0 0 0 \n",
      "1254 900 1728 194 1912 2505 0 0 0 0 0 0 0 0 0 \n",
      "1254 900 1728 194 1912 2505 0 0 0 0 0 0 0 0 0 \n",
      "1254 900 1728 194 1912 2505 0 0 0 0 0 0 0 0 0 \n",
      "3518 2505 2948 1297 339 1744 0 0 0 0 0 0 0 0 0 \n",
      "1431 2893 383 7 2596 372 0 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 4 177 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 4 177 0 0 0 0 0 0 0 0 \n",
      "1104 177 3428 2452 1719 4 177 0 0 0 0 0 0 0 0 \n",
      "1527 900 3461 2452 1719 1072 0 0 0 0 0 0 0 0 0 \n",
      "1527 900 3461 2452 1719 1072 0 0 0 0 0 0 0 0 0 \n",
      "1527 900 3461 2452 1719 1072 0 0 0 0 0 0 0 0 0 \n",
      "2305 1297 2390 2472 1954 1744 1314 0 0 0 0 0 0 0 0 \n",
      "2305 1297 2390 2472 1954 1744 1314 0 0 0 0 0 0 0 0 \n",
      "19 843 234 1870 2505 99 2505 0 0 0 0 0 0 0 0 \n",
      "19 843 234 1870 2505 99 2505 0 0 0 0 0 0 0 0 \n",
      "19 843 234 1870 2505 99 2505 0 0 0 0 0 0 0 0 \n",
      "1104 177 1175 3616 3197 248 0 0 0 0 0 0 0 0 0 \n",
      "1974 2505 1622 7 1491 350 0 0 0 0 0 0 0 0 0 \n",
      "327 1870 2923 1022 3679 1728 3207 0 0 0 0 0 0 0 0 \n",
      "3286 3665 2893 762 2138 7 0 0 0 0 0 0 0 0 0 \n",
      "3286 3665 2893 762 2138 7 0 0 0 0 0 0 0 0 0 \n",
      "3286 3665 2893 762 2138 7 0 0 0 0 0 0 0 0 0 \n",
      "234 2505 99 2604 765 164 900 0 0 0 0 0 0 0 0 \n",
      "234 2505 99 2604 765 164 900 0 0 0 0 0 0 0 0 \n",
      "234 2505 99 2604 765 164 900 0 0 0 0 0 0 0 0 \n",
      "1739 843 3078 961 592 0 0 0 0 0 0 0 0 0 0 \n",
      "2912 2505 1622 7 1491 350 0 0 0 0 0 0 0 0 0 \n",
      "142 2505 2948 996 521 428 0 0 0 0 0 0 0 0 0 \n",
      "51 2696 1066 2289 1006 630 3035 0 0 0 0 0 0 0 0 \n",
      "717 372 1744 1904 3337 961 2277 0 0 0 0 0 0 0 0 \n",
      "2390 2893 122 3395 1719 0 0 0 0 0 0 0 0 0 0 \n",
      "1739 1068 248 843 1838 1068 248 2763 2763 0 0 0 0 0 0 \n",
      "1739 843 2763 1659 511 3079 1551 177 1769 0 0 0 0 0 0 \n",
      "1974 2505 1622 2893 838 7 3637 248 1659 0 0 0 0 0 0 \n",
      "705 2505 989 177 1659 2773 372 2167 2451 1593 0 0 0 0 0 \n",
      "3359 2505 1693 2893 272 529 905 2505 3466 3271 2505 847 1950 3271 900 \n",
      "\n",
      "0.2433265\n",
      "0.036215045\n",
      "0.028994082\n",
      "0.02750388\n",
      "0.02750388\n",
      "0.02679011\n",
      "0.025485516\n",
      "0.024848284\n",
      "0.024848284\n",
      "0.021141617\n",
      "0.02018201\n",
      "0.019363953\n",
      "0.018640049\n",
      "0.018640049\n",
      "0.017012935\n",
      "0.016779346\n",
      "0.016779346\n",
      "0.016779346\n",
      "0.016708847\n",
      "0.016708847\n",
      "0.016626975\n",
      "0.016384725\n",
      "0.016324088\n",
      "0.016012596\n",
      "0.016012596\n",
      "0.015580069\n",
      "0.015580069\n",
      "0.015133656\n",
      "0.014375685\n",
      "0.014375685\n",
      "0.014375685\n",
      "0.013433303\n",
      "0.013209862\n",
      "0.013078988\n",
      "0.013078988\n",
      "0.013078988\n",
      "0.0129293725\n",
      "0.0129293725\n",
      "0.0129293725\n",
      "0.011547417\n",
      "0.011547417\n",
      "0.009059659\n",
      "0.009059659\n",
      "0.009059659\n",
      "0.0075520957\n",
      "0.007387102\n",
      "0.007301897\n",
      "0.006965612\n",
      "0.006965612\n",
      "0.006965612\n",
      "0.006342737\n",
      "0.006342737\n",
      "0.006342737\n",
      "0.006031924\n",
      "0.003987173\n",
      "0.002279776\n",
      "0.0013444525\n",
      "0.0012437067\n",
      "0.0010968195\n",
      "0.0009944921\n",
      "0.00012437365\n",
      "1.6039277e-05\n",
      "3.3890592e-07\n",
      "2.2380073e-07\n",
      "\n",
      "Generator 251\n",
      "Discriminator 251\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0563 - accuracy: 0.9878\n",
      "Generator 252\n",
      "Discriminator 252\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0535 - accuracy: 0.9892\n",
      "Generator 253\n",
      "Discriminator 253\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0516 - accuracy: 0.9896\n",
      "Generator 254\n",
      "Discriminator 254\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0511 - accuracy: 0.9896\n",
      "Generator 255\n",
      "Discriminator 255\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0516 - accuracy: 0.9897\n",
      "Generator 256\n",
      "Discriminator 256\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0526 - accuracy: 0.9890\n",
      "Generator 257\n",
      "Discriminator 257\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0576 - accuracy: 0.9883\n",
      "Generator 258\n",
      "Discriminator 258\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0535 - accuracy: 0.9887\n",
      "Generator 259\n",
      "Discriminator 259\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0557 - accuracy: 0.9880\n",
      "Generator 260\n",
      "Discriminator 260\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0533 - accuracy: 0.9885\n",
      "Generator 261\n",
      "Discriminator 261\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0494 - accuracy: 0.9896\n",
      "Generator 262\n",
      "Discriminator 262\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0518 - accuracy: 0.9893\n",
      "Generator 263\n",
      "Discriminator 263\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0566 - accuracy: 0.9872\n",
      "Generator 264\n",
      "Discriminator 264\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0534 - accuracy: 0.9887\n",
      "Generator 265\n",
      "Discriminator 265\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0559 - accuracy: 0.9887\n",
      "Generator 266\n",
      "Discriminator 266\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0546 - accuracy: 0.9880\n",
      "Generator 267\n",
      "Discriminator 267\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0508 - accuracy: 0.9893\n",
      "Generator 268\n",
      "Discriminator 268\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0504 - accuracy: 0.9895\n",
      "Generator 269\n",
      "Discriminator 269\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0571 - accuracy: 0.9878\n",
      "Generator 270\n",
      "Discriminator 270\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0559 - accuracy: 0.9878\n",
      "Generator 271\n",
      "Discriminator 271\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0572 - accuracy: 0.9885\n",
      "Generator 272\n",
      "Discriminator 272\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0566 - accuracy: 0.9881\n",
      "Generator 273\n",
      "Discriminator 273\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0539 - accuracy: 0.9886\n",
      "Generator 274\n",
      "Discriminator 274\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0555 - accuracy: 0.9885\n",
      "Generator 275\n",
      "Discriminator 275\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0554 - accuracy: 0.9874\n",
      "Generator 276\n",
      "Discriminator 276\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0551 - accuracy: 0.9882\n",
      "Generator 277\n",
      "Discriminator 277\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0551 - accuracy: 0.9882\n",
      "Generator 278\n",
      "Discriminator 278\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0566 - accuracy: 0.9877\n",
      "Generator 279\n",
      "Discriminator 279\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0530 - accuracy: 0.9888\n",
      "Generator 280\n",
      "Discriminator 280\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0526 - accuracy: 0.9895\n",
      "Generator 281\n",
      "Discriminator 281\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0516 - accuracy: 0.9895\n",
      "Generator 282\n",
      "Discriminator 282\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0555 - accuracy: 0.9886\n",
      "Generator 283\n",
      "Discriminator 283\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0512 - accuracy: 0.9891\n",
      "Generator 284\n",
      "Discriminator 284\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0533 - accuracy: 0.9891\n",
      "Generator 285\n",
      "Discriminator 285\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0552 - accuracy: 0.9878\n",
      "Generator 286\n",
      "Discriminator 286\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0565 - accuracy: 0.9885\n",
      "Generator 287\n",
      "Discriminator 287\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0536 - accuracy: 0.9887\n",
      "Generator 288\n",
      "Discriminator 288\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0520 - accuracy: 0.9892\n",
      "Generator 289\n",
      "Discriminator 289\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0514 - accuracy: 0.9891\n",
      "Generator 290\n",
      "Discriminator 290\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0532 - accuracy: 0.9886\n",
      "Generator 291\n",
      "Discriminator 291\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0533 - accuracy: 0.9885\n",
      "Generator 292\n",
      "Discriminator 292\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0555 - accuracy: 0.9883\n",
      "Generator 293\n",
      "Discriminator 293\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0545 - accuracy: 0.9885\n",
      "Generator 294\n",
      "Discriminator 294\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0563 - accuracy: 0.9885\n",
      "Generator 295\n",
      "Discriminator 295\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0576 - accuracy: 0.9885\n",
      "Generator 296\n",
      "Discriminator 296\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 0.0532 - accuracy: 0.9891\n",
      "Generator 297\n",
      "Discriminator 297\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0515 - accuracy: 0.9891\n",
      "Generator 298\n",
      "Discriminator 298\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0583 - accuracy: 0.9874\n",
      "Generator 299\n",
      "Discriminator 299\n",
      "126/126 [==============================] - 2s 13ms/step - loss: 0.0542 - accuracy: 0.9891\n",
      "[[0.8497023582458496], [0.8726438283920288], [0.8883928656578064], [0.8989335298538208], [0.898065447807312], [0.918154776096344], [0.9219990372657776], [0.9265872836112976], [0.9402281641960144], [0.9479166865348816], [0.949032723903656], [0.95151287317276], [0.9495287537574768], [0.9523809552192688], [0.9562252163887024], [0.95473712682724], [0.961309552192688], [0.9636656641960144], [0.9618055820465088], [0.9599454402923584], [0.9623016119003296], [0.962177574634552], [0.9639136791229248], [0.9647817611694336], [0.9626736044883728], [0.963789701461792], [0.9630456566810608], [0.9629216194152832], [0.9594494104385376], [0.962177574634552], [0.9667658805847168], [0.9666418433189392], [0.9668898582458496], [0.971230149269104], [0.9702380895614624], [0.9651537537574768], [0.9678819179534912], [0.9660218358039856], [0.9707341194152832], [0.972842276096344], [0.9708581566810608], [0.9747023582458496], [0.9755704402923584], [0.9739583134651184], [0.9750744104385376], [0.9747023582458496], [0.9759424328804016], [0.976934552192688], [0.976934552192688], [0.9768105149269104], [0.9780505895614624], [0.9773065447807312], [0.9775545597076416], [0.976934552192688], [0.97495037317276], [0.9779266119003296], [0.9776785969734192], [0.9796627163887024], [0.977802574634552], [0.9804067611694336], [0.979414701461792], [0.9786706566810608], [0.9789186716079712], [0.9789186716079712], [0.9773065447807312], [0.97817462682724], [0.9776785969734192], [0.9786706566810608], [0.9813988208770752], [0.9812747836112976], [0.9809027910232544], [0.9786706566810608], [0.9816468358039856], [0.981894850730896], [0.9809027910232544], [0.9804067611694336], [0.980654776096344], [0.980282723903656], [0.981894850730896], [0.9810267686843872], [0.980282723903656], [0.9825148582458496], [0.9820188283920288], [0.9813988208770752], [0.9807787537574768], [0.9804067611694336], [0.9810267686843872], [0.980654776096344], [0.980654776096344], [0.9805307388305664], [0.9807787537574768], [0.9796627163887024], [0.9799107313156128], [0.9811508059501648], [0.983134925365448], [0.9822668433189392], [0.9823908805847168], [0.983134925365448], [0.9832589030265808], [0.9830108880996704], [0.984747052192688], [0.9825148582458496], [0.9848710298538208], [0.9837549328804016], [0.9862351417541504], [0.9838789701461792], [0.9828869104385376], [0.983134925365448], [0.9838789701461792], [0.9838789701461792], [0.9838789701461792], [0.984747052192688], [0.9836309552192688], [0.9842509627342224], [0.9841269850730896], [0.9844990372657776], [0.9857391119003296], [0.9846230149269104], [0.9849950671195984], [0.98598712682724], [0.9863591194152832], [0.984375], [0.9844990372657776], [0.9852430820465088], [0.9846230149269104], [0.9862351417541504], [0.9853670597076416], [0.9863591194152832], [0.9854910969734192], [0.9871031641960144], [0.9864831566810608], [0.9857391119003296], [0.9861111044883728], [0.9869791865348816], [0.985615074634552], [0.98598712682724], [0.9878472089767456], [0.9874752163887024], [0.9877232313156128], [0.98598712682724], [0.9842509627342224], [0.9864831566810608], [0.987227201461792], [0.986855149269104], [0.9864831566810608], [0.9875991940498352], [0.9857391119003296], [0.9871031641960144], [0.9867311716079712], [0.9861111044883728], [0.987227201461792], [0.9867311716079712], [0.987227201461792], [0.9869791865348816], [0.9873511791229248], [0.9861111044883728], [0.9849950671195984], [0.9851190447807312], [0.9853670597076416], [0.9857391119003296], [0.9864831566810608], [0.9864831566810608], [0.9853670597076416], [0.9862351417541504], [0.9869791865348816], [0.98598712682724], [0.9867311716079712], [0.9871031641960144], [0.98598712682724], [0.9863591194152832], [0.9861111044883728], [0.9866071343421936], [0.9858630895614624], [0.9864831566810608], [0.9864831566810608], [0.985615074634552], [0.9863591194152832], [0.985615074634552], [0.9861111044883728], [0.987227201461792], [0.9866071343421936], [0.9871031641960144], [0.9877232313156128], [0.9877232313156128], [0.9871031641960144], [0.9875991940498352], [0.986855149269104], [0.988095223903656], [0.988095223903656], [0.9879712462425232], [0.9873511791229248], [0.9875991940498352], [0.9882192611694336], [0.9885912537574768], [0.9877232313156128], [0.988095223903656], [0.9878472089767456], [0.9874752163887024], [0.9874752163887024], [0.9869791865348816], [0.9877232313156128], [0.9873511791229248], [0.9862351417541504], [0.9878472089767456], [0.9878472089767456], [0.9871031641960144], [0.986855149269104], [0.9885912537574768], [0.9873511791229248], [0.9874752163887024], [0.9871031641960144], [0.9883432388305664], [0.9885912537574768], [0.9875991940498352], [0.9883432388305664], [0.9873511791229248], [0.9866071343421936], [0.988467276096344], [0.988467276096344], [0.9877232313156128], [0.9877232313156128], [0.9875991940498352], [0.9887152910232544], [0.9879712462425232], [0.9861111044883728], [0.9874752163887024], [0.9871031641960144], [0.9877232313156128], [0.9875991940498352], [0.9879712462425232], [0.9883432388305664], [0.9883432388305664], [0.9887152910232544], [0.988467276096344], [0.9877232313156128], [0.9879712462425232], [0.9883432388305664], [0.9887152910232544], [0.9879712462425232], [0.9871031641960144], [0.9889633059501648], [0.9887152910232544], [0.9874752163887024], [0.9875991940498352], [0.988467276096344], [0.9885912537574768], [0.989335298538208], [0.9879712462425232], [0.988467276096344], [0.9883432388305664], [0.987227201461792], [0.9878472089767456], [0.9892113208770752], [0.9895833134651184], [0.9895833134651184], [0.989707350730896], [0.9889633059501648], [0.9883432388305664], [0.9887152910232544], [0.9879712462425232], [0.988467276096344], [0.9895833134651184], [0.989335298538208], [0.987227201461792], [0.9887152910232544], [0.9887152910232544], [0.9879712462425232], [0.989335298538208], [0.9894593358039856], [0.9878472089767456], [0.9878472089767456], [0.988467276096344], [0.988095223903656], [0.9885912537574768], [0.988467276096344], [0.9873511791229248], [0.9882192611694336], [0.9882192611694336], [0.9877232313156128], [0.9888392686843872], [0.9894593358039856], [0.9894593358039856], [0.9885912537574768], [0.9890872836112976], [0.9890872836112976], [0.9878472089767456], [0.988467276096344], [0.9887152910232544], [0.9892113208770752], [0.9890872836112976], [0.9885912537574768], [0.988467276096344], [0.9883432388305664], [0.988467276096344], [0.988467276096344], [0.988467276096344], [0.9890872836112976], [0.9890872836112976], [0.9873511791229248], [0.9890872836112976]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('#########################################################################')\n",
    "print('Start Adversarial Training...')\n",
    "log.write('adversarial training...\\n')\n",
    "\n",
    "# 学習の実行\n",
    "# 今回は200回の訓練を行う\n",
    "\n",
    "# accとlossのグラフ描画のための配列\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "\n",
    "for total_batch in range(TOTAL_BATCH):\n",
    "    print(\"Generator\", total_batch)\n",
    "    # Train the generator for one step\n",
    "    for _ in range(5):\n",
    "        samples = generator.generate_one_batch()\n",
    "        rewards = rollout.get_reward(samples, 12, discriminator)\n",
    "        generator.train_step(samples, rewards)\n",
    "\n",
    "    # target_lossが機能していないと思われるので実行しない\n",
    "    # # Test \n",
    "    # if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
    "    #     generator.generate_samples(generated_num // BATCH_SIZE, eval_file)\n",
    "    #     likelihood_dataset = dataset_for_generator(eval_file, BATCH_SIZE)\n",
    "    #     test_loss = target_lstm.target_loss(likelihood_dataset)\n",
    "    #     buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "    #     print('total_batch: ', total_batch, 'test_loss: ', test_loss)\n",
    "    #     log.write(buffer)\n",
    "\n",
    "    # Update roll-out parameters\n",
    "    rollout.update_params()\n",
    "\n",
    "    # Train the discriminator\n",
    "    print(\"Discriminator\", total_batch)\n",
    "    for _ in range(1):\n",
    "        generator.generate_samples(generated_num // BATCH_SIZE, negative_file)\n",
    "        dis_dataset = dataset_for_discriminator(positive_file, negative_file, BATCH_SIZE)\n",
    "        history = discriminator.train(dis_dataset, 1, (generated_num // BATCH_SIZE) * 2)\n",
    "        \n",
    "        acc_list.append(history.history['accuracy'])\n",
    "        loss_list .append(history.history['loss'])\n",
    "\n",
    "\n",
    "file_output(TOTAL_BATCH)\n",
    "\n",
    "\n",
    "print(acc_list)\n",
    "\n",
    "with open('data/generated/acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(acc_list, f)\n",
    "\n",
    "with open('data/generated/loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(loss_list, f)\n",
    "\n",
    "\n",
    "\n",
    "log.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "I0ypqDICv_Ef",
    "outputId": "b5c03f8f-8b1f-4c04-b9cd-d10967e6b974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/save/ (stored 0%)\n",
      "  adding: content/save/generator_150.h5 (deflated 8%)\n",
      "  adding: content/save/generator_pretrained.h5 (deflated 8%)\n",
      "  adding: content/save/discriminator_150.h5 (deflated 8%)\n",
      "  adding: content/save/prob_200.txt (deflated 65%)\n",
      "  adding: content/save/kankouti_haiku2id_re.txt (deflated 75%)\n",
      "  adding: content/save/target_params_py3.pkl (deflated 7%)\n",
      "  adding: content/save/generator_sample.txt (deflated 90%)\n",
      "  adding: content/save/eval_file.txt (deflated 60%)\n",
      "  adding: content/save/output_text_0.txt (deflated 69%)\n",
      "  adding: content/save/discriminator_200.h5 (deflated 8%)\n",
      "  adding: content/save/discriminator_50.h5 (deflated 8%)\n",
      "  adding: content/save/discriminator_pretrained.h5 (deflated 8%)\n",
      "  adding: content/save/output_text_250.txt (deflated 78%)\n",
      "  adding: content/save/experiment-log.txt (deflated 20%)\n",
      "  adding: content/save/output_text_100.txt (deflated 72%)\n",
      "  adding: content/save/discriminator_250.h5 (deflated 8%)\n",
      "  adding: content/save/acc_list.pickle (deflated 67%)\n",
      "  adding: content/save/output_text_200.txt (deflated 76%)\n",
      "  adding: content/save/prob_150.txt (deflated 60%)\n",
      "  adding: content/save/prob_0.txt (deflated 57%)\n",
      "  adding: content/save/loss_list.pickle (deflated 53%)\n",
      "  adding: content/save/generator_0.h5 (deflated 8%)\n",
      "  adding: content/save/prob_50.txt (deflated 59%)\n",
      "  adding: content/save/generator_50.h5 (deflated 8%)\n",
      "  adding: content/save/generator_100.h5 (deflated 8%)\n",
      "  adding: content/save/output_text_50.txt (deflated 73%)\n",
      "  adding: content/save/prob_250.txt (deflated 68%)\n",
      "  adding: content/save/generator_200.h5 (deflated 8%)\n",
      "  adding: content/save/generator_250.h5 (deflated 8%)\n",
      "  adding: content/save/discriminator_100.h5 (deflated 8%)\n",
      "  adding: content/save/discriminator_0.h5 (deflated 8%)\n",
      "  adding: content/save/output_text_150.txt (deflated 72%)\n",
      "  adding: content/save/prob_100.txt (deflated 60%)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_6f3b76e6-f9b0-420e-be53-9577698cea04\", \"save.zip\", 156682330)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ダウンロードしたいフォルダを zip 圧縮する\n",
    "!zip -r /content/save.zip /content/save\n",
    "\n",
    "# 圧縮した zip ファイルをダウンロードする\n",
    "from google.colab import files\n",
    "files.download(\"/content/data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0qENGlUhUbZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aysGHjUov_EI",
    "HlTEODRPv_EO",
    "hxv4ogz1v_EP",
    "NhCgdNyIv_EY",
    "ISvYb9_8v_Eb",
    "PTg3PiFqd7hK",
    "Gy1GpvoEv_Ec"
   ],
   "name": "seqGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7': pyenv)",
   "language": "python",
   "name": "python387jvsc74a57bd09f1e5f8df66e0355b93d6a73e8e18cceb2fedad000b7b1dd4a514e55097c6a9d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "9f1e5f8df66e0355b93d6a73e8e18cceb2fedad000b7b1dd4a514e55097c6a9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
